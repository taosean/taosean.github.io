[{"title":"打包 Python 工程","url":"/2019/10/29/Packaging-Python/","content":"本文主要介绍了两个用来将 Python 工程的依赖进行打包以便于部署的 Python 库。\n<!-- more -->\n1. PEX [github](https://github.com/pantsbuild/pex) [文档](https://pex.readthedocs.io/) [博客](https://medium.com/ovni/pex-python-executables-c0ea39cee7f1)\n2. shiv [github](https://github.com/linkedin/shiv) [文档](https://shiv.readthedocs.io/en/latest/)\n","tags":["python"]},{"title":"将 Tensorflow 模型移植到 Caffe 上","url":"/2019/10/22/Convert-Tensorflow-Model-to-Caffe/","content":"本文主要以 cosine metric learning 工程为例，记录了如何将一个 Tensorflow 模型 (包含 ckpt 文件) 移植到 Caffe 框架下。\n<!-- more -->\n### 流程\n>1. 根据 Tensorflow 的网络定义源码，手动编写 Caffe 的网络定义文件 *.prototxt.\n>2. 将训练好的 ckpt 文件中的参数 dump 到磁盘，存为 npy 文件。\n>3. 使用 pycaffe API, 加载 prototxt 文件，生成 Net 对象。\n>4. 根据 npy 文件与 Net 对象中网络层的对应关系，将 npy 文件中的值赋给 Net 对象中的参数。\n>5. 将 Net 对象保存为 caffemodel 文件到磁盘。\n\n<br/>\n### 一些应该注意的点\n>**1**. Tensorflow 中的 BN 层对应 Caffe 中的两个层，BatchNorm + Scale. 这是因为 Batch Normalization 算法最后有一个 缩放+偏置 的操作，这就对应 Caffe 中的 Scale 层。通常 Scale 层的缩放参数记为 $\\gamma$, 偏置参数记为 $\\beta$. 有时候，从 ckpt 模型中 dump 出的 npy 文件没有 BN 层对应的 gamma 值，这可能是因为其在训练时没有使用缩放（batch_norm 函数的 scale 参数设为了 None ），也就是 $\\gamma=1$。因此在流程第 4 步时，将相应 shape 的值全为 1 的 ndarray 赋给 Net 对象中 Scale 层对应的 gamma 即可，即 `net.params[conv1_scale][0].data[...] = np.ones(bn_beta)`. 此外，若 npy 文件中有 BN 层对应的 beta 值，则在 prototxt 文件中对应的 Scale 层应设置 `bias_term: true`，因为这里的 beta 值就是 bias term. [参考1](https://blog.csdn.net/zziahgf/article/details/78843350) [参考2](https://www.cnblogs.com/LaplaceAkuir/p/7811383.html)\n\n---\n>**2**. ckpt 中 dump 出的 npy 文件中可能没有某些 Convolution 层的 bias 权重。因此，在 prototxt 文件中，为此 Convolution 层设置 `bias_term: false`.\n\n---\n>**3**. 在从 ckpt 中 dump 出来的参数里，有些可能名如 `*/Adam`, `*/Adam_1`，这个是因为模型使用了 Adam 优化器，这两个是对某个参数更新的时候使用的，如果只是在测试阶段进行前向推导，则不需要这两个参数。[参考](https://www.jianshu.com/p/75d8df8511bc)\n但是如果是需要对模型进行 Finetune, 出现大量 Adam 变量丢失的错误，则有可能是 **要恢复的变量的位置** 和 **Adam 优化器的位置** 出错造成的。[【tensorflow】加载pretrained model出现的大量adam变量丢失](https://blog.csdn.net/shwan_ma/article/details/82868751)\n\n---\n>**4**. 一些 Tensorflow 的项目使用 `tf.image.decode_jpeg()` 函数来读取 jpg 图像，要注意的是，如果直接使用此函数的默认 `dct_method` 的话，此函数读取到的值将会跟 `cv2.imread()` 读取的值不一致。这是因为 `tf.image.decode_jpeg()` 函数默认会为了解码速度而牺牲一些解码精度。如果想要获得跟 `cv2.imread()` 相同的结果的话，设置参数 `dct_method='INTEGER_ACCURATE'`。[参考1](https://github.com/tensorflow/tensorflow/issues/24893#issuecomment-454911098) [参考2](https://stackoverflow.com/a/45520846/8149027)\n此外，`tf.image.decode_jpeg()` 函数返回的图像是 `RGB` 通道的，`cv2.imread()` 是 `BGR` 通道。\n\n---\n>**5**. **Tensorflow 和 Caffe 在某些操作上的区别**\n>&emsp;&emsp;**5-1**. **Feature map 以及 卷积核 维度顺序的区别**\n在 Tensorflow 中，feature map 的默认索引顺序是 `NHWC`, 卷积核是 `HWIO`，而 Caffe 中两者的索引顺序是 `NCHW` 和 `OIHW`.\n需要说明的是，如果输入是完全一样的图片，在将图像以及卷积核按各自索引顺序 transpose 好后，后续生成的 feature map 在理论上来说应是完全一样的，它们只是索引的顺序不一样而已(feature map 内部各元素之间的相对顺序是一致的)。\n>&emsp;&emsp;**5-2**.** Flatten 操作的区别**\n据上文所述，同样的输入以及卷积核在不同框架中计算得到的 feature map 是一致的。但是如果要对 feature map (4D) 进行 flatten 操作的话，则此结论可能不成立。因为 flatten 是将 3D 的 feature map 拉伸成 1D, 那么不同的顺序可能就会产生不同的 1D 向量 (内部元素的相对位置可能发生了改变).\n对同一个 feature map, 若其 shape 为 `NHWC` （这里的 NHWC 与上文所说的 NHWC 意义不一样，针对某个特定的 feature map, 其 NHWC 的值是固定的）, Tensorflow 的顺序是沿着 `C -> W -> H`，而 Caffe 的顺序是沿着 `W -> H -> C`。两者 flatten 的顺序在其各自的输入 feature map 索引顺序中都是 `3 -> 2 -> 1`\n例如，一个 `1x3x3x2` 的 feature map.\n```python\ndata =  [[[1,2,3],   | H\n          [4,5,6],   |\n          [7,8,9]],  |\n      W  -------------\n         [[10,11,12],\n          [13,14,15],\n          [16,17,18]]]\n\nTensorflow: [1, 10, 2, 11, 3, 12, 4, 13, ... ]\nCaffe: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...]\n```\n>由于 两者 flatten 的顺序在其各自的输入 feature map 索引顺序中都是 `3 -> 2 -> 1`，因此若这两个 flatten 的输入是一样的话，则它们的输出也是一样的。由于 Caffe 中没有类似 Transpose 这样的层，因此我采取使用 pycaffe 将前向 inference 后 Flatten 层的输入取出，用 numpy 进行 transpose 后，再重新赋给 Flatten 层当输入，然后再次调用 pycaffe 的接口 `net.forward(start='', end='')` 指定从某个层开始前向传播，这里就指定 start 参数为 Flatten 层的 name. 这样，即可在 Caffe 中得到与 Tensorflow 中同样的 Flatten 层输出。[net.forward 指定起点](https://github.com/BVLC/caffe/issues/2725#issue-93930312)\nCaffe 的 Flatten 层有 `axis`, `end_axis` 两个参数，但是我无论如何设置都无法在不对输入进行 transpose 的情况下得到与 Tensorflow 一样的结果。也许是我没理解对这两个参数的意义。[参考1](https://caffe.berkeleyvision.org/tutorial/layers/flatten.html) [参考2](https://stackoverflow.com/a/40401460/8149027)\n\n>&emsp;&emsp;**5-3**.** Padding 操作的区别**\nCaffe 中的所有 padding 操作都是对称的，也就是说如果设置 `pad_w=1` 则会在 feature map 的左右两边都 pad 一个像素。但是 Tensorflow 不是如此，有可能出现左边 pad 1, 右边 pad 2，或者上边 pad 1,下边 pad 2 的情况。因此，在移植时，要保持在 Tensorflow 和 Caffe 中的 padding 方式都一样，这样才能得到相同的结果。\n下面讨论 Caffe 的 padding 与 Tensorflow 中 `SAME` padding 方式的差异。\n正常情况下，如果 `kernel_size=3, stride=1` 那么 `SAME` padding 模式会保持输入输出的尺寸相同，因此，需要在输入的上下左右各 pad 1 个像素。这时，在 Caffe 里只要设置 `pad: 1` 就行，这样两者 pad 的结果就是一样的了。\n但是，如果遇到 $stride\\neq1$ 的情况，情况就变得复杂。有可能两个框架某个 Convolution 或者 Pooling 操作的输入输出尺寸都一样，但是数值却不同。如下图所示 [来源](https://github.com/Microsoft/MMdnn/wiki/Error-in-mobilenet-conversion-from-Tensorflow-to-Caffe-Different-way-of-padding#the-reason-of-the-inconsistent-shapes-is-due-to-symmetric-padding-in-caffe)\n![](padding_1.png)\n![](padding_2.png)\n在 Tensorflow 中, `SAME` padding 模式的策略是: [来源](https://stackoverflow.com/a/53820765/8149027)\n>>First, consider the `SAME` padding scheme. A detailed explanation of the reasoning behind it is given in these notes. Here, we summarize the mechanics of this padding scheme. When using 'SAME', the output height and width are computed as:\n```python\nout_height = ceil(float(in_height) / float(strides[1]))\nout_width  = ceil(float(in_width) / float(strides[2]))\n```\n\n>>The total padding applied along the height and width is computed as:\n```python\n# SAME padding 长度计算\nif (in_height % strides[1] == 0):\n  pad_along_height = max(filter_height - strides[1], 0)\nelse:\n  pad_along_height = max(filter_height - (in_height % strides[1]), 0)\n\nif (in_width % strides[2] == 0):\n  pad_along_width = max(filter_width - strides[2], 0)\nelse:\n  pad_along_width = max(filter_width - (in_width % strides[2]), 0)\n```\n\n>>Finally, the padding on the top, bottom, left and right are:\n```python\npad_top = pad_along_height // 2\npad_bottom = pad_along_height - pad_top\npad_left = pad_along_width // 2\npad_right = pad_along_width - pad_left\n```\n>>Note that the division by 2 means that there might be cases when the padding on both sides (top vs bottom, right vs left) are off by one. In this case, the bottom and right sides always get the one additional padded pixel. For example, when pad_along_height is 5, we pad 2 pixels at the top and 3 pixels at the bottom. Note that this is different from existing libraries such as cuDNN and Caffe, which explicitly specify the number of padded pixels and always pad the same number of pixels on both sides.\n\n>>For the `VALID` scheme, the output height and width are computed as:\n```python\nout_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\nout_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n```\n>>and no padding is used.\n\n>在我的实例中，由于最后进入 Flatten 层的 feature map 需要是 `13x13` 的，而输入图像此前一共经过了 3 次下采样，一次 MAX Pool, 两次 Convolution, 都是 `kernel_size=3, stride=2`。因此，如何使这三次操作的 padding 操作在两个框架中一致就成了关键问题。由于在 SAME padding 中，\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;                  `out_height = ceil(float(in_height) / float(strides[1])),`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;                  `out_width  = ceil(float(in_width) / float(strides[2]))`\n因此，这三次下采样操作的输入尺寸存在这些可能性\n\n>```\n                                            13                      <-  经过第二次 Conv, stride=2\n                                        /         \\\n                                       /           \\\n                                     25            26               <-  经过第一次 Conv, stride=2\n                                  /     \\       /     \\\n                                 49     50     51      52           <-   经过 MAX Pooling\n                                /  \\   /  \\   /   \\   /   \\\n                              97   98 99 100 101 102 103 104        <-      输入图像\n```\n>由于 Caffe 只能进行对称 padding，因此要选择一个合适的输入图像尺寸，使得在这三次操作时 Tensorflow 不会出现 padding 不对称的情况 (因为这在 Caffe 中无法实现)。\n根据上段 `SAME padding 长度计算部分的公式`，我们要使得 `pad_along_width，pad_along_height` 的数值为 **偶数**，这样才能对称。因此，要使得 **`in_height % strides[1] != 0, in_width % strides[2] != 0`**。由于 `strides[1]=2`，因此，`in_height, in_width` 必须是 **奇数**。这样，就可以得到每次操作前的输入尺寸分别是 `97 -> 49 -> 25 -> 13`。这样，在每次操作时，SAME padding 都会为 feature map 在空间维度上四周各 pad 一个像素。而在 Caffe 的对应层的定义里，只要设置 `pad: 1` 即可。\n\n---\n>**6**. 关于 Tensorflow 中获取 Graph 中所有节点名称以及 ckpt 文件中的变量。\n&emsp;&emsp;**6.1**.**读取 ckpt 中的变量** [参考](https://www.jianshu.com/p/75d8df8511bc)\n``` python\nimport tensorflow as tf;\n\n reader = tf.train.NewCheckpointReader(\"/path/to/model.ckpt\")\n variables = reader.get_variable_to_shape_map()\n for key in variables:\n      w = reader.get_tensor(key)\n```\n>&emsp;&emsp;**6.2**. **获取 Graph 中的所有结点名称，并计算得到某节点的值** [参考](https://www.jianshu.com/p/3cee7ca5ebd8)\n``` python\nall_op_names = [n.name for n in tf.get_default_graph().as_graph_def().node]\nconv1_op = tf.get_default_graph.get_tensor_by_name('a_tensor_name_from_above_line:0')  # 注意要在名称后面加 :0\nconv1_value = sess.run(conv1_op, feed_dict={...})\n```\n\n<br>\n### dump ckpt 中的参数以及生成 caffemodel 的两个脚本\n[dump ckpt](dump.py)\n[生成 caffemodel](tf2caffe.py)\n","tags":["flatten"]},{"title":"卡尔曼滤波","url":"/2019/07/04/Kalman-Filter/","content":"本文主要关注卡尔曼滤波的流程和 5 个公式。\n<!-- more -->\n卡尔曼滤波的主要思想：首先，根据时间步 `t-1` 的状态空间，通过状态转移矩阵和控制矩阵(控制量)，预测 `t` 时间步时的状态空间。由于 `t-1` 时间步的状态空间本身就不是准确的，含有噪声，且状态转移的过程也引入噪声，因此预测得到的 `t` 时间步的状态空间是不准确的。这时，我们在 `t` 时间步进行实际的测量，使用得到的测量结果去修正预测得到的状态空间。其实就是对预测的结果和测量的结果根据其不准确度(用协方差矩阵表示)来计算权重(即卡尔曼增益)，对两个结果进行加权平均，并依此得到 `t` 时间步最优的结果。\n\n<br/>\n### 卡尔曼滤波的五个公式\n<center>![卡尔曼公式](format.jpg)<center/>\n\n<br/>\n### 卡尔曼滤波模型\n<center>![卡尔曼滤波模型](Kalman_filter_model.png)<center/>\n","tags":["卡尔曼滤波"]},{"title":"(转载) 浅谈多节点 CPU+GPU 协同计算负载均衡性设计","url":"/2019/06/26/Heterogeneous-Computing/","content":"本文转载自[这里](https://blog.csdn.net/zhang0311/article/details/8224093)，主要讲述了关于基于 CPU+GPU 的混合异构计算系统的内容。\n<!-- more -->\n近年来，基于 CPU+GPU 的混合异构计算系统开始逐渐成为国内外高性能计算领域的热点研究方向。在实际应用中，许多基于 CPU+GPU 的混合异构计算机系统表现出了良好的性能。但是，由于各种历史和现实原因的制约，异构计算仍然面临着诸多方面的问题，其中最突出的问题是程序开发困难，尤其是扩展到集群规模级别时这个问题更为突出。主要表现在扩展性、负载均衡、自适应性、通信、内存等方面。\n\n### 一、 CPU+GPU协同计算模式\n\nCPU+GPU异构协同计算集群如图1所示，CPU+GPU 异构集群可以划分成三个并行层次：节点间并行、节点内 CPU 与 GPU 异构并行、设备（CPU 或 GPU）内并行。根据这三个层次我们可以得到 CPU+GPU 异构协同计算模式为：<font color='orange'>节点间分布式+节点内异构式+设备内共享式</font>。\n\n**1 节点间分布式**\nCPU+GPU 异构协同计算集群中，各个节点之间的连接与传统 CPU 集群一样，采用网络连接，因此，节点间采用了分布式的计算方式，可以采用 MPI 消息通信的并行编程语言。\n\n**2 节点内异构式**\nCPU+GPU 异构协同计算集群中，每个节点上包含多核 CPU 和一块或多块 GPU 卡，节点内采用了异构的架构，采用主从式的编程模型，即每个 GPU 卡需要由 CPU 进程/线程调用。\n\n由于每个节点上，CPU 核数也比较多，计算能力也很大，因此，在多数情况下，CPU 也会参与部分并行计算，根据 CPU 是否参与并行计算，我们可以把 CPU+GPU 异构协同计算划分成两种计算模式：\n\n&emsp;1) CPU/GPU 协同计算：CPU 只负责复杂逻辑和事务处理等串行计算，GPU 进行大规模并行计算；\n&emsp;2) CPU+GPU 共同计算：由一个 CPU 进程/线程负责复杂逻辑和事务处理等串行计算，其它 CPU 进程/线程负责小部分并行计算，GPU 负责大部分并行计算。\n\n由于 CPU/GPU 协同计算模式比 CPU+GPU 共同计算模式简单，下面的介绍中，我们以 CPU+GPU 共同计算模式为例进行展开介绍各种编程模式。\n\n在 CPU+GPU 共同计算模式下，我们把所有的 CPU 统称为一个设备（device），如双路 8 核 CPU 共有 16 个核，我们把这 16 个核统称成一个设备；每个 GPU 卡成为一个设备。根据这种划分方式，我们可以采用 MPI 进程或 OpenMP 线程控制节点内的各设备之间的通信和数据划分。\n\n**3 设备内共享式**\n&emsp;1) CPU 设备：每个节点内的所有多核 CPU 采用了共享存储模型，因此，把节点内的所有多核 CPU 看作一个设备， 可以采用 MPI 进程或 OpenMP 线程、pThread 线程控制这些 CPU 核的并行计算。\n\n&emsp;2) GPU 设备：GPU 设备内有自己独立的 DRAM 存储，GPU 设备也是共享存储模型，在 GPU 上采用 CUDA 或 OpenCL 编程控制 GPU 众核的并行计算。CUDA 编程模式只在 NVIDIA GPU 上支持，OpenCL 编程模式在 NVIDIA GP U和 AMD GPU 都支持。\n\n根据前面对 CPU+GPU 异构协同计算模式的描述，我们可以得到 CPU+GPU 异构协同计算的编程模型（以 MPI 和 OpenMP 为例）如表1所示。\n\n<center>![图1 CPU+GPU异构协同计算架构](pic_1.jpg)<center/>\n<center>图1 CPU+GPU异构协同计算架构 <center/>\n\n<center>表1 CPU+GPU异构协同计算编程模型<center/>\n<center>![表1 CPU+GPU异构协同计算编程模型](pic_2.png)<center/>\n\n\n### 二、CPU+GPU协同计算负载均衡性设计\n\n下面以 模式2 为例简单介绍多节点 CPU+GPU 协同计算任务划分和负载均衡，模式2 的进程和线程与 CPU 核和 GPU 设备对应关系如 图2 所示。若采用主从式 MPI 通信机制，我们在节点 0 上多起一个进程（0号进程）作为主进程，控制其它所有进程。每个节点上启动3个计算进程，其中两个控制 GPU 设备，一个控制其余所有 CPU 核的并行，在 GPU 内采用 CUDA/OpenCL 并行，在 CPU 设备内采用 OpenMP 多线程并行。\n\n由于 CPU+GPU 协同计算模式分为 3个层次，那么负载均衡性也需要在这 3个层次 上分别设计。在 模式2 的编程方式下，节点内和节点间均采用 MPI 进程，合二为一，设计负载均衡时，只需要做到进程间（设备之间）的负载均衡和 CPU 设备内 OpenMP 线程负载均衡、GPU 设备内 CUDA 线程负载均衡即可。\n\n对于设备内，采用的是共享存储器模型，CPU 设备上的 OpenMP 线程可以采用 schedule(static / dynamic / guided )方式；GPU 设备上只要保证同一 warp 内的线程负载均衡即可。\n\n对于 CPU+GPU 协同计算，由于 CPU 和 GPU 计算能力相差很大，因此，在对任务和数据划分时不能给 CPU 设备和 GPU 设备划分相同的任务/数据量，这就增加了 CPU 与 GPU 设备间负载均衡的难度。CPU 与 GPU 之间的负载均衡最好的方式是采用动态负载均衡的方法，然而有些应用无法用动态划分而只能采用静态划分的方式。下面我们分别介绍动态划分和静态划分。\n\n&emsp;1) 动态划分：对于一些高性能计算应用程序，在 CPU 与 GPU 之间的负载均衡可以采用动态负载均衡的优化方法，例如有 N 个任务/数据，一个节点内有 2 个 GPU 卡，即三个设备（CPU 和 2个 GPU），动态负载均衡的方法是每个设备先获取一个任务/数据进行计算，计算之后立即获取下一个任务，不需要等待其他设备，直到 N 个任务/数据计算完成。这种方式只需要在集群上设定一个主进程，负责给各个计算进程分配任务/数据。\n\n&emsp;2) 静态划分：在一些应用中，无法采用动态划分的方式，需要静态划分方法，然而静态划分方法使异构设备间的负载均衡变得困难，有时甚至无法实现。对于一些迭代应用程序，我们可以采用学习型的数据划分方法，如先让 CPU 和 GPU 分别做一次相同计算量的计算，然后通过各自的运行时间计算出 CPU 与 GPU 的计算能力比例，然后再对数据进行划分。\n\n<center>![图2 CPU+GPU协同计算示意图（以每个节点2个GPU为例）](pic_3.jpg)<center/>\n<center>图2 CPU+GPU协同计算示意图（以每个节点2个GPU为例<center/>\n\n### 三、CPU+GPU协同计算数据划分示例\n\n假设某一应用的数据特点如 图3 所示，从输出看，结果中的每个值的计算需要所有输入数据的信息，所有输出值的计算之间没有任何数据依赖性，可以表示成 outj=；从输入看，每个输入值对所有的输出值都产生影响，所有输入数据之间也没有任何数据依赖性。从数据特点可以看出，该应用既可以对输入进行并行数据划分也可以对输出进行数据划分。下面我们分析 CPU+GPU 协同计算时的数据划分方式。\n<center>![图3 并行数据示例](pic_4.jpg)<center/>\n\n<center>图3 并行数据示例<center/>\n\n**1 按输入数据划分**\n\n假设按输入数据划分，我们可以采用动态的方式给每个 CPU 或 GPU 设备分配数据，做到动态负载均衡，然而这种划分方式，使所有的线程向同一个输出位置保存结果，为了正确性，需要使所有的线程对每个结果进行原子操作，这样将会严重影响性能，极端情况下，所有线程还是按顺序执行的。因此，这种方式效果很差。\n\n**2 按输出数据划分**\n\n按输出数据划分的话可以让每个线程做不同位置的结果计算，计算完全独立，没有依赖性。如果采用静态划分的方式，由于 CPU 和 GPU 计算能力不同，因此，很难做到负载均衡。采用动态的方式可以做到负载均衡，即把结果每次给 CPU 或 GPU 设备一块，当设备计算完本次之后，立即向主进程申请下一个分块，这样可以做到完全负载均衡。按输出数据划分，无论采用静态划分还是动态划分，都会带来另外一个问题，由于每个结果的计算都需要所有输入信息，那么所有进程（设备）都需要读取一遍所有输入数据，动态划分时还不只一次，尤其对于输入数据很大时，这将会对输入数据的IO产生很大的影响，很有可能使 IO 程序性能瓶颈。\n\n**3 按输入和输出同时划分**\n\n由于按输入或按输出划分都存在不同的缺点，我们可以采用输入和输出同时划分的方式进行数据划分，如 图4 所示。\n\n从输出角度，让所有的计算进程（设备）都有一份计算结果，设备内的线程对结果进行并行计算，每个设备都有一份局部的计算结果，所有设备都计算完毕之后，利用MPI进程对所有设备的计算结果进行规约，规约最后的结果即是最终的结果。\n\n从输入角度，按输入数据动态划分给不同的计算进程（设备），这样可以满足所有的计算进程负载均衡。\n<center>![图4 CPU+GPU协同计算数据划分示例](pic_5.jpg)<center/>\n<center>图4 CPU+GPU协同计算数据划分示例<center/>\n","tags":["协同计算"]},{"title":"一些 C++ 相关的概念和操作","url":"/2019/06/26/Cxx-Related/","content":"本文主要记录了一些常用的 C++ 相关的概念和操作。\n<!-- more -->\n<br/>\n##### string 与 char* 互转\n```c++\n// string to char*\nstring name = \"name\";\nchar *str = (char*)name.data();\n\n// char* to string\nchar *name = \"name\";\nstring str = string(name);\n```\n\n<br/>\n##### 释放 std::vector 所占用的内存\n>在容器 vector 中，其内存占用的空间是只增不减的，比如说首先分配了 10,000 个字节，然后 erase 掉后面 9999个，则虽然有效元素只有一个，但是内存占用仍为 10,000 个。所有内存空间在 vector 析构时回收。\n一般，我们都会通过 vector 中成员函数 clear 进行一些清除操作，但它清除的是所有的元素，使 vector 的大小减少至 0，却不能减小 vector 占用的内存。要避免 vector 持有它不再需要的内存，这就需要一种方法来使得它从曾经的容量减少至它现在需要的容量，这样减少容量的方法被称为 “**收缩到合适（shrink to fit）**”。\n\n使用以下代码可以实现此功能:\n```c++\nvector<T>().swap(X)  // X 的类型为 std::vector<T>;\n//其相当于\nstd::vector<T>  temp(X);\ntemp.swap(X);\n```\n>其背后原理为:**`vector()` 使用 `vector` 的默认构造函数建立临时 `vector` 对象，再在该临时对象上调用 `swap` 成员，`swap` 调用之后对象 `X` 占用的空间就等于一个默认构造的对象的大小，临时对象就具有原来对象 `X` 的大小，而该临时对象随即就会被析构，从而其占用的空间也被释放。**\n\n[参考](https://www.cnblogs.com/zhoug2020/p/4058487.html)\n\n<br/>\n#### 二维数组和双重指针在内存中的差别\n首先，下例是不可行的\n```c++\n#define ROW 2\n#define COL 3\n\nvoid myputs(char **pos);\n\nint main() {\n    char **p;\n    char a[ROW][COL]={\"abc\", \"def\"};\n\n    p = a;\n    myputs(p);\n    return 0;\n}\n\nvoid myputs(char **p) {\n    int i, j;\n    for (i = 0; i < ROW; i++) {\n        for (j = 0; j < COL; j++)\n            printf(\"%c \", p[i][j]); // 试图用双重指针的方式访问二维数组，不可行\n        printf(\"\\n\");\n    }\n}\n```\n`myputs(char **p)` 接受双重指针作为参数，`main()` 函数将二维数组的头指针赋给双重指针，并作为 `myputs(char **p)` 的参数传入，再使用 `p[i][j]` 的方式访问某个元素。这是不行的，而这与两者的内存分布有关。\n\n---\n**二维数组的内存分布**\n>定义了二维数组后，就会在内存中分配一块逻辑上连续的内存块。`char c[10][10]`，系统就会分配一块 100 字节的连续内存。也就是说这样的二维数组跟一维数组 `char c[100]` 具有相似的内存分布。\n二维数组的内存分布如下：\n<center>![二维数组的内存分布](2d_array.png)<center/>\n\n**双重指针的内存分布**\n>双重指针的内存分配一般采取动态方式\n<center>![双重指针的内存分布](2d_pointer.png)<center/>\n\n可以看出，当将二维数组的头指针赋值给双重指针后，再使用 `p[i][j]` 的方式访问里面的元素，就会出现错误。这是因为，二维数组的内存是以连续的方式分配的，但是在访问时，却使用了双重指针的方式进行访问，这就会导致段错误。\n\n---\n**总结**\n>`char **p` 和 `char p[2][3]` 之间不能相互传递参数，因为它们具体的内存分布不一样，这样在运行时就会出现段错误。\n此外还需注意的一点：\n二维数组中的 `a[i][j]` 和双重指针中的 `a[i][j]` 的意思是不一样的。\n二维数组 `int a[10][10]` 中，`a[i][j]` 指的是第 `i` 行第 `j` 列数元素。\n双重指针中 `int **a` 中， `a[i][j]` 指的是第 `i` 个存放 `int *` 指针所指向地址中的第 `j` 个元素。也就是 `*(*(a+i)+j)`。\n\n[原文](https://blog.csdn.net/u013684730/article/details/46565577)\n","tags":["C++"]},{"title":"进程和线程及相关概念","url":"/2019/06/14/Process-and-Thread/","content":"本文主要记录了进程，线程等相关内容。\n<!-- more -->\n>抛开各种技术细节，从应用程序角度讲：\n1、在单核计算机里，有一个资源是无法被多个程序并行使用的：CPU。\n没有操作系统的情况下，一个程序一直独占着全部 CPU。\n如果要有两个任务来共享同一个 CPU，程序员就需要仔细地为程序安排好运行计划--某时刻 CPU 和由程序 A 来独享，下一时刻 CPU 由程序 B 来独享,而这种安排计划后来成为 OS 的核心组件，被单独名命为 “**scheduler**”，即“**调度器**”，它关心的只是怎样把单个 CPU 的运行拆分成一段一段的“运行片”，轮流分给不同的程序去使用，而在宏观上，因为分配切换的速度极快，就制造出多程序并行在一个 CPU 上的假象。\n\n>2、在单核计算机里，有一个资源可以被多个程序共用，然而会引出麻烦：**内存**。\n在一个只有调度器，没有内存管理组件的操作系统上，程序员需要手工为每个程序安排运行的空间 -- 程序A使用物理地址 `0x00-0xff`, 程序B使用物理地址` 0x100-0x1ff`，等等。\n然而这样做有个很大的问题：每个程序都要协调商量好怎样使用同一个内存上的不同空间，软件系统和硬件系统千差万别，使这种定制的方案没有可行性。\n为了解决这个麻烦，计算机系统引入了“**虚拟地址**”的概念，从三方面入手来做：\n2.1、硬件上，CPU 增加了一个专门的模块叫 MMU，负责转换虚拟地址和物理地址。\n2.2、操作系统上，操作系统增加了另一个核心组件：**memory management**，即内存管理模块，它管理物理内存、虚拟内存相关的一系列事务。\n2.3、应用程序上，发明了一个叫做【进程】的模型，（注意）每个进程都用【**完全一样的**】虚拟地址空间，然而经由操作系统和硬件MMU协作，映射到不同的物理地址空间上。不同的【进程】，都有各自独立的物理内存空间，不用一些特殊手段，是无法访问别的进程的物理内存的。\n\n>3、现在，不同的应用程序，可以不关心底层的物理内存分配，也不关心 CPU 的协调共享了。然而还有一个问题存在：有一些程序，想要共享 CPU，【并且还要共享同样的物理内存】，这时候，一个叫【线程】的模型就出现了，它们被包裹在进程里面，在调度器的管理下共享 CPU，拥有同样的虚拟地址空间，同时也共享同一个物理地址空间，然而，它们无法越过包裹自己的进程，去访问别一个进程的物理地址空间。\n\n>4、进程之间怎样共享同一个物理地址空间呢？不同的系统方法各异，符合 posix 规范的操作系统都提供了一个接口，叫 mmap，可以把一个物理地址空间映射到不同的进程中，由不同的进程来共享。\n\n>5、PS：在有的操作系统里，进程不是调度单位（即不能被调度器使用），线程是最基本的调度单位，调度器只调度线程，不调度进程，比如 VxWorks\n[来源](http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html#comment-270980)\n\n\n<br/>\n>CPU+RAM+各种资源（比如显卡，光驱，键盘，GPS, 等等外设）构成我们的电脑，但是电脑的运行，实际就是 CPU 和相关寄存器以及 RAM 之间的事情。\n**一个最最基础的事实**：CPU 太快，太快，太快了，寄存器仅仅能够追的上他的脚步，RAM 和别的挂在各总线上的设备完全是望其项背。那当多个任务要执行的时候怎么办呢？轮流着来?或者谁优先级高谁来？不管怎么样的策略，一句话就是在 CPU 看来就是轮流着来。\n**一个必须知道的事实**：执行一段程序代码，实现一个功能的过程介绍 ，当得到 CPU 的时候，相关的资源必须也已经就位，就是显卡啊，GPS 啊什么的必须就位，然后 CPU 开始执行。这里除了 CPU 以外所有的就构成了这个程序的执行环境，也就是我们所定义的程序上下文。当这个程序执行完了，或者分配给他的 CPU 执行时间用完了，那它就要被切换出去，等待下一次 CPU 的临幸。在被切换出去的最后一步工作就是保存程序上下文，因为这个是下次他被 CPU 临幸的运行环境，必须保存。\n**串联起来的事实**：前面讲过在 CPU 看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行 A，保存程序 A 的上下文，调入下一个要执行的程序 B 的程序上下文，然后开始执行 B,保存程序 B 的上下文。。。。\n========= 重要的东西出现了========\n进程和线程就是这样的背景出来的，**两个名词不过是对应的CPU时间段的描述，名词就是这样的功能**。\n进程就是包换上下文切换的程序执行时间总和 = CPU 加载上下文 + CPU 执行 + CPU 保存上下文\n**线程是什么呢？**进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序 A，实际分成 a，b，c 等多个块组合而成。那么这里具体的执行就可能变成：程序 A 得到 CPU => CPU 加载上下文，开始执行程序 A 的 a 小段，然后执行 A 的 b 小段，然后再执行 A 的 c 小段，最后 CPU 保存  A 的上下文。这里 a，b，c 的执行是共享了 A 的上下文，CPU 在执行的时候没有进行上下文切换的。这里的 a，b，c 就是线程，也就是说线程是共享了进程的上下文环境的更为细小的 CPU 时间段。到此全文结束，再一个总结：**进程和线程都是一个时间段的描述，是 CPU 工作时间段的描述，不过是颗粒大小不同。**\n[来源](https://www.zhihu.com/question/25532384/answer/81152571)\n\n\n<br/>\n>一、 cpu个数、核数、线程数的关系\ncpu个数：是指物理上，也及硬件上的核心数；\n核数：是逻辑上的，简单理解为逻辑上模拟出的核心数；\n线程数：是同一时刻设备能并行执行的程序个数，线程数=cpu个数 * 核数【如果有超线程，再乘以超线程数】\n\n>二、 cpu线程数和Java多线程\n首先明白几个概念：\n(1) 单个cpu线程在同一时刻只能执行单一Java程序，也就是一个线程\n(2) 单个线程同时只能在单个cpu线程中执行\n(3) 线程是操作系统最小的调度单位，进程是资源（比如：内存）分配的最小单位\n(4)Java中的所有线程在JVM进程中,CPU调度的是进程中的线程\n(5)Java多线程并不是由于cpu线程数为多个才称为多线程，当Java线程数大于cpu线程数，操作系统使用时间片机制，采用线程调度算法，频繁的进行线程切换。\n\n>a 那么java多进程，每个进程又多线程，cpu是如何调度的呢？\n个人理解：操作系统并不是单纯均匀的分配cpu执行不同的进程，因为线程是调度的最小单位，所以会根据不同进程中的线程个数进行时间分片，均匀的执行每个线程，也就是说A进程中有10个线程，而B进程中有2个线程，那么cpu分给进程的执行时间理论上应该是5:1才合理。\n\n>b cpu线程数和java线程数有直接关系吗？\n个人理解：没有直接关系，正如上面所说，cpu采用分片机制执行线程，给每个线程划分很小的时间颗粒去执行，但是真正的项目中，一个程序要做很多的的操作，读写磁盘、数据逻辑处理、出于业务需求必要的休眠等等操作，当程序在进行I/O操作的时候，线程是阻塞的，线程由运行状态切换到等待状态，此时cpu会做上下文切换，以便处理其他的程序；当I/O操作完成后，cpu 会收到一个来自硬盘的中断信号，并进入中断处理例程，手头正在执行的线程因此被打断，回到 ready 队列。而先前因 I/O 而waiting 的线程随着 I/O 的完成也再次回到 就绪 队列，这时 cpu 可能会选择它来执行。\n\n>c 如何确定程序线程数？\n个人理解：如果所有的任务都是计算密集型的，则创建的多线程数 = 处理器核心数就可以了\n如果io操作比较耗时，则根据具体情况调整线程数，此时 多线程数 = n*处理器核心数\n一般情况程序线程数等于cpu线程数的两到三倍就能很好的利用cpu了，过多的程序线程数不但不会提高性能，反而还会因为线程间的频繁切换而受影响，具体需要根据线程处理的业务考略，不断调整线程数个数，确定当前系统最优的线程数。\n[原文](https://blog.csdn.net/wutongyuWxc/article/details/78732287)\n\n\n<br/>\n### 一篇非常好的文章\n>[进程与线程，单核与多核](https://cloud.tencent.com/developer/article/1352974)\n","tags":["多核"]},{"title":"轻量卷积神经网络的一些操作","url":"/2019/06/03/light-weight-CNN-operations/","content":"本文主要记录一些在卷积神经网络轻量化的研究中出现的一些操作，其主要是针对卷积进行的。\n<!-- more -->\n### Depthwise Convolution and Pointwise Convolution\n<font color='orange'>深度卷积</font> 分解一个标准的卷积为一个 depthwise convolution 和一个 pointwise convolution, 是对输入的每一个 channel 独立进行卷积，输入 feature map 的每个 channel 会输出 **channel_multiplier** (通常为 1) 个通道，最后的 feature map 就会有 in_channels * channel_multiplier 个通道了。\n\n---\n<center>![传统卷积和深度卷积以及逐点卷积的对比_1](convs.jpg)<center/>\n\n---\n<center>![传统卷积和深度卷积以及逐点卷积的对比_2](convs_2.jpg)<center/>\n\n<br/>\n### Group Convolution and channel shuffle\n<center>![分组卷积](GConv.png)<center/>\n\n<font color='orange'>**Group Convolution**</font> 顾名思义，是对输入 feature map 进行分组，然后每组分别卷积。假设输入 feature map 的尺寸为 $C∗H∗W$，输出 feature map 的数量为 $N$，如果设定要分成 $G$ 个 groups，则每组的输入 feature map 数量为 $$\\frac{C}{G}$$，每组的输出 feature map 数量为 $$\\frac{N}{G}$$，每个卷积核的尺寸为 $$\\frac{C}{G}\\times K\\times K$$，卷积核的总数仍为 $N$ 个，每组的卷积核数量为 $\\frac{N}{G}$，卷积核只与其同组的输入 map 进行卷积，卷积核的总参数量为 $N\\times \\frac{C}{G}\\times K\\times K$，可见，总参数量减少为原来的 $\\frac{1}{G}$，其连接方式如上图右所示，group 1 的输出 map 数为 2，有 2 个卷积核，每个卷积核的 channel 数为 4，与 group1 的输入 map 的 channel 数相同，卷积核只与同组的输入 map 卷积，而不与其他组的输入 map 卷积。\n\n<font color='orange'>**Channel shuffle**</font>: 因为在**同一组中不同的通道蕴含的信息可能是相同的**，如果不进行通道交换的话，**学出来的特征会非常局限**。如果在不同的组之后交换一些通道，那么就能**交换信息，使得各个组的信息更丰富**，能提取到的特征自然就更多，这样是有利于得到更好的结果。\n\n---\n<center>![Channel Shuffle](channel_shuffle.jpg)<center/>\n\nShuffleNet主要拥有两个创新点：\n>1. pointwise group convolution <font color='orange'>**逐点组卷积，就是带分组的卷积核为1×1的卷积，也就是说逐点组卷积是卷积核为 1×1 的分组卷积**</font>。\n>2. channel shuffle\n\n原因：\n>1. 逐点卷积占了很大的计算量 ———> 逐点分组卷积\n>2. 不同组之间特征通信问题   ———> channel shuffle\n\n---\n<font color='orange'>**GDC :**</font> 更进一步，如果分组数 $G=N=C$，同时卷积核的尺寸与输入 map 的尺寸相同，即 $K=H=W$，则输出 map 为 $C∗1∗1$ 即长度为 $C$ 的向量，此时称之为 **Global Depthwise Convolution（GDC）**，见 MobileFaceNet，可以看成是全局加权池化，与 Global Average Pooling（GAP） 的不同之处在于，GDC 给每个位置赋予了可学习的权重（对于已对齐的图像这很有效，比如人脸，中心位置和边界位置的权重自然应该不同），而 GAP 每个位置的权重相同，全局取个平均，如下图所示：\n\n---\n<center>![Global Depthwise Convolution](GDC.png)<center/>\n\n<br/>\n### Squeeze-and-Excitation module\n<font color='orange'>**SE module**</font> 通过学习的方式来自动获取到每个特征通道的重要程度，然后依照计算出来的重要程度去提升有用的特征并抑制对当前任务用处不大的特征。\n<center>![SE module](SE.jpg)<center/>\n\n---\n>1. 首先做普通的卷积，得到了一个 output feature map，它的 shape 为 $[C，H，W]$，根据论文观点，这个 feature map 的特征很混乱。为了获得重要性的评价指标，直接对这个 feature map 做一个 Global Average Pooling，然后我们就得到了长度为 $C$ 的向量。（这里还涉及到一个额外的东西，如果你了解卷积，你就会发现一旦某一特征经常被激活，那么 Global Average Pooling 计算出来的值会比较大，说明它对结果的影响也比较大，反之越小的值，对结果的影响就越小）\n>2. 然后我们对这个向量加两个 FC 层，做非线性映射，这两个 FC 层的参数，也就是网络需要额外学习的参数。\n>3. 最后输出的向量，我们可以看做特征的重要性程度，然后与 feature map 对应 channel 相乘就得到特征有序的 feature map 了。\n","tags":["SE"]},{"title":"Git的使用流程","url":"/2019/05/31/Git-Usage/","content":"本文主要记录一些使用 Git 进行版本管理的流程和命令。\n<!-- more -->\n## 概念建立\n1&emsp;工作区: 代码所在的文件路径\n2&emsp;暂存区: 使用 `git add <filename>`，将文件 <filename> 添加进 暂存区，待后续操作。\n3&emsp;本地仓库: 使用  `git commit -m \"comments here\"` 将 暂存区的所有文件 commit 到本地仓库，本地仓库位于本机。\n4&emsp;远程仓库: 在服务器端运行，可将本地仓库内容通过 `git push` 推送到远程仓库。\n\n<br/>\n## 本地仓库和远程仓库的创建\n### 1. 添加远程库\n为了方便管理，创建 git 用户.\n在远程服务器上安装好 git 后，使用以下命令创建远程仓库\n```bash\nmkdir example.git\ncd example.git\ngit --bare init\n```\n用 `chown -R git:git example.git` 将 `example.git` 的所有者和群组改为 git.\n\n### 2. 添加本地仓库\n若已经拥有了远程仓库，可以通过 `git clone <example.git>` 的命令将远程仓库 clone 到本地。\n若是对已存在的工程添加 git 管理，则在工程目录下，使用 `git init` 命令将其变成 git 管理的仓库。\n\n<br/>\n## 常用的 git 流程\n创建好本地仓库和远程仓库后，就可以使用 git 进行版本控制了。\n若是对已存在的工程进行操作，则流程如下:\n>1&emsp; 使用 git add <filename> 的方式将文件添加到暂存区。若某工程第一次使用 git，用 `git add .` 将工程目录下的所有文件添加到暂存区。此操作可以在 `Git bash` 中通过命令行操作，或者在 `Git GUI` 通过图形界面操作。\n\n>---\n>2&emsp; 若已在暂存区中添加了一些文件，可以通过 `git commit -m \"comments here\"` 将暂存区中所有文件 commit 到本地仓库的当前分支。\n\n>---\n>3&emsp; 若要将本地的当前分支，如 master 分支推送到远程仓库的 master 分支，则使用 `git push` 命令。\n>>注意: 应先将本地仓库与远程仓库关联，在本地 `example` 工程下，使用语句 `git remote add origin git@10.167.93.74:/path/to/example.git`. 这样，就将 `10.167.93.74:/path/to/example.git` 与本地 `example` 仓库关联上了。添加后，远程库的名字就是 `origin`，这是 Git 默认的叫法，也可以改成别的，但是 `origin` 这个名字一看就知道是远程库。\n\n>>---\n>>关联后，使用命令 `git push -u origin master` 第一次推送 `master` 分支的所有内容；\n\n>>---\n>>此后，每次本地提交后，只要有必要，就可以使用命令 `git push origin master` 推送最新修改；\n\n>注意: 若不想将工程路径下的所有文件添加版本管理，则可以只将部分文件进行 add, 对从未进行过 add 操作的文件，将被视作 `untracked`.\n\n>---\n>4&emsp; 若对已经被 Git 管理的多个文件在某次 commit 后进行了修改，想将这些文件一次性进行 add，可以使用命令 `git add -u`. 这样，就不会将那些 `untracked` 的文件添加进暂存区 (git 术语为进行 stage). 若使用 `git add .` 命令，将会提交 **新文件** (new) 和 **被修改** (modified) 文件，这时，那些 `untracked` 的文件会被看作 `新文件`。\n>> `Git add` 命令的 3 种用法:\n```bash\ngit add .  : 监控工作区的状态树，使用它会把工作时的所有变化提交到暂存区，包括文件内容修改 (modified) 以及新文件 (new)，但不包括被删除的文件。\ngit add -u : 仅监控已经被 add 的文件（即 tracked file），他会将被修改的文件提交到暂存区。add -u 不会提交新文件（untracked file）。（git add --update 的缩写）\ngit add -A : 上面两个功能的合集（git add --all 的缩写）\n```\n>>示例\n```bash\ngit init\necho Change me > change-me\necho Delete me > delete-me\ngit add change-me delete-me\ngit commit -m initial\n\necho OK >> change-me\nrm delete-me\necho Add me > add-me\n\ngit status\n# Changed but not updated:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit add .\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n# Changed but not updated:\n#   deleted:    delete-me\n\ngit reset\n\ngit add -u\ngit status\n\n# Changes to be committed:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit reset\n\ngit add -A\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n#   deleted:    delete-me\n```\n>>[参考](https://www.cnblogs.com/skura23/p/5859243.html)\n\n\n<br/>\n## 常用 Git 命令\n```bash\ngit add\ngit commit\ngit status\ngit log\ngit push <remote> <local branch name>:<remote branch to push into>\ngit log --graph #查看树状图 \n```\n\n<br/>\n## 常用 Git 工具\n· Git Bash\n· Git GUI\n· Pycharm 上的 Git\n\n<br/>\n## 教程\n[Git教程-廖雪峰](https://www.liaoxuefeng.com/wiki/896043488029600)\n","tags":["version control"]},{"title":"AlexeyAB/Darknet 的使用经验总结","url":"/2019/05/31/AlexyAB-Darknet/","content":"本文主要记录了 GitHub 上的热门 Repo [AlexeyAB/Darknet](https://github.com/AlexeyAB/darknet) 的一些使用要点。\n<!-- more -->\n## 与 OpenCV 的速度比较\n>&emsp;使用 GPU 时此 Repo 速度比 OpenCV 快\n>&emsp;使用 CPU 时此 Repo 速度比 OpenCV 慢\n[来源](https://github.com/AlexeyAB/darknet/issues/3273#issuecomment-497096110)\n\n<br/>\n\n## Finetune 相关\n记在 base 数据集上训练得到的模型为 yolov3-old.weights, 当有新增数据集时，\n>&emsp;若新增数据集和 base 数据\n集类别一致，则在 yolov3-old.weights 的基础上，用 base+新增数据 进行训练。\n\n---\n>&emsp;若新增数据集包含其他类别，则先用 `darknet.exe partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81` 得到 `yolov3.conv.81` 模型，再在此模型上用新数据集进行训练。[来源](https://github.com/AlexeyAB/darknet/issues/3264#issuecomment-496725772) [partial命令](https://github.com/AlexeyAB/darknet/blob/55dcd1bcb8d83f27c9118a9a4684ad73190e2ca3/build/darknet/x64/partial.cmd#L27)\n\n<br/>\n\n## 处理逻辑\n以使用单个 GPU 进行处理为例:\n### <font size='6'>训练</font>\n假设 cfg 文件中定义的 `batch=64`, `subdivisions=16`\n\n---\n><1> 解析各种配置文件，如 `coco.data`, `coco.names` 等，获取各种参数。\n<2> 解析 `cfg` 文件并将其实例化为 `network net` 对象. (注意，此过程中 net.batch 参数不是 cfg 文件中的 batch 值，而是 cfg 文件中 batch/subdivisions 得到的值。`net.batch` 的值为真正进行前向传播时的 batch size)\n<3> 加载预训练模型 - `weights` 文件到 `net` 对象中。\n<4> 获取所有训练图像的路径。\n<5> 创建一个线程用来从磁盘中 load 数据，每次从磁盘中 load cfg 文件中的 batch 张图像到内存。\n<6> 迭代训练。\n&emsp;&emsp;<6.1> 将一次从磁盘中读取的 batch (cfg 文件中) 张图像分成 `subdivisions` 份，即每份为 `net.batch`。使用 `net.batch` 张图像进行一次迭代，同时返回一个 `batch` 的 `loss`，最后，对 `subdivisions` 个 batch 的 loss 进行加和平均，得到一次从磁盘读取的所有图像 (cfg 中 batch 张) 的平均 loss.\n&emsp;&emsp;<6.2> 不断重复步骤 6.1，在某些迭代次数时生成模型以及计算 mAP.\n\n---\n\n### <font size='6'>测试</font>\n由于 AlexeyAB 没有提供 批量测试 函数 (Batch Inference), 因此我自己实现了此功能。\n```c++\n// network.c 中添加以下函数\nfloat **network_predict_image_batch_gpu(float *imgBatch, network* net, float thresh, float hier_thresh, float nms, metadata meta, int* box_nums) {\n\tint w = network_width(net);\n\tint h = network_height(net);\n\tint batch = net->batch;\n\tint c = 3;\n\n\t// predict batch images\n\tnetwork_predict(*net, imgBatch);\n\tfree(imgBatch);\n\n\tfloat **results = (float**)calloc(batch, sizeof(float*));\n\n\tfor (int i = 0; i < batch; i++) {\n\t\tint nboxes = 0;\n\t\tint letterbox = 0;\n\t\tdetection * dets = get_network_boxes(net, w, h, thresh, hier_thresh, 0, 1, &nboxes, letterbox);\n\t\tdo_nms_sort(dets, nboxes, meta.classes, nms);\n\n\t\tint real_box_num = 0;\n\n\t\tfor (int j = 0; j < nboxes; j++) {\n\t\t\tfor (int s = 0; s < meta.classes; s++) {\n\t\t\t\tif (dets[j].prob[s] > 0) {\n\t\t\t\t\treal_box_num += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tmemcpy(box_nums + i, &real_box_num, sizeof(int));\n\t\tfloat *res = (float*)calloc(real_box_num * 6, sizeof(float)); // 6 is {x, y, w, h, prob, class}\n\n\t\tfor (int j = 0; j < nboxes; j++) {\n\t\t\tfor (int s = 0; s < meta.classes; s++) {\n\t\t\t\tfloat now_prob = dets[j].prob[s];\n\t\t\t\tif (now_prob > 0) {\n\t\t\t\t\tbox b = dets[j].bbox;\n\t\t\t\t\tchar * nameTag = meta.names[s];\n\t\t\t\t\tfloat x_ctr = b.x;\n\t\t\t\t\tfloat y_ctr = b.y;\n\t\t\t\t\tfloat width = b.w;\n\t\t\t\t\tfloat height = b.h;\n\t\t\t\t\tfloat cls_idx = (float)s;\n\n\t\t\t\t\tmemcpy(res, &x_ctr, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t\tmemcpy(res, &y_ctr, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t\tmemcpy(res, &width, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t\tmemcpy(res, &height, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t\tmemcpy(res, &now_prob, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t\tmemcpy(res, &cls_idx, 1 * sizeof(float));\n\t\t\t\t\tres += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tres -= real_box_num * 6;\n\t\tresults[i] = res;\n\n\t\tfree_detections(dets, nboxes);\n\n\t\tfor (int k = 0; k < net->n; k++) {\n\t\t\tlayer temp_layer = net->layers[k];\n\t\t\tif (temp_layer.type == YOLO || temp_layer.type == REGION || temp_layer.type == DETECTION) {\n\t\t\t\tnet->layers[k].output = net->layers[k].output + net->layers[k].outputs;\n\t\t\t\t//temp_layer.output = temp_layer.output + temp_layer.outputs; 原来的版本\n\t\t\t}\n\t\t}\n\n\t}\n\n\treturn results;\n}\n\n// 如果要将 darknet 编译成 dll 供其他程序使用，则在 darknet.h 里 network_predict_image 附近 加上\nLIB_API float **network_predict_image_batch_gpu(float *imgBatch, network* net, float thresh, float hier_thresh, float nms, metadata meta, int* box_nums)；\n```\n\n调用此函数的示例代码\n```c++\nint main() {\n\tchar* configPath = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/cfg/yolov3-mlit-SD.cfg\";\n\tchar* weightPath = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/backup/yolov3-mlit-SD_50000.weights\";\n\tchar* metaPath = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/cfg/mlit.data\";\n\tstring result_dir = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/result/\";\n\n\tchar* img_path_1 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_1.jpg\";\n\tchar* img_path_2 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_2.jpg\";\n\tchar* img_path_3 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_3.jpg\";\n\tchar* img_path_4 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_4.jpg\";\n\tchar* img_path_5 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_5.jpg\";\n\tchar* img_path_6 = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/mlit_yolo/mlit_yolo/test_images/test_6.jpg\";\n\n\tint batchSize = 6;\n\n\tchar **img_paths = (char**)calloc(batchSize, sizeof(char*));\n\timg_paths[0] = img_path_1;\n\timg_paths[1] = img_path_2;\n\timg_paths[2] = img_path_3;\n\timg_paths[3] = img_path_4;\n\timg_paths[4] = img_path_5;\n\timg_paths[5] = img_path_6;\n\n\tfloat conf_thresh = 0.6;\n\tfloat hier_thresh = 0.5;\n\tfloat nms = 0.45;\n\n\tcuda_set_device(0);\n\tnetwork* netMain = load_network_custom(configPath, weightPath, 0, batchSize);\n\tmetadata metaMain = get_metadata(metaPath);\n\n\tint input_w = network_width(netMain);\n\tint input_h = network_height(netMain);\n\tint c = 3;\n\t//int num_calsses = metaMain.classes;\n\n\t// 这里得到的 batch 就是上面手动设置的 batchSize\n\tint batch = netMain->batch;\n\n\t// 从 char** 中读取图像数据，并合并成 float *\n\tfloat *imgBatch = (float*)calloc(batch * input_w * input_h * c, sizeof(float));\n\n\t//No OpenCV\n\tfor (int i = 0; i < batch; i++) {\n\t\timage dark_image = load_image_color(img_paths[i], 0, 0);\n\t\timage resized = resize_image(dark_image, input_w, input_h);\n\t\tmemcpy(imgBatch + i*input_w*input_h*c, resized.data, input_w*input_h*c * sizeof(float));\n\t}\n\n\n\tint *box_num_batch = (int*)calloc(batch, sizeof(int));\n\tfloat **results = network_predict_image_batch_gpu(imgBatch, netMain, conf_thresh, hier_thresh, nms, metaMain, box_num_batch);\n\tfloat *res;\n\n\t// 解析 results\n\tfor (int i = 0; i < batch; i++) {\n\t\tcv::Mat image2show = cv::imread(img_paths[i]);\n\t\tint ori_w = image2show.cols;\n\t\tint ori_h = image2show.rows;\n\n\t\tint nbox = *(box_num_batch + i);\n\t\tres = results[i];\n\n\t\tstd::cout << std::endl << nbox << \" boxes detected\" << std::endl;\n\t\tstd::cout << std::endl;\n\n\t\tfor (int j = 0; j < nbox; j++) {\n\t\t\tfloat x_ctr = res[0 + 6 * j];\n\t\t\tfloat y_ctr = res[1 + 6 * j];\n\t\t\tfloat width = res[2 + 6 * j];\n\t\t\tfloat height = res[3 + 6 * j];\n\t\t\tfloat prob = res[4 + 6 * j];\n\t\t\tfloat cls_idx = res[5 + 6 * j];\n\t\t\tchar * nameTag = metaMain.names[(int)(cls_idx)];\n\n\t\t\tint w_on_ori = (int)(width * ori_w);\n\t\t\tint h_on_ori = (int)(height * ori_h);\n\t\t\tint lft = (int)(x_ctr * ori_w - w_on_ori / 2);\n\t\t\tint rgt = (int)(x_ctr * ori_w + w_on_ori / 2);\n\t\t\tint top = (int)(y_ctr * ori_h - h_on_ori / 2);\n\t\t\tint bot = (int)(y_ctr * ori_h + h_on_ori / 2);\n\n\t\t\tcv::Point pt1(lft, top);\n\t\t\tcv::Point pt2(rgt, bot);\n\t\t\tcv::rectangle(image2show, pt1, pt2, Scalar(0, 255, 0), 1);\n\t\t\tstd::string text = std::string(nameTag) + \" [\" + to_string(int(round(prob * 100))) + \"]\";\n\t\t\tcv::putText(image2show, text, Point(pt1.x, pt1.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.5, Scalar(0, 255, 0), 2);\n\n\t\t\tprintf(\">>> %d %d %d %d %f <<< \", (int)(x_ctr * ori_w), (int)(y_ctr * ori_h), (int)(width * ori_w), (int)(height * ori_h), prob);\n\t\t\tstd::cout << nameTag << std::endl;\n\n\t\t}\n\n\t\tcv::imshow(\"detected\", image2show);\n\t\tcv::waitKey(0);\n\t\t//free(res);\n\t\t//free_detections(res, nbox);\n\t}\n\n\tfree(box_num_batch);\n\tfree(results);\n\n\tsystem(\"pause\");\n\n\treturn 0;\n}\n```\n\n### <font size='6'>主存与显存</font>\n在此 repo 的实现中，数据是先从磁盘读取到主存中，然后在使用 GPU 进行训练前，将主存中的数据拷贝至显存对象中，然后使用 GPU 进行运算。\n参考 `network_kernels.cu` 中的 `float *network_predict_gpu(network net, float *input)` 函数。\n\n在 Traffic counter 项目中，将来可能使用 GPU 版本的解码器，此解码器解码后的图像数据是在显存中的。因此，设想在将来的处理中，将略过从主存往显存拷贝数据这一步骤，直接传递显存中的对象，并进行处理。具体实现时，可重点参考 `float *network_predict_gpu(network net, float *input)` 函数。\n\n#### Confidence threshold\n\n#### NMS threshold\n","tags":["cpu"]},{"title":"OpenCV 4.0.1 + CUDA 8.0 + Visual Studio 2015 + Win10","url":"/2019/05/20/Build-opencv-with-GPU/","content":"本文主要记录了使用 CUDA 8.0，Visual Studio 2015，Win10 来编译 OpenCV 4.0.1 的步骤。\n<!-- more -->\n## 安装流程\n>[1].   安装好 CUDA 8.0. 各种路径添加到环境变量。\n\n****\n>[2]. 下载 Opencv 源码到 `<OpenCV_DIR>` 及 对应版本的 opencv_contrib 到 `<OpenCV_CONTRIB_DIR>`.\n\n****\n>[3]. Cmake 生成 VS solution。\n&emsp;&emsp;3.1 打开 cmake. 在 `where is the source code` 中填入 `<OpenCV_DIR>`, 在 `Where to build the binaries` 中 填入 `<OpenCV_DIR/build>`。\n&emsp;&emsp;3.2 点击 configure, 选择 `Visual Studio 14 2015 Win64`. (一定要选带有 Win 64 字样的，否则会出错)\n&emsp;&emsp;3.3 点击 Finish。会进行 Configure。中途可能会跳出红色错误，这是由于下载 ffmpeg, ippicv, data, xfeatures2d 相关的文件失败造成的。报错信息里会提示查看 log 文件，打开 log 文件后，根据信息，手动到网址下去下载 dll, zip, cmake 等文件，下载好后，以 `<MD码>-<name>.<ext>` 的方式命名，放在 `<OpenCV_DIR/.cache>` 下的相关路径中。重新点击 configure.\n&emsp;&emsp;3.4 勾选中复选框 `BUILD_opencv_world`, `WITH_CUDA`, `OPENCV_ENABLE_NONFREE`. 将 `<OpenCV_CONTRIB_DIR/modules>` 路径添加到 `OPENCV_EXTRA_MODULES_PATH` 中，再次点击 Configure.\n&emsp;&emsp;3.5 勾选 `CUDA_FAST_MATH`, 点击 Configure. 屏幕上应该一片白，没有红色信息.\n&emsp;&emsp;3.5 点击 Generate 以生成 sln. 此过程不应报错。\n\n****\n>[4]. VS 编译 Opencv.sln\n&emsp;&emsp;4.1 `<OpenCV_DIR/build>` 下打开 OpenCV.sln, 点击 生成 -> 配置管理器，选择 `Release`, `x64`.\n&emsp;&emsp;4.2 将 `color_detail.hpp` 的 `96-127` 行的 **`const`** 替换为 **`constexpr`**. 否则会出现 `error : dynamic initialization is not supported for a __constant__ variable` 的错误. [来源1](https://github.com/opencv/opencv/issues/13491#issuecomment-450754826)  [来源2](https://answers.opencv.org/question/205673/building-opencv-with-cuda-win10-vs-2017/)\n&emsp;&emsp;4.3 生成 `ALL_BUILD`. (主要关注 opencv_world 工程，此工程生成成功即可，实际过程中出现了 opencv_perf_gapi 和 opencv_test_gapi 工程报错的问题，貌似对我们的项目没有影响。)\n&emsp;&emsp;4.4 成功生成 `ALL_BUILD` 后 (生成了 opencv_world401.lib 和 opencv_world401.dll)，右键 `INSTALL` -> `仅用于项目` -> `仅生成INSTALL`.\n&emsp;&emsp;4.5 在 `<OpenCV_DIR/install>` 下是最后得到的 lib, dll，头文件等。\n\n## 参考文档\n[【OpenCV】opencv4.0.1+opencv_contrib4.0.1+VS2015的编译](https://blog.csdn.net/Gordon_Wei/article/details/85775328)\n","tags":["gpu"]},{"title":"Pyinstaller with Scipy","url":"/2019/04/02/Pyinstaller-with-Scipy/","content":"在将 python 工程用 pyinstaller 打包成 exe 时，由于 import 了 scipy 的一些功能，因此生成的 exe 一直报错，经过努力终于解决。\n<!-- more -->\n## 问题来源\n在 MLIT 的项目中，有使用到 scipy 库中的一个函数。因此，有 `from scipy.spatial.distance import cdist` 这句，但是突然不知道哪里发生了改动 (推测是某些库的版本在安装其他库时发生了变化)，打包好的 exe 在执行上述 import 语句时总是报错。\n\n## Trail and Error\n1 | 由于 scipy 的版本在上次发版之后发生了改变（不知为啥），现在的版本为 1.2.0. 根据 `lib/site-packages/` 的痕迹推测之前的版本为 1.0.0. 但将 scipy 重装为 1.0.0 后仍然不成功。\n2 | 按步骤 1 的方式操作后，`scipy/spatial/_spherical_voronoi.py` 中的 第 18 行 `from . import _voronoi` 仍然报错，大意为 `cannot import name _voronoi`. 其中，`_voronoi` 为 `scipy/spatial/`文件夹下的一个 pyd 文件，为 `_voronoi.pyd`. 可是事实上，我在 convert 的 bat 脚本中明明有通过 `hidden-import` 将此文件导入进去，生成的文件夹中也确实存在这个文件，但是程序总是无法成功导入。\n3 | 后来经过搜索与分析，在 convert 脚本中添加了一行 `--paths=\"H:\\Develop\\Anaconda2\\setup\\Lib\\site-packages\\scipy\\extra-dll\"`，就成功了。\n\n## 分析\n上述方法成功后，分析成功原因。在执行 convert 脚本时，有一系列跟 scipy 有关的 warning. 大致都是说 hidden-import 的 pyd 文件的依赖 dll 找不到，而这些 warning 在我印象中以前并未出现过。在添加了 `--paths=\"H:\\Develop\\Anaconda2\\setup\\Lib\\site-packages\\scipy\\extra-dll` 这一参数后，不再有这些 warning, exe 也可以成功执行。\n\n## 资源\n[完整的 convert 脚本](convert_server.bat)\n[python库版本](mlit_pkg_version_20190402.txt)\n","tags":["scipy"]},{"title":"光流法","url":"/2019/02/25/optical-flow/","content":"对光流法进行了一个大致的了解，并了解了在高速情况下通过图像金字塔进行光流计算的方法。\n<!-- more -->\n## 光流的定义\n**光流** 是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。一般而言，光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。\n\n## 光流法的原理\n### 光流法的基本假设\n1. <font color=\"orange\">亮度恒定不变，</font>即同一目标在不同帧间运动时，其亮度不会发生改变。这是基本光流法的假定（所有光流法变种都必须满足），用于得到光流法基本方程\n2. <font color=\"orange\">时间连续或运动是“小运动”，</font>即时间的变化不会引起目标位置的剧烈变化，相邻帧之间位移要比较小。同样也是光流法不可或缺的假定。\n\n### 基本约束方程\n根据光流法的基本假设，可以推导得出光流法的基本约束方程。\n&emsp;&emsp;考虑一个三维的矩阵 $I$  其三个维度为 $x$, $y$, $t$。$x$,$y$ 为图像的两个维度，$t$ 为时间维度。$I(x,y,t)$ 表示 $t$ 时刻的图像在 $(x,y)$ 坐标上的灰度值。\n&emsp;&emsp;根据两个基本假设，可得到方程\n$$I(x,y,t) = I(x+dx, y+dy, t+dt)$$\n由于是小运动，因此可对 $I(x+dx, y+dy, t+dt)$ 进行泰勒展开，即\n$$I(x+dx, y+dy, t+dt) = I(x,y,t) + \\frac{\\partial I}{\\partial x}dx + \\frac{\\partial I}{\\partial y}dy + \\frac{\\partial I}{\\partial t}dt+\\epsilon$$\n结合上式，得到\n$$\\frac{\\partial I}{\\partial x}dx + \\frac{\\partial I}{\\partial y}dy + \\frac{\\partial I}{\\partial t}dt=0$$\n令 $u=\\frac{dx}{dt}$ $v=\\frac{dy}{dt}$, 得到\n$$\\frac{\\partial I}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial I}{\\partial y}\\frac{dy}{dt} + \\frac{\\partial I}{\\partial t}=0$$\n即\n$$I_x u+I_y v=-I_t$$\n这就是光流的基本约束方程。\n\n### Lucas-Kanade 光流算法\n&emsp;&emsp;由于光流的基本约束方程 $I_x u+I_y v=-I_t$ 只有一个约束，但是却有两个未知数，因此无法求解。为了能够求解出 $(u,v)$, 需要引入新的约束。\n&emsp;&emsp;Lucas-Kanade 光流算法引入了 <font color=\"orange\">**空间一致**</font> 假设，即所有的相邻像素有相似的行动。也即在目标像素周围 $m\\times m$ 的区域内，每个像素均拥有相同的光流矢量。以此假设解决式 $$I_x u+I_y v=-I_t$$ 无法求解的问题。\n\n![LK光流法](LK.png)\n具体推导过程，参考 [总结：光流--LK光流--基于金字塔分层的LK光流--中值流](https://blog.csdn.net/sgfmby1994/article/details/68489944)\n\n<br>\n## 基于金字塔分层的 LK 光流法\n&emsp;&emsp;根据光流的基本假设 2，光流适用于 **小运动** 场景，即相邻帧之间运动较小。因此，光流法无法直接处理运动较大的情况。因此，在处理运动较大的情况时，需要通过图像金字塔的方式。\n具体细节，同样参考 [总结：光流--LK光流--基于金字塔分层的LK光流--中值流](https://blog.csdn.net/sgfmby1994/article/details/68489944)\n\n<br>\n## 基于光流的运动目标检测（前景检测）算法\n![基于光流的运动目标检测（前景检测）算法流程图](fg.png)\n参考这篇文章 [计算机视觉--光流法(optical flow)简介](https://blog.csdn.net/qq_41368247/article/details/82562165)\n\n<br>\n## Reference\n[总结：光流--LK光流--基于金字塔分层的LK光流--中值流](https://blog.csdn.net/sgfmby1994/article/details/68489944)\n[计算机视觉--光流法(optical flow)简介](https://blog.csdn.net/qq_41368247/article/details/82562165)\n[【计算机视觉】光流法简单介绍](https://blog.csdn.net/jobbofhe/article/details/80448961)\n","tags":["optical flow"]},{"title":"协方差及协方差矩阵","url":"/2019/01/25/covariance-and-covariance-matrix/","content":"本文主要关注了协方差的定义、含义，协方差矩阵的性质以及 PCA 与协方差矩阵的关系\n<!-- more -->\n关于协方差的定义，这篇文章解释得很好\n[终于明白协方差的意义了](https://blog.csdn.net/northeastsqure/article/details/50163031)\n\n关于协方差矩阵的特征值特征向量和 PCA 的关系。这篇文章解释得很好。\n[PCA算法是怎么跟协方差矩阵/特征值/特征向量勾搭起来的?](https://www.cnblogs.com/dengdan890730/p/5495078.html)\n","tags":["主成分分析"]},{"title":"Centos 系统下深度学习环境配置及 tensorflow 安装","url":"/2018/12/07/linux-install-setting-process/","content":"本文主要记录了在新安装完 Centos 系统后的 NVIDIA 显卡驱动、cuda、cudnn、以及 TensorFlow 等 python 库的安装及配置。\n<!-- more -->\n更改启动设置\n>(1) 查看系统默认启动环境。`systemctl get-default` -> `graphical.target`\n>(2) 将默认启动环境设置为命令行。`systemctl set-default multi-user.target`\n>(3) `reboot`\n\n\n禁用系统自带的 nouveau 显卡驱动。\n>(4) run the NVIDIA driver file, it will create two `.conf` file to disable the nouveau driver for you under `/etc/modeprobe.d` and `/usr/...`\n\n重做内核镜像\n>(5) `mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img`  # Backup image\n>(6) `dracut /boot/initramfs-$(uname -r).img $(uname -r)` # Create a new image\n>(7) `reboot`\n\n安装 NVIDIA 显卡驱动\n>(8) run the NVIDIA driver file again, finish the installation of NVIDIA driver.\n\n安装 Cuda\n>(9) Install cuda 8.0, do not install the driver provided by cuda since we already installed the NVIDIA driver before.\n>(10) add ```export PATH=/usr/local/cuda-8.0/bin:$PATH``` and `export LD_LIBRARY_PATH=/usr/local/cuda-8/lib64:$LD_LIBRARY_PATH` to `~/.bashrc`\n>(11) `source ~/.bashrc`\n\n解压 cudnn\n>(12) `tar zxvf cudnn.tgz` # extract cudnn files\n>    copy extracted files to the corresponding folders under cuda installation directory\n\n安装 Anaconda/python\n>(13) Install Ananconda. Do not add the Ananconda path to `/root/.bashrc`, add it to `~/.bashrc`.\n>(14) `source ~/.bashrc`\n\n安装 tensorflow 或其他 python 库\n>(15) `pip install tensorflow-gpu==1.0.0`. # Install GPU version tensorflow 1.0.0\n\n注意，安装 `1.0.0` 的 tensorflow 时，`numpy==1.16.2` 貌似会报错。因此，需要 check 一下 Anaconda 中 numpy 的版本。如果需要安装，则安装 `pip install numpy==1.14.2`。\n"},{"title":"Windows 下利用共享内存实现进程间通信","url":"/2018/12/03/Inter-Process-Communication-by-Shared-Memory-on-Windows/","content":"本文主要记载了在 Windows 下如何通过调用 WinApi ，利用 `共享内存` 实现 进程间通信 (Inter Process Communication, IPC).\n<!-- more -->\n&emsp;&emsp; 本文主要是通过 FileMapping 的方式实现进程间通信。目前已实现进程间共享字符串以及 opencv 的 Mat 对象 （以 `uchar *` 方式）。\n\n<br>\n## 两个 C++ 进程之间的通信\n参考文章 [C++共享内存实现（windows和linux）](https://blog.csdn.net/u012234115/article/details/82114631)\n\n上面的文章只实现了字符串的进程间通信，根据实际需要，需要进程间传递图像对象，此处使用 opencv 的 `Mat` 对象来表示。由于 `Mat` 对象有一个 `uchar* data` 属性，指向 Mat 的实际数据。因此，这里通过传输 `uchar* data` 到共享内存，并在 Reader 中重建 `Mat` 对象 (C++)。\n参考文章 [Windows进程间通信--共享内存映射文件（FileMapping）](https://www.cnblogs.com/Lalafengchui/p/4223584.html).\n\n**注意：**这里在传递时需要将图像的 长、宽、高 等参数一并传递，这样在 Reader 中可以根据读到的尺寸进行恢复。因为在 Reader 端一开始得到的是共享内存的首地址。C++ 中用三个 `int` 类型来表示 `width`, `height`, `channel`。这三个 `int` 是写到表示数据的 `uchar *` 的最前面的。\n```cpp\n// code snippet in img_reader.cpp\nint width, height;\nuchar* tmp = (uchar*) malloc(buffer_size);\nmemcpy(tmp, &width, sizeof(width)) // 将 width 首地址开始的 4 个 byte 内容拷贝到 tmp 所指向的地址。\ntmp += sizeof(width); // tmp 向后移动 sizeof(width) 个 uchar 的长度。\nmemcpy(tmp, &height, sizeof(height))\ntmp += sizeof(height)\n```\n<br>\n## C++ 和 python 进程之间的通信\n*C++ 作 Writer， python 作 Reader*\n参考文章：[Shared Memory Example (Python, ctypes, VC++)](https://mail.python.org/pipermail/python-list/2005-March/302108.html)。\npython 中 使用 `ctypes` 来调用 `win32 api`，方法与 C++ Reader 中的类似。区别在于获取共享内存的首地址后，python 中可以通过 slicing 的方式获取不同地址的值。最后得到 width, height, channel 以及数据，将数据转换成 ndarray 即可使用 opencv 显示。\n\n<br>\n## FileMapping 原理\n这里使用了 FileMapping 的方式进行 IPC。\nWriter 使用的 API 有\n>[CreateFile](https://docs.microsoft.com/en-us/windows/desktop/api/fileapi/nf-fileapi-createfilea)\n>[CreateFileMapping](https://docs.microsoft.com/en-us/windows/desktop/api/winbase/nf-winbase-createfilemappinga)\n>[MapViewOfFile](https://msdn.microsoft.com/en-us/library/windows/desktop/aa366761%28v=vs.85%29.aspx?f=255&MSPPError=-2147217396)\n `MapViewOfFile` 最后返回的是共享内存的指针。通过将数据 `memcpy` 到这里实现共享。\n\nReader 使用的 API 有\n>[OpenFileMapping](https://docs.microsoft.com/en-us/windows/desktop/api/winbase/nf-winbase-openfilemappinga)\n>[MapViewOfFile](https://msdn.microsoft.com/en-us/library/windows/desktop/aa366761%28v=vs.85%29.aspx?f=255&MSPPError=-2147217396)\n`MapViewOfFile` 最后返回的是共享内存的指针。可通过从这里读数据重建图像。\n\n关于以上提及 API 的详细解释：\n>[Windows核心编程-CreateFile详解](https://blog.csdn.net/bxsec/article/details/76566011)\n\n<br>\n## File Mapping 代码\n```cpp\n// img_writer.cpp\n#include <Windows.h>\n#include <iostream>\n#include <string>\n#include <thread>\n#include <chrono>\n#include <vector>\n#include <opencv2\\core\\core.hpp>\n#include <opencv2\\highgui\\highgui.hpp>\n\nusing namespace std;\nusing namespace cv;\n\n//struct MyData\n//{\n//\tint width;\n//\tint height;\n//\tint channel;\n//\tMyData(int _width, int _height, int _channel) : width(_width), height(_height), channel(_channel)\n//\t{}\n//};\n\nvoid writeMemory(char* imgDir) {\n\t// define shared data\n\tchar *shared_file_name = \"file_name_sean\";\n\tchar * shared_object_name = \"Local\\\\object_name_sean\";\n\n\tMat img = imread(imgDir, IMREAD_COLOR);\n\n\tint width = img.cols;\n\tint height = img.rows;\n\tint channel = img.channels();\n\tuchar* img_data = img.data;\n\n\tunsigned long data_size = sizeof(uchar) * width * height * channel;\n\tunsigned long buffer_size = data_size + 3 * sizeof(int);\n\n\tcout << \"share buffer \" << endl;\n\n\t// create shared memory file\n\tHANDLE hFile = CreateFile(shared_file_name,\n\t\tGENERIC_READ | GENERIC_WRITE,\n\t\tFILE_SHARE_READ | FILE_SHARE_WRITE,\n\t\tNULL,\n\t\tOPEN_ALWAYS, // open exist or create new, overwrite file\n\t\tFILE_ATTRIBUTE_NORMAL,\n\t\tNULL);\n\n\tif (hFile == INVALID_HANDLE_VALUE)\n\t\tcout << \"create file error\" << endl;\n\n\tHANDLE shared_file_handler = CreateFileMapping(\n\t\thFile, // Use paging file - shared memory\n\t\tNULL,                 // Default security attributes\n\t\tPAGE_READWRITE,       // Allow read and write access\n\t\t0,                    // High-order DWORD of file mapping max size\n\t\tbuffer_size,            // Low-order DWORD of file mapping max size\n\t\tshared_object_name);    // Name of the file mapping object\n\n\tif (shared_file_handler) {\n\t\t// map memory file view, get pointer to the shared memory\n\t\tLPVOID lp_base = MapViewOfFile(\n\t\t\tshared_file_handler,  // Handle of the map object\n\t\t\tFILE_MAP_ALL_ACCESS,  // Read and write access\n\t\t\t0,                    // High-order DWORD of the file offset\n\t\t\t0,                    // Low-order DWORD of the file offset\n\t\t\tbuffer_size);           // The number of bytes to map to view\n\n\t\t// write width, height, channel to a memory\n\t\tuchar* tmp = (uchar*) malloc(buffer_size);\n\t\tmemcpy(tmp, &width, sizeof(width));\n\t\ttmp += sizeof(width);\n\t\tmemcpy(tmp, &height, sizeof(height));\n\t\ttmp += sizeof(height);\n\t\tmemcpy(tmp, &channel, sizeof(channel));\n\t\ttmp += sizeof(channel);\n\t\tmemcpy(tmp, img.data, data_size);\n\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tuchar val = tmp[i];\n\t\t\tcout << (int)val << endl;\n\t\t}\n\n\t\t// copy data to shared memory\n\t\ttmp -= sizeof(int) * 3;\n\t\tmemcpy(lp_base, tmp, buffer_size);\n\n\t\tfree(tmp);\n\n\t\tFlushViewOfFile(lp_base, buffer_size); // can choose save to file or not\n\n\t\t// process wait here for other task to read data\n\t\tcout << \"already write to shared memory, wait ...\" << endl;\n\n\t\tthis_thread::sleep_for(chrono::seconds(6000));\n\n\t\t// close shared memory file\n\t\tUnmapViewOfFile(lp_base);\n\t\tCloseHandle(shared_file_handler);\n\t\tCloseHandle(hFile);\n\t\tcout << \"shared memory closed\" << endl;\n\t} else\n\t\tcout << \"create mapping file error\" << endl;\n}\n\nint main(int argc, char* argv[]) {\n\tchar* imgDir = \"C:/Users/taoxuan.G08/Documents/Visual Studio 2015/Projects/cnpy-solution/cv/vehicle.jpeg\";\n\twriteMemory(imgDir);\n\n\treturn 0;\n}\n\n```\n\n```cpp\n// img_reader.cpp\n#include <iostream>\n#include <string>\n#include <Windows.h>\n#include <opencv2\\core\\core.hpp>\n#include <opencv2\\highgui\\highgui.hpp>\n\nusing namespace std;\nusing namespace cv;\n\nvoid readMemory() {\n\tchar *shared_object_name = \"Local\\\\object_name_sean\";\n\n\t// open shared memory file\n\tHANDLE shared_file_handler = OpenFileMapping(\n\t\tFILE_MAP_ALL_ACCESS,\n\t\tNULL,\n\t\tshared_object_name);\n\n\tif (shared_file_handler) {\n\n\t\tLPVOID lp_base = MapViewOfFile(\n\t\t\tshared_file_handler,\n\t\t\tFILE_MAP_ALL_ACCESS,\n\t\t\t0,\n\t\t\t0,\n\t\t\t0);\n\n\t\t// copy shared data from memory\n\t\tcout << \"read shared data: \" << endl;\n\n\t\tuchar* tmp = (uchar*)lp_base;\n\t\tint width;\n\t\tint height;\n\t\tint channel;\n\t\tmemcpy(&width, tmp, sizeof(width));\n\t\ttmp += sizeof(int);\n\t\tmemcpy(&height, tmp, sizeof(height));\n\t\ttmp += sizeof(int);\n\t\tmemcpy(&channel, tmp, sizeof(channel));\n\t\ttmp += sizeof(int);\n\n\t\tMat img = Mat(height, width, CV_8UC3);\n\t\tmemcpy(img.data, tmp, height * width * channel);\n\n\t\tnamedWindow(\"test\");\n\t\timshow(\"test\", img);\n\t\twaitKey(2);\n\n\t\t// close share memory file\n\t\tUnmapViewOfFile(lp_base);\n\t\tCloseHandle(shared_file_handler);\n\n\t} else\n\t\tcout << \"open mapping file error\" << endl;\n}\n\nint main(int argc, char* argv[]) {\n\treadMemory();\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n```python\n# img_reader.py\nimport cv2\nimport numpy as np\nfrom ctypes import *\n\n\nFILE_MAP_ALL_ACCESS = 0xF001F\nszName = c_char_p('Local\\object_name_sean')\n\nhMapObject = windll.kernel32.OpenFileMappingA(FILE_MAP_ALL_ACCESS, False, szName)\nprint \"Error after OpenFileMappingA ->\", GetLastError()  # If everything runs smoothly, GetLastError should return 0\nif hMapObject == 0:\n    print 'Could not open file mapping object'\n    raise windll.WindowsError()\n\npBuf = windll.kernel32.MapViewOfFile(hMapObject, FILE_MAP_ALL_ACCESS, 0, 0, 0)\nprint \"Error after MapViewOfFile ->\", GetLastError()  # If everything runs smoothly, GetLastError should return 0\n\nif pBuf == 0:\n    print 'Could not map view of file'\n    windll.kernel32.GetLastError()\nelse:\n    pBuf_int = cast(pBuf, POINTER(c_int))\n    width, height, channel = pBuf_int[:3]\n\n    data_len = width * height * channel\n    pBuf_ubyte = cast(pBuf, POINTER(c_ubyte))\n    data_ubyte = pBuf_ubyte[: data_len]  # The pointer is already incremented by 12 bytes, not sure why.\n\n    image = np.asarray(data_ubyte, dtype=np.uint8)\n    image = image.reshape((height, width, channel))\n\n    cv2.imshow('python image', image)\n    cv2.waitKey()\n\nwindll.kernel32.UnmapViewOfFile(pBuf)\nwindll.kernel32.CloseHandle(hMapObject)\n```\n\n[img_writer in C++](img_writer.cpp) [img_reader in C++](img_reader.cpp) [img_reader in python](img_reader.py)\n\n*<font color='orange'>不知为什么，python 版的 reader 用脚本执行正常，但是用 pyinstaller 打包成 exe 后就无法获取共享内存的指针，`MapViewOfFile` 始终返回 0，正在解决这个问题。</font>*\n\n*<font color='green'>除此之外，发现 python 版中在执行完 `OpenFileMappingA` 和 `MapViewOfFile` 后，分别调用 `GetLastError` 都应该返回 0 (0 表示没有错误)。C++ 版本的 reader 就是如此。但是 `img_reader.py` 用脚本方式执行却都返回 2，按理说返回 2 了就表示有错，应该无法正确读取共享内存数据，但是用脚本方式执行却可以读取图像数据。这一点很奇怪。而将 `img_reader.py` 转换成 exe 后，在 `OpenFileMappingA` 后调用 `GetLastError` 返回 2，`MapViewOfFile` 后返回 6，也无法正确读取图像数据。</font>*\n\n\n由于现有的方法无法在转成 exe 后成功运行，考虑使用新的方法来读取共享内存。这里使用了 `mmap` 库。\n```python\nimport mmap\nimport struct\nimport numpy as np\nimport cv2\n\n\nshm = mmap.mmap(0, 720*480*3+12, \"object_name_sean\", mmap.ACCESS_READ)\n\nprint shm\nif shm:\n    header = shm.read(12)\n    width_str = header[:4]\n    height_str = header[4:8]\n    channel_str = header[8:12]\n\n    width = struct.unpack('<i', width_str)[0]\n    height = struct.unpack('<i', height_str)[0]\n    channel = struct.unpack('<i', channel_str)[0]\n\n    print width, height, channel\n\n    data = shm.read(width * height * channel)\n\n    img_data = [ord(x) for x in data]\n    print img_data[:10]\n\n    image = np.asarray(img_data, dtype=np.uint8)\n    image = image.reshape((height, width, channel))\n\n    cv2.imshow('python image', image)\n    cv2.waitKey()\n\nshm.close()\n```\n[img reader in python mmap](reader_mmap.py)\n","tags":["进程通信"]},{"title":"Visual Studio 环境变量配置","url":"/2018/11/26/Visual-Studio-Configuration/","content":"本文主要记录了一些关于 Visual Studio 的环境变量的配置，本文使用的版本是 VS 2015.\n<!-- more -->\n## VS 的项目结构\n&emsp;&emsp;Visual Studio 主要由两层结构，最大的一层是 `解决方案 (Solution)`，一个 `解决方案` 可以包含多个 `项目 (Project)`。可以在一个已有的 `Solution` 里添加 `Project`.\n\n<br/>\n## VS 平台选项\n&emsp;&emsp;VS 由 `Debug` 和 `Release` 两种模式，一般在开发阶段都使用 Debug，而在最后的发布阶段使用 Release。在目标平台选项主要有 `x64` 和 `x86` 两种。模式和平台相互组合就会产生 4 种方式。可根据需要进行环境变量的配置。\n\n<br/>\n## 项目属性配置\n&emsp;&emsp;在某个项目上(是项目，非解决方案)`右键`，`属性`，选中左侧 `VC++目录`，右侧会出现一些路径的配置。主要有 `可执行文件目录`，`包含目录`，`引用目录`，`库目录`等等。在安装 VS 的时候，VS 已经将一些依赖的目录命名到一些宏了（就是类似于系统的环境变量之类的变量，与 C++ 的宏不一样），因此上面这些 `可执行文件目录` 等都包含了一些宏，作为默认的查找路径，如果自己的项目需要额外引入其他的依赖，则需要在对应的目录里添加自己的路径。比如，C++ 中调用 Python 的项目需要 `Python.h` 和 `python27.lib`，因此就将这两个文件所在的路径分别添加进 `包含目录` 和 `库目录`。上面的每一个目录都是和环境变量的某个变量对应的。比如，`包含目录` 与 环境变量 `INCLUDE` 对应，`库目录` 与 环境变量 `LIB` 对应。\n**Note:** 当选用 `Debug` 模式时，编译的时候会报找不到 `python27_d.lib` 的错，这是因为我们下载的 python 都是 Release 版本的。因此只有 `python27.lib` 没有 `python27_d.lib`。[解决方法](https://blog.csdn.net/Chris_zhangrx/article/details/78947526)\n\n\n<br/>\n## 包含目录、附加包含目录以及库目录和附加库目录的区别\n在 VS 中，右键一个 Project，可以发现有两个地方设置 `Include` 的相关目录：\n\n>VC++ Directories -> Include Directories\n>C/C++ -> General -> Additional Include Directories\n\n同理，设置 Lib 也有两个地方\n\n>VC++ Directories -> LibraryDirectories\n>Linker -> General -> Additional Library Directories\n\n应该如何设置呢？\nMSDN 对这两个条目的解释如下\n>\"VC++ Directories -> Include Directories\" : Directory settings displayed in the window are the directories that Visual Studio will search for include files referred to in your source code files. Corresponds to environment variable INCLUDE.\n>\"C/C++-> General -> Additional Include Directories\": The directory to be added to the list of directories searched for include files.\n\n编译器在编译过程中查找包含目录（**Include** 文件）的顺序：\n>The compiler searches for directories in the following order:\n1.&emsp;Directories containing the source file.\n2.&emsp;Directories specified with the **/I** option, in the order that CL encounters them.\n3.&emsp;Directories specified in the **INCLUDE** environment variable.\n\n其中 step2 中的 `/I` 是由 `C/C++ -> General -> Additional Include Directories` 设置的。\n而 step3 中的 `INCLUDE` 是由 `VC++ Directories -> Include Directories` 设置的。\n所以从这里看出，不同的设置有不同的编译链接顺序。\n\n因此，总结出两种设置方法：\n\n>VC++ Directories -> Include Directories 配合 VC++ Directories -> LibraryDirectories\n>C/C++ -> General -> Additional Include Directories 配合 Linker -> General -> Additional Library Directories\n\n但是要注意，由于编译顺序，这种用法情况下需要确保在 `VC++ Directories -> Include Directories` 中填入 `$(IncludePath)` (继承其他 Include 路径).\n\nLib 的设置与 Include 同理。\n\n<br/>\n## 参考文章\n[visual studio配置中包含目录和附加包含目录的区别以及auto-linking](https://blog.csdn.net/u012234115/article/details/54233095)\n[VS属性配置和auto-linking](https://blog.csdn.net/zcedar/article/details/51444343)\n","tags":["发行"]},{"title":"相机成像原理和参数标定","url":"/2018/11/15/camera-calibration/","content":"最近学习了相机的成像原理和参数标定，将参考文献记录如下。\n<!-- more -->\n## 成像原理\n简化后的相机模型和针孔相机的成像原理很相似，因此我们把简化后的相机模型称为针孔相机模型。\n具体内容参考[相机成像过程的简化与建模](https://zhuanlan.zhihu.com/p/30813733)\n<br>\n\n## 世界坐标到相机坐标变换\n[刚体变换](https://zhuanlan.zhihu.com/p/23090593)\n[罗德里格斯旋转公式](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula)\n[仿射变换](https://www.matongxue.com/madocs/244.html)\n\n## 相机坐标到数字图像坐标，畸变矫正\n[相机标定究竟在标定什么？](https://zhuanlan.zhihu.com/p/30813733)\n\n## 心得\n1. 由于针孔相机模型的特性（射线），CCD 上的某个点可能对应了一个射线上的所有点，因此无法只通过相机内参将像素坐标 $(u,v)$ 映射为相机坐标 $(x_c, y_c, z_c)$。因而无法计算出两个像素点之间的实际物理距离。（远处的大物体移动大距离和近处的小物体移动小距离可能成像结果完全一样）\n","tags":["成像原理"]},{"title":"tensorflow-1.x-cookbook 读书笔记","url":"/2018/11/14/tensorflow-1-x-cookbook/","content":"本文是关于 《TensorFlow 1.x Deep Learning Cookbook》 的读书笔记。\n<!-- more -->\n<br>\n## Hello world in TensorFlow\n1. 设置 tensorflow 输出日志等级\n```python\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n```\n\n2. TensorFlow 与设备\n>TensorFlow allows you to use specific devices (CPU/GPU) with different objects of the computation graph using with `tf.device()`.\n\n\n## Working with constants, variables, and placeholders\n3. Constants 和 Variables 的存储方式\n>Constants are stored in the computation graph definition; they are loaded every time the graph is loaded. In other words, they are memory expensive. Variables, on the other hand, are stored separately; they can exist on the parameter server.\n\n4. 设置全局 seed\n> When there are large numbers of random tensors in use, we can set the seed for all randomly generated tensors using tf.set_random_seed(); the following command sets the seed for random tensors for all sessions as 54: `tf.set_random_seed(54)`\n\n5. 以某种分布初始化 Variable\n```python\nrand_t = tf.random_uniform([50,50], 0, 10, seed=0)\nt_a = tf.Variable(rand_t)\nt_b = tf.Variable(rand_t)\n```\n\n6. 将 Variable 作常量使用以优化内存 (Constant 存储在 graph 中，Variable 不是)\n```python\n t_large = tf.Variable(large_array, trainable = False)\n```\n\n## Performing matrix manipulations using TensorFlow\n7. elementwise operations\n>All arithmetic operations of matrices like add, sub, div, multiply (elementwise multiplication), mod, and cross require that the two tensor matrices should be of the same data type.\n\n## Invoking CPU/GPU devices\n8. TensorFlow naming CPU and GPU devices\n>TensorFlow names the supported devices as \"`/device:CPU:0`\" (or \"`/cpu:0`\") for the CPU devices and \"`/device:GPU:I`\" (or \"`/gpu:I`\") for the ith GPU device.\n"},{"title":"ORB 特征提取","url":"/2018/11/13/ORB-Feature/","content":"ORB 特征的提取过程\n<!-- more -->\n以下两篇博客讲解得不错，结合起来看更易理解。\n[ORB特征提取详解](https://blog.csdn.net/zouzoupaopao229/article/details/52625678)\n[见过的介绍ORB最清楚的博文](https://blog.csdn.net/yang843061497/article/details/38553765)\n","tags":["keypoint"]},{"title":"Faster RCNN 中自定义 Python 层的作用理解","url":"/2018/11/06/Faster-RCNN-Python-layer/","content":"本文主要分析了 Faster RCNN 中 `anchor_target_layer.py`, `proposal_target_layer.py` 和 `proposal_layer.py` 三个自定义 python 层的代码。\n<!-- more -->\n\n## 训练阶段的主要网络结构\n![训练阶段的主要网络结构](structure.PNG)\n\n## 测试阶段的主要网络结构\n![测试阶段的主要网络结构](structure_test.PNG)\n\n<br>\n## proposal_layer.py （训练，测试阶段都有）\n[source](https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/proposal_layer.py)\n输入： score, bbox_deltas, im_info\n1. 生成 anchors, 利用预测得到的 bbox_deltas 作为输入，对所有的 anchors 作回归，得到 proposals. (注意，这里生成 anchors 的方式与训练时 anchor_target_layer.py 里一致)\n2. 对超出原图的 proposals 进行 clip, 筛除尺寸过小的 proposals.\n3. 根据输入的 scores 进行排序，选取前 N 个保留。（eg. 6000）\n4. 对剩下的 proposals 进行 nms，筛除一部分 proposals.\n5. 再根据 scores 排序，选取前一部分 proposals. (eg. 300)\n6. 输出 proposals\n\n<br>\n## anchor_target_layer.py (仅在训练阶段)\n[source](https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/anchor_target_layer.py)\n输入: 'rpn_cls_score','gt_boxes','im_info', 'data'\n(1) 生成 所有的 anchors，记为 `all_anchors`，选出在图像内部的 anchor，记为 `anchors`.\n(2) 生成与 1 步中 `anchors` 同尺寸的 `labels`，初始化为 `-1`.\n(3) 计算 `anchors` 与 `gt_boxes` 的 IoU，得到 `overlaps`，得到每个 anchor 对应的 gt_box (IoU 最大).\n(4) 根据 RPN 正负样本选取规则 1，将每个 gt_box 的 IoU 最大 anchor 的 label 置为 `1`.\n(5) 根据 RPN 正负样本选取规则 2，将与任意 gt_box 的 IoU 大于某阈值的 anchor 的 label 置为 `1`. 将与所有 gt_box 的 IoU 小于某阈值的 achor 的 label 置为 `0`. 剩下的保留为 `-1`. 这样，就为每个 anchor 分配了标签。\n(6) 根据正负样本的数量限制，将一部分正样本（label 为 1）置为 `-1`. 负样本同样。\n(7) 计算 `anchors` 中每个 anchor 和 其对应的 gt_box 之间的 delta 作为 `bbox_targets`.\n   这样，上面计算得到的 `labels` 和 `bbox_targets` 其实就是 RPN 网络的 loss 的真值。\n(8) 将得到的 `anchors` 和 `bbox_targets` unmap 回原来的 `all_anchors` 中。这样，所有生成的 anchors 都有一个类别标记（-1，0，1）和 bbox_targets. (在 `all_anchors` 不在 `anchors` 中的 anchor 的 bbox_target 用 0 填充。label 用 -1 填充。这样在计算 RPN loss 时不会计算此 anchor 的 loss.(-1 被忽略))。\n(9) 将 `all_anchors` 的 `labels` 和 `bbox_targets` 输出，与预测的结果计算 rpn_loss.\n\n<br>\n## proposal_target_layer.py (仅在训练阶段)\n [source](https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/proposal_target_layer.py)\n接受 proposal_layer.py 的输出 和 gt_boxes (每个 GT box 的坐标和类别)作为输入，格式 (0, x1, y1, x2, y2)\n1. 将输入的 rois 和 gt_boxes 合并 (vstack)，形成总的 rois, 共 M 个。\n2. 计算每个 roi 和每个 gt_box 的 IoU，形成一个 matrix，设 gt_boxes 共有 N 个，则尺寸为 $M\\times N$.\n3. 找到每个 roi 对应的 IoU 最大的 gt_box 的索引，并将此 gt_box 的类别赋给此 roi. 这样每个 roi 都有一个物体类别。\n4. 选择一些 IoU 大于某阈值的 roi 作为 foreground，选择一些 IoU 在某区间内的 roi 作为 background。将作为 background 的 roi 的类别改为 0，用作最后 loss_cls 的计算。将 fg rois 和 bg rois 合并(vstack)，作为一个输出。记为 rois，用作 ROI Pooling.\n5. 计算这些 rois (第四部获得)于其对应的 gt_boxes 之间的 delta 作为位置预测的目标，用来计算 loss_bbox.\n","tags":["Python layer"]},{"title":"python 中关于字符的编码方式","url":"/2018/07/24/encoding-problem/","content":"在工作中遇到了有关字符编码的一个问题，在解决之后记录一下。\n<!-- more -->\n&emsp;&emsp;在一个深度学习的目标检测任务中，需要将检测出的结果输出到一个 xml 文件中，或者以 xml 的形式打印到控制台上。当向 xml 文件中传递的字符都是英文和数字时，不会有问题。但是当 xml 文件中出现中文或日文字符时，会出现一系列的编码问题。\n\n\n## 问题\n&emsp;&emsp;当设置 xml 某个元素的值时，若传递进去的为中文或日文，就会报错\n```python\nValueError: All strings must be XML compatible: Unicode or ASCII, no NULL bytes or control characters [8444] Failed to execute script runcaffe_fpn\n```\n这个错是由\n```python\nresource.text = image_name\n```\n这句话引发的，其中 image_name 是用 glob 函数从 Windows 系统中的某个路径下读进来的文件名。\n\n<br>\n## 分析和解决\n经过搜索发现，lxml 模块中，当对一个树的某个节点进行赋值时，其传入值必须是 Unicode 或 ASCII 码，而使用 glob 函数读进来的文件名并不是，因而造成了错误。\n\n经过研究发现，简体中文版的 Windows 系统使用 GBK 字符集，而不是 Unicode 字符集，因而传入的文件名不能直接赋值给 lxml 的节点对象。因此，需要对其进行转换，转换成 Unicode 码，再赋值给节点对象，也就是 `name = image_name.decode('gbk')` 或 `name = image_name.decode('Shift-JIS')`。\n\n这样之后不会报错了，但是在生成的 xml 文件中，中日字符都是以 unicode 码显示的，此时，只要将 `etree.tostring()` 函数的 `encoding=` 参数设置为 utf-8 即可。\n\n传入的文件名除了不能直接赋给 lmxl 的节点对象，直接进行 print 也是会乱码的（当 cmd 的 code page 与此字符串编码不一致时）。这是因为，传入的文件名的编码方式是由其代表文件所在的系统决定的，简体中文版 Windows 是 GBK，日文版是 Shift-JIS。当传入的字符串为 Shift-JIS 编码，但是尝试在 GBK 的 cmd 上打印出来时，就会报错。如下所示\n```\nC:\\Users\\sean\\Pictures\\01.嶥杫巗彫栰杫.png\nC:\\Users\\sean\\Pictures\\fc_barcelona___wallpaper_by_ccrt.png\nC:\\Users\\sean\\Pictures\\vs添加lib文件.PNG\nC:\\Users\\sean\\Pictures\\户口簿首页.png\nC:\\Users\\sean\\Pictures\\捕获.PNG\n```\n可以看出，第一个文件的文件名是乱码，这是因为它是 Shift-JIS 编码的，但是却尝试在 GBK 的 cmd 上打印出来，因此会报错。\n\n总结: python2 中，如果在文件开头声明了某种文件编码格式，那么此文件中<font color='orange'>**定义**</font>的字符串就是跟文件同样的编码格式。如果想要将此字符串打印到控制台上，则其编码必须跟控制台的编码一致。否则就需要手动进行 decode 成 unicode 进行打印。对于从系统中读进来的文件名，其编码格式是根据所在系统的编码格式决定的，与源码文件头部声明的编码格式无关。例如，以下代码中：\n\n```python\n# -*- coding:utf-8 -*-\nimport glob\nimport os.path as osp\n\nnames = glob.glob(osp.join('C:\\Users\\sean\\Pictures', '*.png'))\nfor name in names:\n    print name\n\ns = '路飞学院'\nprint s\n```\n`s` 变量的编码就是头部声明的 utf-8，而 `names` 中每个元素的编码则是根据路径下每个文件的编码格式而异的。\n\n\n值得注意的时，在 python2 中，字符串一共有两种类型，str 或 unicode。\n> ```python\n# -*- coding: utf-8 -*-\ns = '中文'  # 注意这里的 str 是 str 类型的，而不是 unicode\ns.encode('gb18030')\n```\n> 这句代码将 s 重新编码为 gb18030 的格式，即进行 unicode -> str 的转换。因为 s 本身就是 str 类型的，因此 Python 会自动的先将 s 解码为 unicode ，然后再编码成 gb18030。因为解码是 python 自动进行的，我们没有指明解码方式，python 就会使用 sys.defaultencoding 指明的方式来解码。很多情况下 sys.defaultencoding 是 ASCII，如果 s 不是这个类型就会出错。拿上面的情况来说，我的 sys.defaultencoding 是 ASCII，而 s 的编码方式和文件的编码方式一致，是 utf8 的，所以出错了:\n```\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe4 in position\n0: ordinal not in range(128)\n```\n>此问题的解决方案有以下两种:\n> 一是明确的指示出 s 的解码方式\n>```\n> # -*- coding: utf-8 -*-\n\ns = '中文'\ns.decode('utf-8').encode('gb18030')\n```\n>二是更改 `sys.defaultencoding` 为文件的编码方式\n```\n# -*- coding: utf-8 -*-\n\nimport sys\nreload(sys) # Python2.5 初始化后会删除 sys.setdefaultencoding 这个方法，我们需要重新载入\nsys.setdefaultencoding('utf-8')\n\nstr = '中文'\nstr.encode('gb18030')\n```\n\n注意: 在 pycharm 等 IDE 中，一般新建一个文件都是默认的 UTF-8 编码，而在文件中显式地声明一个字符串时(前面没加 u)，此字符串采用的就是与文件同样的编码方式，即 UTF-8. 也就是说，当你在一个 python 脚本里直接定义一个字符串常量的时候，此字符串的编码方式和环境编码方式相同。注意，如果一个 str 变量是从外部传过来的，如 glob 函数返回的，则此变量的编码方式可能会和源文件编码方式不一样。\n\n<br>\n## python 中的 str 和 unicode\n\n简体中文版 Windows 系统，编码方式为 GBK\n```python\n>>> a='你好'\n>>> a\n'\\xc4\\xe3\\xba\\xc3'\n>>> b=u'你好'\n>>> b\nu'\\u4f60\\u597d'\n>>> print a\n你好\n>>> print b\n你好\n>>> a.__class__\n<type 'str'>\n>>> b.__class__\n<type 'unicode'>\n>>> len(a)\n4\n>>> len(b)\n2\n```\n\n由以上代码段可以看出，直接定义一个字符串常量的时候，即 str 的时候，其编码方式为 GBK，是环境的编码方式，\n\n在一个系统编码为UTF-8的Linux环境下\n```python\n>>> a = '你好'\n>>> a\n'/xe4/xbd/xa0/xe5/xa5/xbd'\n>>> b = u'你好'\n>>> b\nu'/u4f60/u597d'\n>>> print a\n你好\n>>> print b\n你好\n>>> a.__class__\n<type 'str'>\n>>> b.__class__\n<type 'unicode'>\n>>> len(a)\n6\n>>> len(b)\n2\n```\n可以看出，在此 Linux 中，str 的编码方式为 utf-8，也是环境的编码方式。\n\nlen(string)返回string的字节数，len(unicode)返回的是字符数\n\n>print(string) 的时候，如果 string 是按当前环境编码方式编码的，可以正常输出，不会乱码；如果 string 不是当前环境编码的，就会乱码。而 print(unicode) 是不会乱码的。why？因为 print(unicode) 的时候，会把 unicode 先转成当前编码，然后再输出。我没看过 print 的源码，不过估计是这样的。\n\n关于 `如果 string 不是当前环境编码的，就会乱码。` 这句，可由一下代码证实:\ncoding.py\n```python\n# -*- coding:utf-8 -*-\ns = '路飞学院'\nprint s\n```\n在使用 GBK 的 terminal 中，输出为:\n```python\nC:\\Users\\sean\\PycharmProjects\\working>python coding.py\n璺瀛﹂櫌\n```\n这是因为，s 使用文件编码方式 utf-8 进行编码，而 terminal 是 gbk 编码的，因此会出现乱码。若在声明中使用 gbk，则不会出现乱码(已亲测)。\n\n<br>\n## python2 还是 python3\n以上所说的都是针对 python2，python3 针对编码部分进行了改进。\n```python\n# -*- coding:utf-8 -*-\ns = '路飞学院'\nprint(s)\n```\n以上这段代码，在 python2 中执行结果为\n```\nC:\\Users\\sean\\PycharmProjects\\working>python coding.py\n璺瀛﹂櫌\n```\n在 python3 中为:\n```python\nC:\\Users\\sean\\PycharmProjects\\working>D:\\Develop\\Anaconda2\\setup\\envs\\tensorflow\\python coding.py\n路飞学院\n\n```\n\n原因如下：\n>&emsp;&emsp;utf-8 编码之所以能在 windows gbk 的终端下显示正常，是因为到了内存里 python 解释器把 utf-8 转成了 unicode, 但是这只是 python3, 并不是所有的编程语言在内存里默认编码都是 unicode。比如，万恶的 python2 就不是，它的默认编码是 ASCII，想写中文，就必须声明文件头的 coding 为 gbk 或 utf-8, 声明之后，python2 解释器仅以文件头声明的编码去解释你的代码，加载到内存后，并不会主动帮你转为 unicode，也就是说，你的文件编码是 utf-8，加载到内存里，你的变量字符串就也是 utf-8 编码的。如果需要显示在 gbk 的终端下，就只能使用 decode 或 encode 函数转换成 unicode 或者再使用 gbk 进行编码。\n\n---\n>PY3 除了把字符串的编码改成了 unicode, 还把str 和 bytes 做了明确区分， str 就是 unicode 格式的字符， bytes 就是单纯二进制\n\n---\n>python2 的字符串其实更应该称为字节串。通过存储方式就能看出来，但 python2 里还有一个类型是 bytes ，难道又叫 bytes 又叫字符串？是的，在 python2 里，bytes == str ， 其实就是一回事。除此之外，python2 里还有个单独的类型 unicode , 把字符串解码后，就会变成 unicode。\n\n---\n>总之，Python只要出现各种编码问题，无非是哪里的编码设置出错了\n常见编码错误的原因有：\n- Python解释器的默认编码\n- Python源文件文件编码\n- Terminal使用的编码\n- 操作系统的语言设置\n\n>掌握了编码之前的关系后，挨个排错就好了。　　\n\n[这篇文章非常不错](http://www.cnblogs.com/huxi/archive/2010/12/05/1897271.html)，可以参考了解。\n<br>\n## 参考文章\n[https://blog.csdn.net/ktb2007/article/details/3876436](https://blog.csdn.net/ktb2007/article/details/3876436)\n[https://blog.csdn.net/ktb2007/article/details/3876429](https://blog.csdn.net/ktb2007/article/details/3876429)\n[https://www.zhihu.com/question/31833164/answer/381137073](https://www.zhihu.com/question/31833164/answer/381137073)\n[https://blog.csdn.net/abyjun/article/details/50190243](https://blog.csdn.net/abyjun/article/details/50190243)\n[永久修改 cmd 代码页方法](https://medium.com/@jackyu/windows-cmd-ae8d4f84f8bf)\n","tags":["xml"]},{"title":"将包含 caffe 的 python 工程打包成一个 exe 文件","url":"/2018/07/09/Convert-python-project-to-exe/","content":"由于工作原因，需要将一个基于 caffe 的深度学习检测工程转换成 exe 文件，在此过程中遇到了许多问题，现将其记录下来，以供参考。\n<!-- more -->\n\n\n## fpn_ported 改动\n### Caffe related\n1 caffe/pycaffe.py L13 `from ._caffe` 改成了 `from caffe._caffe`。因为最后 `dist\\runcaffe_fpn\\` 里有一个  `caffe._caffe.pyd` 文件，因此需要 import 此文件。（个人解释见下文）\n2 caffe/\\__init\\__.py L2 `from._caffe` 改成了 `from caffe._caffe`\ncaffe/\\__init\\__.py L3 `from._caffe` 改成了 `from caffe._caffe`\n3 caffe 中关于图像读入的部分都是用的 skimage 进行实现，但是似乎 pyinstaller 对 skimage 的支持并不好，因此需要对 caffe 中的相关部分用 cv2 进行替代。\n>My Python project includes A Caffe module which run a simple image classification process. One basic function is Caffe calling skimage.io to load image:\nhttps://github.com/BVLC/caffe/blob/master/python/caffe/io.py\n```python\ndef load_image(filename, color=True):\n    img = skimage.img_as_float(skimage.io.imread(filename, as_grey=not color)).astype(np.float32)\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        if color:\n            img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n```\n>I wonder if PyInstaller currently has a good support for Python package skimage. But from what I know by now, it doesn’t.\n\n>Run from Python source code files, it works fine. But when I packed all things into one single binary file, it can not load image at all. And after debugging and googleing for a long time – I always thought maybe I did something wrong – I get rid of this. PyInstaller hates skimage! So at last I use cv2 instead. And it works smoothly.\n```python\ndef cv2_load_image(filename, color=True):\n    img = cv2.imread(filename).astype(np.float32) / 255\n    if img.ndim == 3:\n        img[:,:,:] = img[:,:,2::-1]\n\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        if color:\n            img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n```\n>For all above details, please do check out PyInstaller Documentation: https://media.readthedocs.org/pdf/pyinstaller/latest/pyinstaller.pdf\n\n具体参考[这篇文章](https://handong1587.github.io/programming_study/2016/12/24/pyinstaller-and-others.html#continue-tackling-weird-stuffs)\n\n\n<br>\n### Numpy related\n1 numpy.core.\\__init\\__.py 中 `from .info import __doc__` 改成 `from numpy.core.info import __doc__`\n2 `from . import multiarray` 改成 `from numpy.core import multiarray`\n3 将 numpy.core.\\__init\\__.py 中其他的代表当前目录的 `.` 都替换成 `numpy.core`\n4 将 numpy.fft.fftpack.py 中的 `from . fft import fftpack_lite as fftpack` 改成 `from numpy.fft import fftpack_lite as fftpack`\n5.将 numpy.random.\\__init\\__.py 中的 `from .mtrand import *` 改成 `from numpy.random.mtrand import * `\n**个人观点：** 似乎无需像第3步一样将所有的 . 都改成绝对路径，貌似只有使用 `.` 导入 pyd 文件的导入语句会报找不到的错误。因此只需将导入 pyd 文件的语句更改即可。\n\n<br>\n### os, os.path related\n#### Problem\n&emsp;&emsp;打包后运行时，在最外层的 runcaffe_fpn.py 中 import os 可以成功，但是在  `models/rpn/proposal_layer.py` 和 `lib/config.py` 中 `import os` 就会报错 `no module named path`。\n\n<br>\n#### Analysis\n&emsp;&emsp;由于 os 和 os.path 的特殊性，即 os.py 中根据平台不同，import 不同的文件作为 path 模块，例如，在 Windows 中，使用 `import ntpath as path` 并将其命名为 os.path 模块。此外，os.py 和 ntpath.py 文件存在相互 import 的情况，不知为什么，工程中一些文件中，如 config.py 中，import os 语句会报错 no module namded path。\n\n<br>\n#### Solution\n&emsp;&emsp;~~由于 config.py 等文件中调用了 os.py 中的 makedirs 等函数以及 ntpath.py 中的一些函数。因此，尝试将 os.py 与 ntpath.py 文件解耦（其实是无法解耦的，因为 os.py 需要导入 ntpath.py 中的属性并供其函数使用），因此将 os.py 中 `import ntpath as path`  句注释，将 `sys.modules['os.path'] = path` 和 `from os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep, devnull)` 注释掉，换成 `from _ntpath import (curdir, pardir, sep, pathsep, defpath, extsep, altsep, devnull)` （`from _ntpath` 是因为如果直接使用 `from ntpath` 的话，它会自动调用 anaconda 下的 `ntpath.py` 文件，会导致前面的错误，因此，将 os.py 和 ntpath.py 文件拷贝至一个包下，并都在前面加上下划线进行重命名）。实际操作中，我是创建了一个名为 os_modified 的包，并将重命名后的 _os.py 和 _ntpath.py 文件放进去作为模块。在工程文件中调用 os 和 os.path 的地方，将 `import os` 和 `import os.path as osp` 改成 `import modified_os._os as os` 和 `import modified_os._ntpath as osp`。工程文件中导入的其他一些包，如 genericpath，也会 import os，我也将这些文件拷贝进 modified_os 中重命名， 如 _genericpath，并将其中的 import os 改成 import _os as os。在工程文件中有 `import genericpath` 的地方改成 `import modified_path._genericpath as genericpath`。其他有类似情况的也按此方法处理。总之，os_modified 模块下，都是在工程文件中使用到，且其有 `import os` 或 `import os.path` 的，我将所有这些文件集中到一个名为 modified_os 的模块下，当工程文件中调用到这些时，就调用此模块下的版本，而不是 anaconda 的原始版本。这样就解决了 `no module named path` 的错误。~~\n\n<br>\n#### Comment\n&emsp;&emsp;上面所述的关于 os, os.path 的部分全部不需要，只需要将 os.py, ntpath.py 两个文件通过 ` --add-binary=\"/path/to/os.py;.\" --add-binary=\"/path/to/ntpath.py;.\"` 添加进去即可。这样，在 `dist/runcaffe_fpn/` 下就会有 `os.py` 和 `ntpath.py` 两个文件。当一个 `.py` 文件是从 exe 文件中开始执行，即是 boundled 的时候，pyinstaller 的 bootloader 将 `frozen` 属性添加进 sys 模块中。因此，可以通过\n```python\nimport sys\nif getattr(sys, 'frozen', False):\n    # running in a bundle\nelse:\n    # running live\n```\n来在工程文件中进行判断此工程文件是用 python 脚本调用执行的还是用 exe 文件调用执行的。如果是用 python 脚本调用，则根据其导入模块的结构进行 import。比如 `fast_rcnn/nms_wrapper.py` 需要导入 `nms/gpu_nms.pyd` 和 `fast_rcnn/config.py`。在脚本模式下，需使用 `from nms.gpu_nms import gpu_nms` 和 `from fast_rcnn.config import cfg` 进行调用。而在 boundle 模式下，使用 `from gpu_nms import gpu_nms` 和 `from config import cfg` 进行调用即可。需要注意的是，在 boundle 模式下，还需使用 `--add-binary` 选项将 `gpu_nms.pyd` 和 `config.py` 两个文件添加进去，这样它的 import 语句才会生效。\n\n上述做法有效的<font color=orange>**原因**</font>是：pyinstaller 似乎将工程中不同层次的脚本（即存在调用关系）所需的依赖都展开放在最后打包的依赖项所在的文件夹下，即 `sys._MEIPASS` 所指向路径下，是没有目录层次的。因此，当处于 boundle 模式下时，`from gpu_nms import gpu_nms` 语句会直接对 `sys.__MEIPASS` 目录下的 `gpu_nms.pyd` 文件进行 import。在解决 os, os.path 问题时，将 os.py 和 ntpath.py 文件都添加进去，就不会有找不到的问题。并且按照理论，import os 所导入的文件应该是 `sys._MEIPASS` 路径下的 os.py。（已验证）\n\n工程文件中的示例如下：\n```python\nimport sys\nif getattr(sys, 'frozen', False): # boundle 模式\n    from gpu_nms import gpu_nms\n    from config import cfg\nelse: # 脚本模式\n    from nms.gpu_nms import gpu_nms\n    from fast_rcnn.config import cfg\n```\n\n<br>\n&emsp;&emsp;解决了 os 和 pyd 文件导入的问题后，exe 可以成功运行了。但是美中不足的是，运行 exe 除了需要作为参数的那些文件外，还一直需要 `proposal_layer.py` 文件，这是不好的，因为不能向客户透露源码。因此，尝试将 `proposal_layer.py` 文件也用 `--add-binary` 选项添加进去。但是这样并没有用。究其原因是，`proposal_layer.py` 是在 `.prototxt` 文件中指定的（即 python 层中的 module 参数），原来为 `module: rpn.proposal_layer`。这是因为 prototxt 文件同级目录下有一个 rpn 文件夹，其下有一个 `proposal_layer.py` 文件。由于将 `proposal_layer.py` 文件添加进 `sys._MEIPASS` 后其名称仍为 `proposal_layer.py`，而 prototxt 中指定为 `rpn.proposal_layer`，不统一。因此，将 rpn 文件夹下的两个文件，`proposal_layer.py` 和 `generate_anchors.py`，放置到与 prototxt 文件同级目录下，并将 prototxt 中的 `rpn.proposal_layer` 改成 `proposal_layer`. 再将 `proposal_layer.py` 用 `--add-binary` 添加进去，就可以在 `sys._MEIPASS` 中调用 `proposal_layer.py` 了，这样再运行单个的 exe 文件时就不需带着 `proposal_layer.py` 文件了，因为它会调用 `sys._MEIPASS` 下的 `proposal_layer.py`。\n\nPyinstaller 打包命令如下:\n```\npyinstaller -F --add-binary=\"models/nms/gpu_nms.pyd;.\" ^\n               --add-binary=\"models/fast_rcnn/bbox_transform.py;.\" ^\n               --add-binary=\"models/fast_rcnn/config.py;.\" ^\n               --add-binary=\"models/fast_rcnn/nms_wrapper.py;.\" ^\n               --add-binary=\"models/proposal_layer.py;.\" ^\n               --add-binary=\"models/generate_anchors.py;.\" ^\n               --add-binary=\"H:/Develop/Anaconda2/setup/Lib/os.py;.\" ^\n               --add-binary=\"H:/Develop/Anaconda2/setup/Lib/ntpath.py;.\" ^\n               runcaffe_fpn.py\n```\n\n最后，如上面的代码所示，我是将所有的除 caffe 模块外的额外 python 脚本都添加进了 `sys._MEIPASS` 中，保证可以有效调用。\n\n\n## Pyinstaller 使用方法记录\n### Pyinstaller 运行时路径问题\n&emsp;&emsp;pyinstaller 在打包后，会将 `frozen` 属性添加进 sys 变量。因此，可以使用 `getattr(sys, 'frozen', False)` 来判断当前执行的脚本是否是用 python 脚本执行的还是 exe 文件执行的。\n\n&emsp;&emsp;pyinstaller 有两种打包模式，一种是 onefile 模式，即将所有的依赖文件都打包成一个单独的 exe 文件，在运行此 exe 文件时，将其所包含的依赖文件临时解压到某个目录下，并在此目录下进行执行。另一种是 one-folder 模式，将生成的 exe 文件和依赖文件都放在 dist 文件夹下与被打包的 script 同名的文件夹中。在运行 exe 文件时，不需要将依赖文件解压，因此速度更快一些。很显然，one-file 模式的 exe 文件会比较大。当运行 exe 文件时，pyinstaller 的 bootloader 将 bundle folder 的绝对路径添加进 sys._MEIPASS 中。其实，这就是 exe 文件执行时，依赖文件所在的位置。需要注意的是，one-folder 模式下的 exe 文件必须和依赖文件在同一路径下，否则会找不到依赖。\n\n<br>\n### hidden-import\n在一次测试中，我用 `R.py` 导入了 `libs/rpn/proposal_layer.py`，在` proposal_layer.py` 中 `from ..fast_rcnn.nms_wrapper import nms`，在 `libs/fast_rcnn.nms_wrapper.py` 中 `from ..nms.gpu_nms import gpu_nms`，在 `libs/nms/` 下有一个 `gpu_nms.pyd` 文件。\n","tags":["caffe"]},{"title":"Linux 下 Caffe 的路径设置问题","url":"/2018/07/04/Linux-caffe-path-setting/","content":"在 FRDC 的 8 GPU 服务器上，在 `import caffe` 后，在 ipython 中一直无法补全 caffe 的函数。研究了一下，终于解决了这个问题。\n<!-- more -->\n\n## 环境配置\n- Centos 7\n- Caffe\n\n## 问题\n1)  个人用户目录下有一 caffe 库，将其添加进 `PYTHONPATH` 后，无法有效调用。\n2)  尝试将其他用户下的 caffe 路径添加进 `PYTHONPATH`，依然无法有效调用。\n3)  在 `PYTHONPATH` 中删除所有 caffe 有关的路径，依然可以 `import caffe`，但无法有效调用。\n\n<br>\n## 分析\n1)  针对问题 3，使用 `caffe.__file__` 可以查看所调用 caffe 的文件地址，发现调用的是 caffe2 库下的一个 caffe 文件夹。这显然是不对的。由于 python 在其 `PYTHONPATH` 中依次向后查找调用库，当查找到第一个后，就使用第一个路径中的库，因此如果后面的 `PYTHONPATH` 中也包含某个库的话，后面的库就不会被调用。因此，我们需要将 caffe 的路径放在 caffe2 路径的前面。\n2)  针对问题 1，由于我在 `/home/taoxuan/` 路径下有一个 caffe 库，因此将其添加进 `PYTHONPATH`，但无法有效调用。将其从 `PYTHONPATH` 中删除后，使用 `caffe.__file__` 却依然指向此 caffe 库，遂将此库改名。\n3)  针对问题 2，将其他用户的 caffe 路径添加进 `PYTHONPATH` 后，依然正确调用，个人猜测是由于权限的问题。\n\n<br>\n## 解决方案\n将其他用户下的有效 caffe 库拷贝至个人目录下，更改所有者及组，`sudo chown -R taoxuan:taoxuan /path/to/caffe`。然后将此库的路径添加进 `PYTHONPATH`，这样就可以有效调用。\n","tags":["权限"]},{"title":"Windows 下 Caffe 安装及测试","url":"/2018/06/27/Caffe-install/","content":"由于工作需求，需在 Windows 平台安装 Caffe, 并在此基础上添加新的层，对 Caffe 进行重新编译。\n<!-- more -->\n<br>\n## 环境配置\n- Windows 10\n- Visual Studio 2015 Community Edition\n- Cuda 8.0\n- Cudnn 5.1\n- Anaconda2 虚拟环境中的 python 3.5\n- Cmake 3.11.1\n- BVLC/Caffe\n- Git\n\n<br>\n## STEP 1\n默认已经安装好 Cuda 8.0，并已经将 Cuda 的安装路径中的某些文件夹添加进环境变量。\n默认已经将 Cudnn 中的文件放置到 Cuda 安装路径的对应文件夹中。\n将 Cmake 命令所在路径添加进环境变量\n将 python 环境中 python.exe 以及 lib 文件夹所在路径添加进环境变量。\n将 Git 命令添加进环境变量。\n\n<br>\n## STEP 2\n从 BVLC 的 Caffe Repo 上下载 Caffe 工程。\n``` bash\ngit clone https://github.com/BVLC/caffe.git\ncd caffe\ngit checkout windows\n```\n\n<br>\n## STEP 3\n修改 BVLC/Caffe windows 分支下 `scripts\\build_win.cmd` 文件\n因为我们没有定义 `APPVEYOR`，所以直接拉到 `else`（大约 69 行）以后。在 `else` 代码块中，设置相应参数：\n``` cmd\nWITH_NINJA=0 // 不使用 ninja 这个 generator，因为会报错。查看 cmake 拥有的 generator, 在 cmd 中输入 cmake --help\nCPU_ONLY=0\nPYTHON_VERSION=3 // 根据自己的 python 版本来\nBUILD_PYTHON=1 // 安装 pycaffe\nBUILD_PYTHON_LAYER=1\n\n// 在 cake -G ... 这一段中，添加一个选项.(其实这一段可以不加我感觉，因为已经将 cudnn 文件放到 cuda 的安装路径中了，不过加了也没啥)\n-DCUDNN_ROOT=H:/Develop/cudnn/cudnn5.1/cuda ^\n```\n\n<br>\n## STEP 4\n运行 `scripts/build_win.cmd` 文件，等其完成。它会先编译 `.cu` 文件，再编译 `.cpp` 文件。最后会生成 `caffe.exe` 等可执行文件以及 caffe 的 python 接口。\n注意：在编译过程中，脚本可能会下载一个依赖包到 `C:\\\\Users\\username\\.caffe\\dependencies` 文件夹下，可手动下载，将其解压到此文件夹即可。\nNote：此脚本执行完毕后，不用再打开 `Caffe.sln` 工程进行 build，因为这些工作在脚本里都完成了。\n最后得到的可执行程序在 `caffe/build/tools/Release` 中，python 接口在 `caffe/python/caffe` 中，将 `caffe/python/caffe` 路径添加进 python 的环境变量中或者将此文件夹拷贝进 python 的 `site-packages` 中均可在 python 中调用 caffe 的 python 接口。\n\n<br>\n## STEP 5\n对编译结果进行测试。参考 [这篇文章](https://software.intel.com/zh-cn/articles/installation-and-configuration-of-bvlc-caffe-under-windows-the-caffe-learning-notes-part1)\n\n<br>\n## 添加自定义层\n由于任务需求，因此需要在 Caffe 中添加新的层，这只要涉及 caffe 中的 `/include` 和 `/src` 文件夹，将需要添加的文件拷贝进原生的 BVLC/Caffe 中，（注意：不可整体替换这两个文件夹中的所有源码文件，因为相同文件名文件的代码可能是有区别的，因此，只将新增的源码添加进 Caffe 工程中即可），并根据改变，修改 `src/proto/caffe.proto` 文件，具体方法看 [这篇文章](https://blog.csdn.net/bvl10101111/article/details/74837156)。\n最后，按上述步骤重新编译即可。\n\n\n<br>\n## 参考文章\n1. [https://software.intel.com/zh-cn/articles/installation-and-configuration-of-bvlc-caffe-under-windows-the-caffe-learning-notes-part1](https://software.intel.com/zh-cn/articles/installation-and-configuration-of-bvlc-caffe-under-windows-the-caffe-learning-notes-part1)\n2. [https://blog.csdn.net/bvl10101111/article/details/74837156](https://blog.csdn.net/bvl10101111/article/details/74837156)\n\n<br>\n## Trouble Shooting\n### Trouble on Windows\n1.在按上面的步骤成功编译后，再使用命令 `/path/to/caffe.exe test -model /path/to/my.prototxt -weights /path/to/my.caffemodel` 进行测试，会报错如下图。 ![报错信息](1.png)\n可以看出，新添加的五个层里有三个已经存在，分别是 `DeformableROIPooling`, `ROIPooling` 和 `SmoothL1Loss`, 不存在的两个层为 `DeformableConvolution` 和 `ResizeBilinear`. 通过查看 `src/caffe/layers/deformabe_conv_layer.cpp` 和 `src/caffe/layers/resize_bilinear_layer.cpp` 文件，发现这两个文件都在最后一行差了 `REGISTER_LAYER_CLASS(DeformableConvolution);` 或 `REGISTER_LAYER_CLASS(ResizeBilinear);`。在这两个文件后分别添加对应的代码，重新编译。\n通过比较，发现原版 BVLC-Caffe 的 `src/caffe/layer_factory.cpp` 和 添加过新层的 cafe-fast-rcnn 这个 Caffe 的 `src/caffe/layer_factory.cpp` 文件有所不同，更改过的文件中有关于 `DeformableConvolution` 和 `ResizeBilinear` 的内容，（可通过文档比较工具查看）。个人猜测这个就是上面 `src/caffe/layers/deformabe_conv_layer.cpp` 和 `src/caffe/layers/resize_bilinear_layer.cpp` 文件没有最后一个 `REGISTER` 语句，而其在 Linux 平台能够调用 `DeformableConvolution` 层的原因。但是，我从网上找到一个博客说，新版 Caffe 不需要在 `layer_factory.cpp` 文件中添加相关内容，只需要在对应 `.cpp` 文件中写上 `REGISTER` 语句就好。\n>2）配置该层：包括注册和在caffe.proto中设置相关参数。在较新版本的caffe中，用户只需要\n>在编译caffe的过程中，会根据caffe.proto文件中定义的参数接口来编译相关层的代码。一般只需要做3.1和3.2两部即可。\n>注意：不需要在layer_factory.cpp文件中添加新层的头文件。\n>[来源](https://blog.csdn.net/king_lu/article/details/53812216)\n\n经过验证，我发现确实只要在 `.cpp` 文件后面添加对应的 `REGISTER` 语句就好。\n\n\n2.在解决了上述问题后，执行 `/path/to/caffe.exe test -model /path/to/my.prototxt -weights /path/to/my.caffemodel -gpu 0`，报错 `Py_Initialize fails - unable to load the file system codec`，找了很多资料，觉得可能是多个版本的 python 冲突问题，但是我把环境变量 `PATH` 中全部改成有关虚拟环境 `tensorflow` 下的路径都无效。个人感觉是不知道在哪里仍然默认指向 Anaconda2 的外层 python，即使在 `PATH` 中修改也无效。因此，我重新用 python2.7 编译了自定义的 caffe。完成后执行上句命令，不再报此错误，而是报 `ImportError: No module named site\n`，我使用[此处的最高票答案](https://stackoverflow.com/questions/5599872/python-windows-importerror-no-module-named-site)解决了问题。继续执行前面的命令，出现 `ImportError: No module named gpu_nms`，这是由代码 `from nms.gpu_nms import gpu_nms` 引起的，目的是从 `gpu_nms.so` 文件中导入 `gpu_nms` 函数。而 `gpu_nms.so` 是 linux 平台上的动态链接文件，不能在 windows 上使用，因此需要从源码重新编译出 windows 下的动态链接文件。使用 [此工程](https://github.com/MrGF/py-faster-rcnn-windows) 并运行 `python setup_cuda.py build` 可编译出 Windows 下的 `.pyd` 文件供 python 导入。（运行前需先修改 `include_dirs = [numpy_include, '/path/to/cuda/include']`，且有可能会提示需安装 VC 9.0，根据提示下载安装即可），这样，就能导入 gpu_nms 函数了。\n\n3.解决上述问题后，执行命令，会报一个 `MS VC` 有关的错 - `Runtime error R6034`，这个是一个与 `msvcr90.dll` 有关的错。这个 `dll` 文件是由系统提供的，根据后来分析发现，Anaconda 中也会提供这两个文件，分别位于 Anaconda 根目录和 `Library/bin` 目录下。我根据[这个问题的最佳答案](https://stackoverflow.com/questions/14552348/runtime-error-r6034-in-embedded-python-application)解决了此问题。由于环境变量 `PATH` 较为复杂，我无法修改 `PATH`，因此我将 Anaconda 中的两个文件进行了重命名，解决了这个问题。\n\n4.解决了上述问题后，使用上述命令执行依旧错误，主要是在 `Creating layer rpn_rois` 处，会出现 `Caffe.exe 已经停止运行` 的错误。但是没有任何错误信息，困扰了好久。后来，我尝试使用 pycaffe 进行 test，在 `caffe.Net()` 函数中传入 `model_path` 和 `weights_path` 后执行，报错 `AttributeError: 'ProposalLayer' object has no attribute 'param_str_'`，这是由于 ProposalLayer 的定义中，有一句 `layer_params = yaml.load(self.param_str_)`，这是老版的 caffe 中定义的属性。由于我的是在新版的 bvlc-caffe 的基础上添加的新的层，因此会报错。而在 Linux 中没有报错，个人觉得是由于 Linux 版本的 modified caffe 是基于老版的 caffe 得到的。解决办法：将 `param_str_` 改成 `param_str` 即可，参考[这里](https://github.com/rbgirshick/py-faster-rcnn/issues/219)。\n\n<br>\n### Trouble on Linux\n在 linux 中同样执行 `/path/to/caffe test -model /path/to/my.prototxt -weights /path/to/my.caffemodel` 时，会先报错\n`Creating layer rpn_rois\nImportError: No module named rpn.proposal_layer\n`\n这是因为在执行时，`my.prototxt` 需要调用 `rpn/proposal_layer.py` 文件，因此，需要将 `rpn` 所在路径加入 `PYTHONPATH` 或者在此路径下执行。\n上一步解决后，再次执行命令，会报错 `Check failed: registry.count(type) == 0 (1 vs. 0) Solver type Nesterov already registered.`. 这是因为 `.bashrc` 文件中将一个原生的 caffe 的 python 接口添加进了 `PYTHONPATH`, 而真正使用的是自己更改过的 caffe. 因此，将 `.bashrc` 中 `PYTHONPATH` 关于原生 caffe 的部分去掉。\n后来，又报 `rpn/proposal_layer.py` 文件中 `calss ProposalLayer(caffe.Layer):` `AttributeError:'module' object has no attribute 'Layer'`。此时，将自己更改过的 caffe 的 python 接口添加进 `PYTHONPATH` 中即可。\n总之，解决上面 2 个错误的关键就是将自己所使用的版本的 caffe 添加进 `PYTHONPATH`，并确保其他版本的 caffe 不在其中。\n接着，继续运行同样的命令，会报错 `Check failed: status == CUDNN_STATUS_SUCCESS (8 vs. 0) CUDNN_STATUS_EXECUTION_FAILED`. 个人感觉这是一个跟 gpu 有关的错，由于 `.prototxt` 文件中在 `rpn_rois` 层设置了 `gpu_id` 参数，因此，需将此参数与 `caffe test` 命令执行时使用的 gpu id 相同。若文件内设置为 1，则命令应为 `/path/to/caffe test -model /path/to/my.prototxt -weights /path/to/my.caffemodel -gpu 1`\n","tags":["Windows"]},{"title":"python 多线程模块 threading 及 多进程模块 multiprocessing","url":"/2018/05/22/python-thread/","content":"关于 python 多线程模块 threading 以及 多进程模块 multiprocessing 的使用。\n\n*<font color='orange'>注意：本文代码均在 python 3.5 中实现，其他版本未亲测。</font>*\n\n<!-- more -->\n## threading 模块\n&emsp;&emsp;关于 python 的多线程模块 threading 的使用方式。\n```python\ndef fun1():\n    print('thread 1 start', time.time())\n    for i in range(10):\n        time.sleep(0.5)\n    print('thread 1 end', time.time())\n\ndef fun2():\n    print('thread 2 start', time.time())\n    for i in range(20):\n        time.sleep(0.5)\n    print('thread 2 end', time.time())\n\ndef fun3():\n    print('thread 3 start', time.time())\n    for i in range(30):\n        time.sleep(0.5)\n    print('thread 3 end', time.time())\n\nt1 = threading.Thread(target=fun1)\nt2 = threading.Thread(target=fun2)\nt3 = threading.Thread(target=fun3)\n\nt1.start()\nt1.join()\nt2.start()\nt3.start()\nprint('主线程中间', time.time())\nt3.join()\nt2.join()\nprint('主线程结束', time.time())\n```\n输出为：\n>thread 1 start 1526973092.5235336\nthread 1 end 1526973097.535105\nthread 2 start 1526973097.5354195\nthread 3 start主线程中间 1526973097.5474474\n 1526973097.5414264\nthread 2 end 1526973107.548385\nthread 3 end 1526973112.5574799\n主线程结束 1526973112.5574799\n\n\n&emsp;&emsp;可以看出，thread 1 执行完毕了后，同时开始执行（几乎） thread2, thread3 和 print 语句，（print 语句是主线程中的），这是因为，在 thread1, thread2 调用之前，t1.join() 已经阻塞了别的线程。由于 thread3 用时较长，因此 thread2 先结束，然后 thread3 结束。在 thread3 结束后，主线程才执行最后的 print 语句。\n\n&emsp;&emsp;由于 python GIL (Global Interpreter Lock) 的限制，python 的多线程并没有达到真正的多线程的效果，不能充分地利用 CPU。但是多进程可以充分地利用 CPU，因此使用 multiprocessing 模块来使用多进程。\n\n## multiprocessing 模块\n&emsp;&emsp;此处采用线程池的方式进行多线程操作。\n\n```python\nfrom multiprocessing import Pool\nfrom itertools import product\n\ndef fun(l, name, sur):\n    print(l + ' ' + name + ' ' + sur)\n\narg_1 = ['adf', 'freg', 'gr3q', 'saf', 'qrg']\narg_2 = ['sean']\narg_3 = ['tao']\n\nif __name__ == '__main__':\n    pool = Pool(processes=8)\n    pool.starmap(func=fun, iterable=product(arg_1, arg_2, arg_3))\n    print('all done')\n```\n输出为:\n![multiprocessing 输出](pic_1.png)\n\n以上代码为针对多参数的函数，\n```python\npool.starmap(func=fun, iterable=product(arg_1, arg_2, arg_3))\n```\n\n是通过 itertools 的 product 函数，将三个参数进行笛卡儿积。例如：\n```python\nproduct(['a', 'b', 'c'], ['1']) # 结果为 ('a','1')，('b', '1'), ('c', '1')\n```\n\n关于 product 函数以及 itertools 包的使用，可点击[这里](https://docs.python.org/3/library/itertools.html#itertools.product)了解更多。\n\n值得注意的是，product 函数的输入必须是 iterable 的。这样，才可以进行 笛卡儿积。\n\n此外，多参数进程池还有别的方法，但未亲测。\n[pool.map - multiple arguments](http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments)\n\n\n## 参考文章\n1.[https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments/24446525](https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments/24446525)\n2.\n","tags":["进程池"]},{"title":"jupyter notebook 增加内核","url":"/2018/05/15/jupyter-add-kernel/","content":"在 jupyter notebook 中添加不同的 python 内核。\n<!-- more -->\n&emsp;&emsp;由于在 anaconda2 中安装了虚拟环境 tensorflow, 且虚拟环境中的 python 为 3.5，在虚拟环境中安装了 jupyter notebook，当在虚拟环境中的 jupyter notebook 中新建脚本时，只能选择 python3，因此想要把 anaconda2 中 2.7 的 python 内核导入进来，这样可以直接在一个 jupyter notebook 中新建不同内核的 python 脚本。\n本文介绍在 python3 中添加 python2 内核。\n首先确认在Python3下已安装了内核：\n```python\nipython kernel install --user\n```\nor\n```python\npython3 -m ipykernel install --user\n```\n然后确保 python2 下安装了 ipykernel\n```python\nsudo pip2 install -U ipykernel\n```\n然后运行如下命令：\n``` python\npython2 -m ipykernel install --user\n```\n至此，运行 jupyter notebook 就可以添加多个内核。\n"},{"title":"vim","url":"/2018/05/08/vim/","content":"\n<!-- more -->\n以下命令可以对标点内的内容进行操作。\n`ci'、ci\"、ci(、ci[、ci{、ci<` - 分别更改这些配对标点符号中的文本内容\n`di'、di\"、di(或dib、di[、di{`或`diB、di<` - 分别删除这些配对标点符号中的文本内容\n`yi'、yi\"、yi(、yi[、yi{、yi<` - 分别复制这些配对标点符号中的文本内容\n`vi'、vi\"、vi(、vi[、vi{、vi<` - 分别选中这些配对标点符号中的文本内容\n\n另外如果把上面的 `i` 改成 `a` 则可以连带配对的标点一起操作～～\n"},{"title":"Deformable Convolutional Networks 论文理解","url":"/2018/05/07/Deformable-Convolutional-Networks/","content":"关于 Deformable Convolutional Networks 的一些个人理解。\n<!-- more -->\n--   有效感受野 Effective Receptive Field\n[Understanding the effective receptive field in deep convolutional neural networks]()\n> - 感受野中的像素对最后的输出响应贡献不一。靠近中心的像素贡献更大。\n- 有效感受野只占理论感受野的一小部分，并且呈高斯分布。\n- 有效感受野尺寸的增长幅度不是像理论感受野那样随卷积数目线性增长，而是与卷积数目的均方根线性相关。\n\n这个发现说明，**即使是很深的 CNN 网络的顶层的单元的有效感受野也可能不够大。**\n\n<br>\n##  **Deformable Convolution**\n&emsp;&emsp;对一个3\\*3的 deformable conv 来说，先在分支上进行卷积操作，得到的 offset field 与 input feature map 尺寸相同，通道数为 2N（对 3\\*3 卷积，N=9）。这样，offset field 每个位置处的 2N 维向量代表在 input feature map 上此处卷积时对应的 N 个位置的偏移。如下图所示。\n![3*3 deformable convolution](deform_conv.PNG)\n\n## Deformable ROI Pooling\n&emsp;&emsp;首先通过标准的兴趣区域池化（绿色）获得兴趣区域对应的特征，该特征通过一个全连接层得到兴趣区域每个部位(每个 bin)的位移量。用该位移作用在标准的 ROI Pooling 的每个区域块上，得到 可变形兴趣区域池化（蓝色），以获得不局限于兴趣区域固定网格的特征\n&emsp;&emsp;简单地说，假如要对某个 ROI 区域进行 3\\*3 的 Deformable ROI Pooling，那么则先对此 ROI 进行普通的 ROI Pooling，得到 3\\*3\\*C (C 为 ROI pooling 前的 feature map 的 channel 数) 的 feature map。然后，对得到的 feature map 进行 fc，fc 层的输出个数为 k\\*k\\*2，其中 k 为 pooling 后的尺寸，此处为 3。这样，就得到 ROI 区域中每块 （共 k\\*k 块）的偏移了。根据这个偏移，得到每个块的新的位置，进行 ROI Pooling，就是 Deformable ROI Pooling。\n所以，在 Deformable ROI Pooling 中，偏移是针对每个块（bin）的，某个 bin 内的所有像素的偏移都是一样的。\n![3*3 deformable ROI Pooling](deform_roi_pooling.PNG)\n\n## Position-Sensitive (PS) RoI Pooling - From R-FCN\n","tags":["deformable RoI pooling"]},{"url":"/2018/05/07/FPN/","content":"\n\n\n\n\n\n\n\n\n\n\n---\ntitle: FPN 论文理解\nauthor: taosean\ntags: [目标检测, Faster R-CNN, FPN]\ndate: 2018-04-26 11:06:00\n---\n本文给出了对 FPN 网络的理解。\n<!-- more -->\n## 网络结构\n&emsp;&emsp;常见的特目标测网络中特征的利用方式主要有 4 种\n>*(a) 图像金字塔，即将图像做成不同的 scale，然后不同 scale 的图像生成对应的不同 scale 的特征。这种方法的缺点在于增加了时间成本。有些算法会在测试时候采用图像金字塔。\n(b) 像 SPP net，Fast RCNN，Faster RCNN 是采用这种方式，即仅采用网络最后一层的特征。\n(c) 像 SSD (Single Shot Detecto) 采用这种多尺度特征融合的方式，没有上采样过程，即从网络不同层抽取不同尺度的特征做预测，这种方式不会增加额外的计算量。作者认为 SSD 算法中没有用到足够低层的特征（在 SSD 中，最低层的特征是 VGG 网络的 conv4_3），而在作者看来足够低层的特征对于检测小物体是很有帮助的。\n(c) 本文作者是采用这种方式，顶层特征通过上采样和低层特征做融合，而且每层都是独立预测的。\n*\n\n![检测网络的 4 种特征利用方式](pic1.png)\n图中，线越粗表示特征的语义程度越强。\n\n&emsp;&emsp;与以往的方式不同，FPN 网络在 pyramid 的所有 level 上都进行预测。\n![FPN 网络在 pyramid 的所有 level 上进行预测](pic2.png)\n\n&emsp;&emsp;FPN 网络的内部结构\n![FPN 网络的内部结构](pic3.png)\n>**自底向上**其实就是网络的前向过程 (论文中使用的是 ResNet )。在前向过程中，feature map 的大小在经过某些层后会改变，而在经过其他一些层的时候不会改变，作者将不改变 feature map 大小的层归为一个 stage，因此每次抽取的特征都是每个 stage 的最后一个层输出，这样就能构成特征金字塔。\n**自顶向下**的过程采用上采样进行，而横向连接则是将上采样的结果和自底向上生成的相同大小的 feature map 进行融合。在融合之后还会再采用 3*3 的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应。并假设生成的 feature map 结果是 $\\{P_2, P_3,P_4,P_5\\}$，和原来自底向上的卷积结果 $\\{C_2,C_3,C_4,C_5\\}$ 一一对应。\n\n<br>\n## Feature Pyramid Network for RPN\n&emsp;&emsp;FPN 可以用在 RPN 网络中，从而提高多尺度物体的检测性能。与 Faster R-CNN 中不同的是，FPN 在不同的 level 上使用 anchor，$\\{P_2, P_3,P_4,P_5, P_6\\}$ 上的 anchor 尺寸分别是 $\\{32^2, 64^2, 128^2, 256^2, 512^2\\}$。可以看出，尺寸较大的 feature map 上的 anchor 的尺寸较小，适合用来检测小物体。\n![FPN 的 anchor 机制](pic4.png)\n<hr>\n*<font color=\"orange\">关于 anchor 尺寸选取的个人理解。</font>*\n&emsp;&emsp;RPN 网络的输入 feature map 在原图上的感受野是固定的，取决于此 featured map 相对于原图的尺寸缩小了多少倍。缩小的倍数越大，相邻感受野的中心距离越远（因为 feature map 尺寸越小，就需要用越少的感受野覆盖整张原图，因此相邻感受野的中心距离就会越远）。如果在尺寸较小的 feature map 上使用尺寸较小的 anchor 的话，相邻感受野的 anchor 可能会不重叠，从而漏掉物体。\n![anchor 尺寸较小](pic5.png)\n&emsp;&emsp;但是在这种情况下选择较大尺寸 anchor 也会产生新的问题。当遇到小物体时，如果与大的 anchor 计算 IoU 时，IoU 可能较小，在生成样本时也会遇到问题。\n![anchor 尺寸较大时检测小物体](pic6.png)\n&emsp;&emsp;可以看出，由于 feature map 尺寸较小，对应在原图上的感受野个数也较少，因而需要用较少的 anchor 来覆盖原图。若 feature map 的尺寸较大，则用来覆盖原图的感受野的个数较多，则每个 anchor 的尺寸就不用太大，因而可以更好地检测小物体。\n![较大 feature map](pic7.png)\n&emsp;&emsp;可以看出，当 feature map 尺寸较大时，就可以用较小的 anchor 来覆盖原图，这样既不会出现 anchor 不相交从而漏检的情况，也不会造成小物体和 anchor 的 IoU 较小从而无法正确产生正样本的情况。\n\n**从以上分析可以得出结论，尺寸较大的 feature map 适合用来检测用来检测小物体，尺寸较小的 feature map 适合用来检测大物体。因此，FPN 的思想就是将各种尺寸的 feature map 结合起来，将大物体分配给小的 feature map 进行检测，将小物体分配给大 feature map 进行检测，从而提升检测性能。**\n\n<br>\n## 总结\n&emsp;&emsp;在 FPN 中，anchor 机制跟 Faster R-CNN 有所不同。FPN 产生了上文所述的 $\\{P_2, P_3,P_4,P_5, P_6\\}$ 不同尺寸的 feature map，在一个 feature map 上只使用一个尺度的 anchor（Faster R-CNN 中为 3 个尺度）。在尺寸较大的 feature map 中使用尺度较小的 anchor，能够更好地检测小物体。\n\nFPN 中有多个尺度的 feature map，每一个 feature map 作为输入进入一个 RPN。因此，有多少 feature map，就有多少 RPN。最后，将这些 RPN 的输出进行综合，得到所有的 proposals。\n\n<br>\n## 参考文章\n[https://blog.csdn.net/u014380165/article/details/72890275](https://blog.csdn.net/u014380165/article/details/72890275)\n"},{"title":"RPN 中的 loss 理解","url":"/2018/04/25/Faster-R-CNN/","content":"本文解释了 Faster R-CNN 中的 RPN 部分的 GT 生成以及 loss 的计算方式.\n<!-- more -->\n## RPN 结构\n&emsp;&emsp;RPN 网络使用 特征提取网络的输出 feature map 作为输入. 然后在此基础上,用 anchor 的方式计算 分类误差 和 定位误差. Faster R-CNN 中的 RPN 网络在 caffe 框架中如下图所示.\n![RPN 网络结构图](pic1.PNG)\n假设 RPN 网络输入 feature map 尺寸为 $s\\times s$, 那么图中 *rpn_cls_score* 和 *rpn_bbox_pred* 层的输出尺寸分别为 $s\\times s\\times8$ 以及 $s\\times s\\times36$. *rpn-data* 是真实值, 与这两个 feature map 计算 loss. 可以推测, *rpn-data* 具有两个与 *rpn_cls_score* 和 *rpn_bbox_pred* 分别对应的 GT feature map. 对应于 *rpn_cls_score* 的真实值的尺寸为 $s\\times s\\times 9$, 对应于 *rpn_bbox_pred* 的真实值的尺寸为 $s\\times s\\times 36$.\n\n<br>\n## Ground Truth 的尺寸和计算方式\n*<font color=\"orange\">以下部分为个人理解</font>*\n&emsp;&emsp;在 Faster R-CNN 中, RPN 输入 feature map 的每个空间位置都对应 9 个 anchor, (共有 $s\\times s$个空间位置), 每个 anchor 产生一个预测框. Faster R-CNN 的正负样本生成策略是:\n>1. 将每一个 GT box 与所有的 anchor 计算 IoU, 将 IoU 最大的 anchor 与当前 GT box 对应, 作为正样本, 记为 1. (这就保证了每个 GT box 至少有一个 anchor 与之对应)\n2. 将每一个 anchor 与 所有 GT box 计算 IoU, 若当前 anchor 与某一个 GT box 的 IoU 超过 0.7, 则将当前 anchor 与此 GT box 对应, 将当前 anchor 作为正样本, 记为 1. 对与所有 GT box 的 IoU 都小于 0.3 的 anchor 作为负样本, 记为 -1. 不满足上述条件的其他 anchor 不计入讨论, 记为 0.\n\n&emsp;&emsp;根据上述策略可以知道, anchor 和 GT box 之间是 多对多 的关系. 但是这是不合理的, 若一个 anchor 对应了 多个 GT box, 那岂不是这个 anchor 要负责检测多个物体? 这是不可能的. 因此, 个人感觉上述策略中, 第 2 步是在不满足第 1 步条件的 anchor 中进行的. 一旦一个 anchor 找到了与之对应的 GT box, 它就不会再与其他 GT box 计算 IoU, 这样, 即使某个 anchor 与 多个 GT box 的 IoU 都大于 0.7, 此 anchor 仍然只能与一个 GT box 对应.\n\n<font color=\"orange\">**2018/11/06 更新:**</font> 事实上，在为每个 anchor 匹配 GT box 时，是每一个 anchor 和每一个 GT box 算 IoU，为每个 anchor 找到与之 IoU 最大的 GT box。并计算每个 anchor 和与之对应的 GT box 之间的 delta 作为 RPN 学习的目标。\n\n\n&emsp;&emsp;因此, 从 GT box 的角度看, <font color=\"green\">一个 GT box 有一个或多个 anchor 与之对应, 这些 anchor 负责检测这个 GT box.</font>\n\n<br>\n## Ground Truth 及 相关图示\n下图为每一个空间位置处的向量.\n![每个空间位置处的向量](pic2.png)\n&emsp;&emsp;在上图中, GT class 是一个 9 维的向量, 其可能值为 $1,-1,0$, 1 表示正样本, 0 表示负样本, -1 表示非样本. 相同颜色表示某一空间位置处的不同类型的值. 在 GT class 向量中某处为 1 , 表示此空间位置处的此处的 anchor 为正样本, GT bbox 向量中对应的 4 个值为此 anchor 对应的 GT box 的真实 $(x,y,w,h)$. 这样就可以与 rpn_cls_score 和 rpn_bbox_pred 对应位置的预测值计算损失. 针对正样本, 要计算 分类 和 定位 两种损失, 对于负样本, 只计算 分类损失, 因此 GT bbox 向量中对应位置的值不参与计算. 对于非样本, 不计算任何损失. 因此在生成 GT bbox 向量时, 只需计算正样本 anchor 对应 GT box 的 $(x,y,w,h)$ 值.\n","tags":["anchor"]},{"title":"Yolo v2 论文理解","url":"/2018/04/20/Yolo-v2/","content":"本文针对 Yolo v2 的一些处理方法和细节给出了自己的理解。不一定正确，如有错误请指正。\n<!-- more -->\n本文尝试了使用一系列的方法来提升检测效果。\n<br>\n## Better\n### Batch Normalization\n- 在所有卷积层后加 Batch Normalization。\n- mAP 提高了2%。\n\n### High Resolution Classifier\n- Yolo v1 使用 224×224 的图像预训练分类器，并用来对448的图像进行检测。这意味着网络的卷积层要在适应新的分辨率的同时还要学习检测。\n- Yolo v2 直接使用 448×448 的图像进行预训练。\n- mAP提高了4%。\n\n### Convolutional With Anchor Boxes\n- Yolo 通过在卷积层上加全连接层来直接预测bbox的坐标。而 Faster R-CNN 使用 RPN 网络来预测 bbox 相对于 anchor 的 offset 和 confidence。预测 offset 比直接预测坐标来得简单。\n- Yolo v2 去除了全连接层，并减少了一个 Pooling 层。这样最后的 feature map 尺寸是输入的1/32。在这里，网络使用 416 尺寸而不是 448，因为作者想要最后的 feature map 尺寸为奇数。$416\\div32=13$。这是因为作者观察发现，很多物体都在图像的中间，因此检测这些图像中间的物体时，只需用最中心的那个位置而不是偶数情况下的中心4个位置。(个人理解为：在这种情况下，那些在图像中心位置的物体的中心点都会落在这 $13\\times 13$ 栅格的中心格子中)\n- 使用 anchor boxes 预测坐标的同时，Yolo v2 还对 conditional class probability 的预测机制和空间位置（栅格）做了解耦。\n&emsp;&emsp;在Yolo v1 将输入图像划分为 $S×S$ 的栅格，每一个栅格预测 $B$ 个 bounding boxes，以及这些 bounding boxes 的 confidence scores。\n&emsp;&emsp;每一个栅格还要预测 $C$ 个 conditional class probability（条件类别概率）：$Pr(Classi|Object)$。即在一个栅格包含一个 Object 的前提下，它属于某个类的概率。且每个栅格预测一组 ($C$个) 类概率，而不考虑框 $B$ 的数量。\n&emsp;&emsp;Yolo v2 不再由栅格去预测条件类别概率，而由 Bounding boxes 去预测。在 Yolo v1 中每个栅格只有1组条件类别概率，而在 Yolo v2 中，因为每个栅格有 $B$ 个 bounding boxes，所以有 $B$ 组条件类别概率。\n在 Yolo v1 中输出的维度为 $S\\times S \\times (B \\times 5 + C )$，而 Yolo v2 为$S \\times S \\times (B \\times (5 + C))$。如下图所示。\n- 使用了Anchor box 后，mAP 从 69.5% 降到了 69.2%，但是 recall 从 81% 增加到了 88%。\n\n<hr>\n ![v1 和 v2 输出维度对比](pic1.png)\n\n### Dimension Clusters\n&emsp;&emsp;采用 Anchor 机制后遇到两个遇到<font style=\"color:orange\"> 两个问题 </font>，其中<font style=\"color:cyan\"> 第一个 </font>为 anchor 尺寸的设置问题。Faster R-CNN 等网络的 anchor 的尺寸是人工选定的，虽然网络可以通过学习来调整预测框的尺寸，但是如果一开始就给一个较合适的 anchor 的话，网络学习起来会更加的容易。Yolo v2 通过 k-means 的方式来学习到 anchor 的尺寸分布情况。对训练集中的所有标定的框，即 GT box，根据他们的 (x,y,w,h) 的值进行 k-means 聚类。如果将用 (x,y,w,h) 来代表一个 GT box，并用4维向量的标准欧式距离来作为距离度量的话，大的框可能会比小的框产生更大的误差，比如 (1,1,2,2) 和 (2,2,4,4) 的欧式距离为$\\sqrt{10}$，而 (2,2,4,4) 和 (4,4,8,8) 的欧式距离为 $2\\sqrt{10}$，而如果采用IoU的度量方式，这两对框的距离相等都是3/4。而后一种情况所代表的两种框是前一种情况两种框尺寸的两倍。因此，可以看出，采用IoU的方式作为两个框之间的距离度量，可以避免框的尺寸带来的影响。\n&emsp;&emsp;通过聚类，可以将所有的 (x,y,w,h) 聚为 k 类，最后得到 k 个类别中心，这 k 个类别中心就代表 k 个矩形框。论文中使用 Dimension Clusters 的结果如下图所示。\n![维度聚类-5类](pic2.jpg)\n\n在网上看到另一种说法\n> 算法过程（k-means）是: 将每个 bbox 的宽和高相对整张图片的比例 $(w_r,h_r)$ 进行聚类,得到 k 个anchor box.\n算法实现代码可以参考: [k_means_yolo.py](https://github.com/PaulChongPeng/darknet/blob/master/tools/k_means_yolo.py)\n其实根据距离函数就可以看出，k-means 一定是对 (x,y,w,h) 进行聚类的（计算 IoU 需要用到 (x,y) ）。只不过最后不关注聚类中心的 (x,y) ，只关注聚类中心的 (w,h)。上面代码中的操作也证实了这一点。\n\n&emsp;&emsp;与手工挑选的相比，使用 Dimension Clusters 的方法效果更好。对比效果如下图所示。\n![使用维度聚类效果对比](pic3.jpg)\n其中 Cluster SSE 表示使用 sum squared error 作为度量进行聚类，Cluster IoU 表示使用 IoU 作为度量进行聚类。Anchor boxes 为采取类似 Faster R-CNN 中的方法。\n### Direct location prediction\n&emsp;&emsp;前面提到了两个问题，其中 <font color=\"cyan\">第二个</font> 问题是：模型的不稳定性。不稳定性主要来源于预测框的(x,y)坐标。在 RPN 中，网络预测 $t_x$ 和 $t_y$ ，因此，框的中心(x,y)计算方式为:\n$$ \\begin{cases} x = (t_x * w_a) + x_a\\\\\n    y = (t_y * h_a) + y_a\\\\\n   \\end{cases}\n$$\n在原论文中，$x_a$ 和 $y_a$ 前使用的是减号，估计是作者的笔误。其中，$w_a$ 和 $h_a$ 为 anchor 的宽和长。如果 $t_x>0$，bounding box 会向右移动 anchor 宽的 $t_x$ 倍，如果 $t_x<0$，bounding box 会向左移动 anchor 宽的 $t_x$ 倍。由公式可以看出，由于 $t_x$ 和 $t_y$ 没有限制（取值范围没有限制），因此最后得到的 $x$ 和 $y$ 可以落在图像上的任意一个位置，因此一个 anchor 可能检测一个离自己很远的物体，尽管这个物体应该由离其自身近的 anchor 来检测。\n&emsp;&emsp;Yolo v2 不采用预测 offset 的方法，而是延续 Yolo v1 预测 bbox 相对每个 grid cell 左上角的坐标，确保每个 bbox 的中心落在 grid cell 内。作者使用 logistic 函数来约束预测值。\n&emsp;&emsp;对网络的输出 feature map (13×13)，Yolo v2 对每一个 grid cell 预测 5 个 bbox (对应 5 个 anchor)，每个 bbox 由 5 维向量 $( t_x,t_y,t_w,t_h,t_o)$ 表示。因此，对 feature map 上的每一个位置来说，都会预测出一个 5×5 的向量。(要牢记 feature map 上的每一个位置都对应图像上的一个 grid cell )。如果这个 grid cell 的左上角距离图像左上角偏移为 $(c_x,c_y)$，anchor box 的宽高为 $p_w, p_h$，那么这个预测的 bounding box 的中心点为 $(b_x, b_y)$，宽高为 $(b_w, b_h)$。计算方式如下图:\n<hr>\n ![一个预测框的计算方式](pic4.png)\n <hr>\n $\\sigma(t_x)$ 是 bounding box 的中心相对栅格左上角的横坐标，$\\sigma(t_y)$ 是纵坐标。$\\sigma(t_o)$ 是 bounding box 的 confidence score。\n 这样，就可以预测出 5 组 bbox.\n<font color=\"green\">个人理解: </font>由于预测的值是 $( t_x,t_y,t_w,t_h,t_o)$ 这 5 个数，且 $(b_x,b_y,b_w,b_h,confidence)$ 可以由上面的预测值计算出来，因此不确定最后的 GT 向量是用哪一个来计算 loss。~~个人猜测是 $(b_x,b_y,b_w,b_h,confidence)$。因为这样与 Yolo v1 中的 loss 一致。因此有可能 5 个 GT 都是一样的，就像 Yolo v1 中的一样。~~\n<center><font color=\"orange\">---------------------------  损失计算理解更新  -----------------------------</font></center>\n通过阅读损失部分源码，感觉上面的猜测不正确。应该是使用 $(t_x,t_y,t_w,t_h,t_o)$ 这 5 个数进行损失计算的。\n```c++\nfloat delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale, int stride)\n{\n    box pred = get_region_box(x, biases, n, index, i, j, w, h, stride);\n    float iou = box_iou(pred, truth);\n\n    // 这里的 i,j 是对 cell 的索引，因此 i,j 的范围都是 [0,12] 的整数. w,h 都是 13，delta 存储 loss， x 是预测结果，stride=169.\n    // n 是 [0,5] 的整数，是一个确定 cell 内 bbox 的索引。\n    float tx = (truth.x*w - i); // 这里跟论文中有一点不一样，这里的 tx 其实相当于论文中的$\\sigma{t_x}$，这里的 i 就相当于论文中的 cx。ty同理。\n    float ty = (truth.y*h - j);\n    float tw = log(truth.w*w / biases[2*n]); // biases 数组中存储的是 5 个 anchor 的长宽。\n    float th = log(truth.h*h / biases[2*n + 1]);\n\n    delta[index + 0*stride] = scale * (tx - x[index + 0*stride]);\n    delta[index + 1*stride] = scale * (ty - x[index + 1*stride]);\n    delta[index + 2*stride] = scale * (tw - x[index + 2*stride]);\n    delta[index + 3*stride] = scale * (th - x[index + 3*stride]);\n    return iou;\n}\n ```\n从代码可以看出，计算每一个 bbox 的损失时，是将当前 bbox 所在 cell 对应的 GT box 的 $(x,y,w,h)$ 转换成 $(t_x,t_y,t_w,t_h)$ 后，再跟网络输出的预测值计算损失的。可以看出，针对同一 cell 内不同的 anchor 对应的预测框（一个 anchor 对应一个预测框），它们的 $(t_x,t_y)$ 真实值都是相等的，因为 $i,j$ 是 cell 的索引，所以对某一 cell 内的预测框来说，$i,j$ 都是相等的。但是 n 是 cell 内预测框，或者是 anchor 的索引，因此对每个预测框来说，他们的 $(t_w,t_h)$ 的真实值是不等的。（这里没有说一个 cell 是否只计算 responsable 的那个 bbox 的损失，需要查看别的地方的代码）\n\nYolo v2 论文没有给出损失函数。网上找到的一个损失函数如下，不知正确与否。\n![Yolo v2 损失函数](pic6.png)\n\n### Fine-Grained Features\n&emsp;&emsp;上述网络上的修改使 Yolo v2 最终在 13×13 的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但是用上细粒度特征的话，这可能对小尺度的物体检测有帮助。Faster R-CNN 和 SSD 都在不同层次的特征图上产生区域建议（SSD 直接就可看得出来这一点）（Faster R-CNN 有吗？），获得了多尺度的适应性。这里使用了一种不同的方法，简单添加了一个转移层 ( passthrough layer )，这一层要把浅层特征图 （分辨率为 26×26，是底层分辨率4倍）连接到深层特征图（concat）。\n> 补充：关于 passthrough layer，具体来说就是特征重排（不涉及到参数学习），前面 26×26×512 的特征图使用按行和按列隔行采样的方法，就可以得到4个新的特征图，维度都是 13×13×512，然后做 concat 操作，得到 13×13×2048 的特征图，将其拼接到后面的层，相当于做了一次特征融合，有利于检测小目标。\n\n### Multi-Scale Training\n&emsp;&emsp;为了使网络具有较强的尺寸鲁棒性，即对不同尺寸的输入都有较好的检测效果。Yolo v2 每迭代几个 epoch 后就会随机选择输入图像的尺寸。由于网络下采样率为32，因此尺寸都是32的倍数，从 ${320,352,...,608}$ 中随机选择。这样，网络不得不学着对不同分辨率的图像都要检测得很好。\n<br>\n\n## Faster\n为了速度，避免使用 VGG-16。提出了 Yolo v2 专用网络 Darknet-19。\n### Darknet-19\n特点：\n1. 主要使用 3×3 卷积核\n2. 每次 Pooling 后，通道数翻倍\n3. 在 3×3 卷积之间使用 1×1 进行特征压缩表示\n\n网络参数如下图所示:\n![网络参数](pic5.png)\n&emsp;&emsp;在进行检测时，将Darknet-19的全连接去掉，换上3个 3×3 的卷积层，每一个卷积层通道均为 1024，每一个 3×3 卷积后面都要接上一个 1×1 的卷积。最后的输出尺寸为 $13×13×(5×(5+20))=13×13×125$。在最后一个 3×3×512 层和倒数第二个卷积层之间加上了 passthrough layer.\n\n<br>\n## 参考文章\n1. [https://blog.csdn.net/hrsstudy/article/details/70767950](https://blog.csdn.net/hrsstudy/article/details/70767950)\n2. [https://blog.csdn.net/jesse_mx/article/details/53925356](https://blog.csdn.net/jesse_mx/article/details/53925356)\n3. [https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md](https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md)\n","tags":["Yolo"]},{"title":"Yolo v1 论文理解","url":"/2018/04/19/Yolo/","content":"本文针对 Yolo v1 的一些处理方法和细节给出了自己的理解。不一定正确，如有错误请指正。\n<!-- more -->\n## Yolo 整体思想\n&emsp;&emsp;YOLO 将输入图像分成 `SxS` 个格子，每个格子负责检测‘落入’该格子的物体。若某个物体的中心位置的坐标落入到某个格子，那么这个格子就负责检测出这个物体。如下图所示，图中物体狗的中心点（红色原点）落入第5行、第2列的格子内，所以这个格子负责预测图像中的物体-狗。\n![yolo 检测思想](pic1.jpg)\n&emsp;&emsp;<p style=\"color:green\">  在计算 loss 时，此格子预测得到的 B 个 bounding box 分别与‘狗’这个物体的 gt box（即图中的红色框）计算 IoU，用 IoU 较大的 bounding box 来计算 localization 损失（称这个 bnd box 负责检测这个物体 --‘狗’），剩下的 bnd box 不参与计算。这边所讨论的是有物体的格子计算损失的方式，没有物体的格子的损失计算与之稍有不同。</p> 具体见损失函数。\n## Yolo输出特征图\n&emsp;&emsp;根据论文中的参数，Yolo 网络的输出为 7×7×30，每一个格子对一个一个 30 维的向量。这个 30 维向量的含义如下图所示。\n![30维向量含义](pic2.gif)\n&emsp;&emsp;如图所示，`x,y` 分别是预测出的 bnd box 的中心相对于当前格子的边界的坐标值。其被格子的尺寸归一化到了[0,1]。也就是说，**预测出的 bnd box 的中心是在格子内部的**。`w,h` 是预测出的 bnd box 的长宽，其被图像尺寸归一化到了[0,1]。也就是说，预测出的 bnd box 的尺寸是不大于图像尺寸的任意尺寸。confidence 就是预测出的 bnd box 和 gt box的 IoU 值。\n## 损失函数\n网络的损失函数定义如下图：\n![网络损失函数](pic3.png)\n&emsp;&emsp;从图中可以看出，损失函数主要分为【坐标预测】【confidence】【类别预测】三个部分。是计算出每一个格子的损失并将所有格子的损失加和作为最后的损失值。由于格子主要分为 【有物体】和【无物体】两种，因此对不同类型的格子，损失的计算方式不完全一样。对于有物体的格子，由于每个格子预测了两个bnd box，因此计算这两个 bnd box 和当前格子对应物体的 GT box 的 IoU，用 IoU 值大的 bnd box 来负责这个 GT box 的预测（也就是回归），也就是说，只计算 IoU 较大的 bnd box 的损失，不计算其他 bnd box 的损失。 $\\mathbb{1}^{obj}_{i}$ 表示第$i$个格子中是否有物体出现，若有，则为1，否则为0。$\\mathbb{1}^{obj}_{ij}$表示第$i$个格子中的第$j$个 bnd box 是否负责当前格子所对应 GT box 的预测。个人理解，当格子中没有物体，即 $\\mathbb{1}^{obj}_i=0$ 时，对任何$j$，$\\mathbb{1}^{obj}_{ij}$都为 0。因此，对于没有物体的格子来说，此格子对损失函数的贡献只有$$ \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}^{noobj}_{ij} (C_i-\\hat C_i)^2 $$ 这一项，而其他的如分类误差和定位误差都为0。\n## Ground Truth设定\n&emsp;&emsp;Yolo 的网络输出为 7×7×30 的 feature map，为了能够构造损失函数，因此需要一个同样尺寸的 Ground Truth。针对每一个格子，它的 Ground Truth 也为一个 30 维的向量。其中 20 维为这个格子所对应的物体的类别，个人理解为 one hot。剩下10维为两个 bnd box 的 GT ，两个 bnd box 的 GT 值相同，即两个相同的 5 维向量。5 维向量分别为当前格子对应的物体的 `w,h,x,y,confidence`。对于有物体的格子，`w,h,x,y` 为格子对应物体的 GT box 的这四项参数，`confidence` 项为 `1`。对于没有物体的格子, `confidence` 项为 `0`，且在计算损失时，不计算当前格子预测出的两个 bnd box 的定位损失（$\\mathbb{1}^{obj}_{ij}=0$）和分类损失（$\\mathbb{1}^{obj}_{i}=0$），因此没有物体的格子的 Ground Truth 中的 w,h,x,y 并不重要，因为它们不参与计算。这样，如上节所讲，没有物体的格子对损失函数的贡献只有$$ \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}^{noobj}_{ij} (C_i-\\hat C_i)^2 $$ 这一项。\n## Inference流程\n&emsp;&emsp;前面讲到，每一个格子最后会得到一个 30 维的向量，其中 10 维分别为两个 bnd box 预测的 w,h,x,y,confidence 值，20 维为此格子的条件类别概率 $Pr(Class_i|Object)$。将两个 bnd box 预测的 confidence 分别和这个条件类别概率相乘，则得到两个 20 维的向量，称作 confidence score，它既包含了 bounding box 中预测的 class 的 probability 信息，也反映了 bounding box 是否含有 Object 和 bounding box 坐标的准确度。这样，对每一个格子，都得到两个 20 维的 score 向量。如下图。\n![score向量](pic4.gif)\n>**这里我有一个疑问：**前面的 score 向量是最后用来比较的标准，即判断每个 bnd box 好坏的标准，但是这里并没有用到 w,h,x,y 四个预测坐标来计算与 GT box 的 IoU。我的解释是：在训练阶段，w,h,x,y 和 confidence 是绑定的，即预测的 bnd box 的 w,h,x,y 向真实值靠近(针对有物体的格子)，confidence 向 1 靠近，因此，当 confidence 较接近 1 时，说明 w,h,x,y 四个值也接近真实值。这样在预测阶段，就不用把 w,h,x,y 拿出来计算，直接拿预测的 confidence 计算，就可以表达 bnd box 是否很好地预测了 GT box 的坐标。\n\n### 得到score向量后的流程\n&emsp;&emsp;在得到每个 bnd box 的 score 向量后（论文中共有 2×7×7=98 个），将其看作一个 20×98 的矩阵。其中每一列为每个 bnd box 的 score 向量，每一行为每个 bnd box 对某个类别的预测 score。针对此矩阵，逐行，即逐类别进行操作。\n- 对某一行（假设第一行，即第一类）的 98 个 score 值进行阈值化，将小于固定阈值的 score 值置零。\n- 根据此行阈值化后的 score 值的大小，对每个 bnd box 的 score 向量（列与列进行排序，不是列的元素排序）进行降序排序。\n- 降序排序后，根据此行的 98 个 score 值以及预测出的 98 对 $(w,h,x,y)$ 进行 NMS，将此行部分 bnd box 的 score 值置零 (见 NMS 的具体方式图)。\n- 对每一行（即每个类别）重复上述三个步骤，这样就得到一个全新的 20×98 的矩阵，其中有大量 0 元素。\n- 对新矩阵的每一列（每一个 bnd box），找出其 score 值最大的类别，若其最大 score 值大于 0，则用此类别对应的颜色画出当前 bnd box 预测的框。\n\n其中，阈值化-降序排序-NMS 流程如下图所示：\n![阈值化-降序排序-NMS](pic5.gif)\nNMS 的具体方式如下图所示：\n![NMS 流程](pic6.gif)\n从新矩阵中画出检测框的过程如下图所示：\n![画检测框](pic7.gif)\n\n## Yolo 的缺点\n- localization error 较大，定位不准确\n- recall rate 较低 (由于每个格子内只预测 B 个 bnd box，因此当有多个物体的中心落入同一个格子时，Yolo 是无法检出的，因此 Yolo 对成群出现的小物体检测效果很差)\n\n## 参考文章\n[https://blog.csdn.net/hrsstudy/article/details/70305791](https://blog.csdn.net/hrsstudy/article/details/70305791)\n[论文原文 https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)\n","tags":["Yolo"]},{"title":"使用 Node.js 生成随机数的服务","url":"/2018/04/16/node-random-number-service/","content":"使用 Node.js 写一个服务器程序，用来返回一个不大于给定数的随机数。\n<!-- more -->\n\n程序代码为:\n{% codeblock lang:js %}\n// these modules need to be imported in order to use them.\n// Node has several modules.  They are like any #include\n// or import statement in other languages\nvar http = require(\"http\");\nvar url = require(\"url\");\n\n// The most important line in any Node file.  This function\n// does the actual process of creating the server.  Technically,\n// Node tells the underlying operating system that whenever a\n// connection is made, this particular callback function should be\n// executed.  Since we're creating a web service with REST API,\n// we want an HTTP server, which requires the http variable\n// we created in the lines above.\n// Finally, you can see that the callback method receives a 'request'\n// and 'response' object automatically.  This should be familiar\n// to any PHP or Java programmer.\nhttp.createServer(function(request, response) {\n\n   // The response needs to handle all the headers, and the return codes\n   // These types of things are handled automatically in server programs\n   // like Apache and Tomcat, but Node requires everything to be done yourself\n   response.writeHead(200, {\"Content-Type\": \"text/plain\"});\n\n   // Here is some unique-looking code.  This is how Node retrives\n   // parameters passed in from client requests.  The url module\n   // handles all these functions.  The parse function\n   // deconstructs the URL, and places the query key-values in the\n   // query object.  We can find the value for the \"number\" key\n   // by referencing it directly - the beauty of JavaScript.\n   var params = url.parse(request.url, true).query;\n   var input = params.number;\n\n   // These are the generic JavaScript methods that will create\n   // our random number that gets passed back to the caller\n   var numInput = new Number(input);\n   var numOutput = new Number(Math.random() * numInput).toFixed(0);\n\n   // Write the random number to response\n   response.write(numOutput);\n\n   // Node requires us to explicitly end this connection.  This is because\n   // Node allows you to keep a connection open and pass data back and forth,\n   // though that advanced topic isn't discussed in this article.\n   response.end();\n\n // When we create the server, we have to explicitly connect the HTTP server to\n // a port.  Standard HTTP port is 80, so we'll connect it to that one.\n}).listen(80);\n\n// Output a String to the console once the server starts up, letting us know everything\n// starts up correctly\nconsole.log(\"Random Number Generator Running...\");\n{% endcodeblock %}\n在命令行执行此js脚本\n{% codeblock lang: %}\n  node xxx.js\n{% endcodeblock %}\n\n然后，在客户端，在地址栏输入\n{% codeblock %}\n  http://localhost:80?number=46\n{% endcodeblock%}\n","tags":["JavaScript"]},{"title":"如何截取网页长屏","url":"/2018/04/16/Webpage2Picture/","content":"本文主要展示如何通过使用 nodejs 进行网页截屏，并截取整个网页。\n<!-- more -->\n&nbsp;\njs 部分代码如下：\n{% codeblock lang:js %}\n  var page = require('webpage').create();\n  page.open('http://www.guancha.cn/', function (status) {\n    console.log(\"status:\" + status);\n    if (status == \"success\") {\n      page.render(\"example.png\")\n    }\n    phantom.exit();\n  });\n{% endcodeblock %}\n\n&nbsp;\n这对一般的网站来说能得到很好的效果。但是存在一个问题，当截取如优酷等视频网站时，由于网页加载时间较长，因此容易在网页还蛮完全渲染完全时进行截图，造成图中部分图片缺失。\n{% asset_img failed.png failed %}\n\n{% link 这篇文章 https://www.v2ex.com/t/67689%}也遇到了同样的问题，并给出了解决方案。\n&nbsp;\n我尝试了使用sleep方式解决。\n{% codeblock lang:js %}\nfunction sleep(d){\n  for( var t = Date.now(); Date.now() - t <= d; );\n}\n\nvar start_time = Date.now();\nvar page = require('webpage').create();\n\npage.open('http://www.youku.com/', function (status) {\n  console.log(\"status:\" + status);\n  if (status == \"success\") {\n    sleep(10000);\n    page.render(\"example.png\");\n    var end_time = Date.now();\n    var used_time = (end_time - start_time) / 1000;\n    console.log(\"Used time is: \" + used_time + ' s');\n\n  }\n  phantom.exit();\n});\n{% endcodeblock %}\n但是效果并不好。\n\n&nbsp;\n因此，又尝试了获取页面 img 元素的方式，判断某个图片是否加载完成。\n{% codeblock lang:js %}\nfunction check() {\n  var stat = page.evaluate(function() {\n    var laifeng = document.getElementById(\"m_250037\");\n    var pict = laifeng.getElementsByTagName(\"img\")[0];\n    return pict.complete;\n  });\n\n  console.log('complete---' + stat);\n  return stat;\n}\n\nfunction waitForReady() {\n  if (!check()) {\n    setTimeout(waitForReady(), 10);\n  } else {\n    console.log('element showed');\n    page.render(\"example.png\");\n  }\n}\n\nvar page = require('webpage').create();\npage.open('http://www.youku.com/', function (status) {\n  console.log(\"status:\" + status);\n  if (status == \"success\") {\n    waitForReady();\n  }\n  phantom.exit();\n});\n{% endcodeblock %}\n但是并没有达到想要的效果，虽然能够获取到元素的完成状态，但实际上截图时图片并没有显示出来。\n{% asset_img example.png \"bad result\" %}\n","tags":["截屏"]}]