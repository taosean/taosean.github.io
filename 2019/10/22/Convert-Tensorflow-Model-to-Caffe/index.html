<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="将 Tensorflow 模型移植到 Caffe 上"><meta name="keywords" content="Caffe,Tensorflow,Porting,移植,padding,flatten"><meta name="author" content="taosean"><meta name="copyright" content="taosean"><title>将 Tensorflow 模型移植到 Caffe 上 | taosean's 学习之旅</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '3.7.1'
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#流程"><span class="toc-number">1.</span> <span class="toc-text">流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一些应该注意的点"><span class="toc-number">2.</span> <span class="toc-text">一些应该注意的点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dump-ckpt-中的参数以及生成-caffemodel-的两个脚本"><span class="toc-number">3.</span> <span class="toc-text">dump ckpt 中的参数以及生成 caffemodel 的两个脚本</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">taosean</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">34</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">104</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">taosean's 学习之旅</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">将 Tensorflow 模型移植到 Caffe 上</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-22</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文主要以 cosine metric learning 工程为例，记录了如何将一个 Tensorflow 模型 (包含 ckpt 文件) 移植到 Caffe 框架下。<br><a id="more"></a></p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><blockquote>
<ol>
<li>根据 Tensorflow 的网络定义源码，手动编写 Caffe 的网络定义文件 *.prototxt.</li>
<li>将训练好的 ckpt 文件中的参数 dump 到磁盘，存为 npy 文件。</li>
<li>使用 pycaffe API, 加载 prototxt 文件，生成 Net 对象。</li>
<li>根据 npy 文件与 Net 对象中网络层的对应关系，将 npy 文件中的值赋给 Net 对象中的参数。</li>
<li>将 Net 对象保存为 caffemodel 文件到磁盘。</li>
</ol>
</blockquote>
<p><br></p>
<h3 id="一些应该注意的点"><a href="#一些应该注意的点" class="headerlink" title="一些应该注意的点"></a>一些应该注意的点</h3><blockquote>
<p><strong>1</strong>. Tensorflow 中的 BN 层对应 Caffe 中的两个层，BatchNorm + Scale. 这是因为 Batch Normalization 算法最后有一个 缩放+偏置 的操作，这就对应 Caffe 中的 Scale 层。通常 Scale 层的缩放参数记为 $\gamma$, 偏置参数记为 $\beta$. 有时候，从 ckpt 模型中 dump 出的 npy 文件没有 BN 层对应的 gamma 值，这可能是因为其在训练时没有使用缩放（batch_norm 函数的 scale 参数设为了 None ），也就是 $\gamma=1$。因此在流程第 4 步时，将相应 shape 的值全为 1 的 ndarray 赋给 Net 对象中 Scale 层对应的 gamma 即可，即 <code>net.params[conv1_scale][0].data[...] = np.ones(bn_beta)</code>. 此外，若 npy 文件中有 BN 层对应的 beta 值，则在 prototxt 文件中对应的 Scale 层应设置 <code>bias_term: true</code>，因为这里的 beta 值就是 bias term. <a href="https://blog.csdn.net/zziahgf/article/details/78843350" target="_blank" rel="noopener">参考1</a> <a href="https://www.cnblogs.com/LaplaceAkuir/p/7811383.html" target="_blank" rel="noopener">参考2</a></p>
</blockquote>
<hr>
<blockquote>
<p><strong>2</strong>. ckpt 中 dump 出的 npy 文件中可能没有某些 Convolution 层的 bias 权重。因此，在 prototxt 文件中，为此 Convolution 层设置 <code>bias_term: false</code>.</p>
</blockquote>
<hr>
<blockquote>
<p><strong>3</strong>. 在从 ckpt 中 dump 出来的参数里，有些可能名如 <code>*/Adam</code>, <code>*/Adam_1</code>，这个是因为模型使用了 Adam 优化器，这两个是对某个参数更新的时候使用的，如果只是在测试阶段进行前向推导，则不需要这两个参数。<a href="https://www.jianshu.com/p/75d8df8511bc" target="_blank" rel="noopener">参考</a><br>但是如果是需要对模型进行 Finetune, 出现大量 Adam 变量丢失的错误，则有可能是 <strong>要恢复的变量的位置</strong> 和 <strong>Adam 优化器的位置</strong> 出错造成的。<a href="https://blog.csdn.net/shwan_ma/article/details/82868751" target="_blank" rel="noopener">【tensorflow】加载pretrained model出现的大量adam变量丢失</a></p>
</blockquote>
<hr>
<blockquote>
<p><strong>4</strong>. 一些 Tensorflow 的项目使用 <code>tf.image.decode_jpeg()</code> 函数来读取 jpg 图像，要注意的是，如果直接使用此函数的默认 <code>dct_method</code> 的话，此函数读取到的值将会跟 <code>cv2.imread()</code> 读取的值不一致。这是因为 <code>tf.image.decode_jpeg()</code> 函数默认会为了解码速度而牺牲一些解码精度。如果想要获得跟 <code>cv2.imread()</code> 相同的结果的话，设置参数 <code>dct_method=&#39;INTEGER_ACCURATE&#39;</code>。<a href="https://github.com/tensorflow/tensorflow/issues/24893#issuecomment-454911098" target="_blank" rel="noopener">参考1</a> <a href="https://stackoverflow.com/a/45520846/8149027" target="_blank" rel="noopener">参考2</a><br>此外，<code>tf.image.decode_jpeg()</code> 函数返回的图像是 <code>RGB</code> 通道的，<code>cv2.imread()</code> 是 <code>BGR</code> 通道。</p>
</blockquote>
<hr>
<blockquote>
<p><strong>5</strong>. <strong>Tensorflow 和 Caffe 在某些操作上的区别</strong><br>&emsp;&emsp;<strong>5-1</strong>. <strong>Feature map 以及 卷积核 维度顺序的区别</strong><br>在 Tensorflow 中，feature map 的默认索引顺序是 <code>NHWC</code>, 卷积核是 <code>HWIO</code>，而 Caffe 中两者的索引顺序是 <code>NCHW</code> 和 <code>OIHW</code>.<br>需要说明的是，如果输入是完全一样的图片，在将图像以及卷积核按各自索引顺序 transpose 好后，后续生成的 feature map 在理论上来说应是完全一样的，它们只是索引的顺序不一样而已(feature map 内部各元素之间的相对顺序是一致的)。<br>&emsp;&emsp;<strong>5-2</strong>.<strong> Flatten 操作的区别</strong><br>据上文所述，同样的输入以及卷积核在不同框架中计算得到的 feature map 是一致的。但是如果要对 feature map (4D) 进行 flatten 操作的话，则此结论可能不成立。因为 flatten 是将 3D 的 feature map 拉伸成 1D, 那么不同的顺序可能就会产生不同的 1D 向量 (内部元素的相对位置可能发生了改变).<br>对同一个 feature map, 若其 shape 为 <code>NHWC</code> （这里的 NHWC 与上文所说的 NHWC 意义不一样，针对某个特定的 feature map, 其 NHWC 的值是固定的）, Tensorflow 的顺序是沿着 <code>C -&gt; W -&gt; H</code>，而 Caffe 的顺序是沿着 <code>W -&gt; H -&gt; C</code>。两者 flatten 的顺序在其各自的输入 feature map 索引顺序中都是 <code>3 -&gt; 2 -&gt; 1</code><br>例如，一个 <code>1x3x3x2</code> 的 feature map.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data =  [[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],   | H</span><br><span class="line">          [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],   |</span><br><span class="line">          [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]],  |</span><br><span class="line">      W  -------------</span><br><span class="line">         [[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>],</span><br><span class="line">          [<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>],</span><br><span class="line">          [<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>]]]</span><br><span class="line"></span><br><span class="line">Tensorflow: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">11</span>, <span class="number">3</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">13</span>, ... ]</span><br><span class="line">Caffe: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, ...]</span><br></pre></td></tr></table></figure></p>
<p>由于 两者 flatten 的顺序在其各自的输入 feature map 索引顺序中都是 <code>3 -&gt; 2 -&gt; 1</code>，因此若这两个 flatten 的输入是一样的话，则它们的输出也是一样的。由于 Caffe 中没有类似 Transpose 这样的层，因此我采取使用 pycaffe 将前向 inference 后 Flatten 层的输入取出，用 numpy 进行 transpose 后，再重新赋给 Flatten 层当输入，然后再次调用 pycaffe 的接口 <code>net.forward(start=&#39;&#39;, end=&#39;&#39;)</code> 指定从某个层开始前向传播，这里就指定 start 参数为 Flatten 层的 name. 这样，即可在 Caffe 中得到与 Tensorflow 中同样的 Flatten 层输出。<a href="https://github.com/BVLC/caffe/issues/2725#issue-93930312" target="_blank" rel="noopener">net.forward 指定起点</a><br>Caffe 的 Flatten 层有 <code>axis</code>, <code>end_axis</code> 两个参数，但是我无论如何设置都无法在不对输入进行 transpose 的情况下得到与 Tensorflow 一样的结果。也许是我没理解对这两个参数的意义。<a href="https://caffe.berkeleyvision.org/tutorial/layers/flatten.html" target="_blank" rel="noopener">参考1</a> <a href="https://stackoverflow.com/a/40401460/8149027" target="_blank" rel="noopener">参考2</a></p>
<p>&emsp;&emsp;<strong>5-3</strong>.<strong> Padding 操作的区别</strong><br>Caffe 中的所有 padding 操作都是对称的，也就是说如果设置 <code>pad_w=1</code> 则会在 feature map 的左右两边都 pad 一个像素。但是 Tensorflow 不是如此，有可能出现左边 pad 1, 右边 pad 2，或者上边 pad 1,下边 pad 2 的情况。因此，在移植时，要保持在 Tensorflow 和 Caffe 中的 padding 方式都一样，这样才能得到相同的结果。<br>下面讨论 Caffe 的 padding 与 Tensorflow 中 <code>SAME</code> padding 方式的差异。<br>正常情况下，如果 <code>kernel_size=3, stride=1</code> 那么 <code>SAME</code> padding 模式会保持输入输出的尺寸相同，因此，需要在输入的上下左右各 pad 1 个像素。这时，在 Caffe 里只要设置 <code>pad: 1</code> 就行，这样两者 pad 的结果就是一样的了。<br>但是，如果遇到 $stride\neq1$ 的情况，情况就变得复杂。有可能两个框架某个 Convolution 或者 Pooling 操作的输入输出尺寸都一样，但是数值却不同。如下图所示 <a href="https://github.com/Microsoft/MMdnn/wiki/Error-in-mobilenet-conversion-from-Tensorflow-to-Caffe-Different-way-of-padding#the-reason-of-the-inconsistent-shapes-is-due-to-symmetric-padding-in-caffe" target="_blank" rel="noopener">来源</a><br><img src="padding_1.png" alt=""><br><img src="padding_2.png" alt=""><br>在 Tensorflow 中, <code>SAME</code> padding 模式的策略是: <a href="https://stackoverflow.com/a/53820765/8149027" target="_blank" rel="noopener">来源</a></p>
<blockquote>
<p>First, consider the <code>SAME</code> padding scheme. A detailed explanation of the reasoning behind it is given in these notes. Here, we summarize the mechanics of this padding scheme. When using ‘SAME’, the output height and width are computed as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out_height = ceil(float(in_height) / float(strides[<span class="number">1</span>]))</span><br><span class="line">out_width  = ceil(float(in_width) / float(strides[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure></p>
<p>The total padding applied along the height and width is computed as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SAME padding 长度计算</span></span><br><span class="line"><span class="keyword">if</span> (in_height % strides[<span class="number">1</span>] == <span class="number">0</span>):</span><br><span class="line">  pad_along_height = max(filter_height - strides[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  pad_along_height = max(filter_height - (in_height % strides[<span class="number">1</span>]), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (in_width % strides[<span class="number">2</span>] == <span class="number">0</span>):</span><br><span class="line">  pad_along_width = max(filter_width - strides[<span class="number">2</span>], <span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  pad_along_width = max(filter_width - (in_width % strides[<span class="number">2</span>]), <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>Finally, the padding on the top, bottom, left and right are:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pad_top = pad_along_height // <span class="number">2</span></span><br><span class="line">pad_bottom = pad_along_height - pad_top</span><br><span class="line">pad_left = pad_along_width // <span class="number">2</span></span><br><span class="line">pad_right = pad_along_width - pad_left</span><br></pre></td></tr></table></figure></p>
<p>Note that the division by 2 means that there might be cases when the padding on both sides (top vs bottom, right vs left) are off by one. In this case, the bottom and right sides always get the one additional padded pixel. For example, when pad_along_height is 5, we pad 2 pixels at the top and 3 pixels at the bottom. Note that this is different from existing libraries such as cuDNN and Caffe, which explicitly specify the number of padded pixels and always pad the same number of pixels on both sides.</p>
<p>For the <code>VALID</code> scheme, the output height and width are computed as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out_height = ceil(float(in_height - filter_height + <span class="number">1</span>) / float(strides[<span class="number">1</span>]))</span><br><span class="line">out_width  = ceil(float(in_width - filter_width + <span class="number">1</span>) / float(strides[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure></p>
<p>and no padding is used.</p>
</blockquote>
<p>在我的实例中，由于最后进入 Flatten 层的 feature map 需要是 <code>13x13</code> 的，而输入图像此前一共经过了 3 次下采样，一次 MAX Pool, 两次 Convolution, 都是 <code>kernel_size=3, stride=2</code>。因此，如何使这三次操作的 padding 操作在两个框架中一致就成了关键问题。由于在 SAME padding 中，<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;                  <code>out_height = ceil(float(in_height) / float(strides[1])),</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;                  <code>out_width  = ceil(float(in_width) / float(strides[2]))</code><br>因此，这三次下采样操作的输入尺寸存在这些可能性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              13                      &lt;-  经过第二次 Conv, stride=2</span><br><span class="line">          /         \</span><br><span class="line">         /           \</span><br><span class="line">       25            26               &lt;-  经过第一次 Conv, stride=2</span><br><span class="line">    /     \       /     \</span><br><span class="line">   49     50     51      52           &lt;-   经过 MAX Pooling</span><br><span class="line">  /  \   /  \   /   \   /   \</span><br><span class="line">97   98 99 100 101 102 103 104        &lt;-      输入图像</span><br></pre></td></tr></table></figure>
<p>由于 Caffe 只能进行对称 padding，因此要选择一个合适的输入图像尺寸，使得在这三次操作时 Tensorflow 不会出现 padding 不对称的情况 (因为这在 Caffe 中无法实现)。<br>根据上段 <code>SAME padding 长度计算部分的公式</code>，我们要使得 <code>pad_along_width，pad_along_height</code> 的数值为 <strong>偶数</strong>，这样才能对称。因此，要使得 <strong><code>in_height % strides[1] != 0, in_width % strides[2] != 0</code></strong>。由于 <code>strides[1]=2</code>，因此，<code>in_height, in_width</code> 必须是 <strong>奇数</strong>。这样，就可以得到每次操作前的输入尺寸分别是 <code>97 -&gt; 49 -&gt; 25 -&gt; 13</code>。这样，在每次操作时，SAME padding 都会为 feature map 在空间维度上四周各 pad 一个像素。而在 Caffe 的对应层的定义里，只要设置 <code>pad: 1</code> 即可。</p>
</blockquote>
<hr>
<blockquote>
<p><strong>6</strong>. 关于 Tensorflow 中获取 Graph 中所有节点名称以及 ckpt 文件中的变量。<br>&emsp;&emsp;<strong>6.1</strong>.<strong>读取 ckpt 中的变量</strong> <a href="https://www.jianshu.com/p/75d8df8511bc" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf;</span><br><span class="line"></span><br><span class="line"> reader = tf.train.NewCheckpointReader(<span class="string">"/path/to/model.ckpt"</span>)</span><br><span class="line"> variables = reader.get_variable_to_shape_map()</span><br><span class="line"> <span class="keyword">for</span> key <span class="keyword">in</span> variables:</span><br><span class="line">      w = reader.get_tensor(key)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<strong>6.2</strong>. <strong>获取 Graph 中的所有结点名称，并计算得到某节点的值</strong> <a href="https://www.jianshu.com/p/3cee7ca5ebd8" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_op_names = [n.name <span class="keyword">for</span> n <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">conv1_op = tf.get_default_graph.get_tensor_by_name(<span class="string">'a_tensor_name_from_above_line:0'</span>)  <span class="comment"># 注意要在名称后面加 :0</span></span><br><span class="line">conv1_value = sess.run(conv1_op, feed_dict=&#123;...&#125;)</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><br></p>
<h3 id="dump-ckpt-中的参数以及生成-caffemodel-的两个脚本"><a href="#dump-ckpt-中的参数以及生成-caffemodel-的两个脚本" class="headerlink" title="dump ckpt 中的参数以及生成 caffemodel 的两个脚本"></a>dump ckpt 中的参数以及生成 caffemodel 的两个脚本</h3><p><a href="dump.py">dump ckpt</a><br><a href="tf2caffe.py">生成 caffemodel</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">taosean</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://taosean.github.io/2019/10/22/Convert-Tensorflow-Model-to-Caffe/">https://taosean.github.io/2019/10/22/Convert-Tensorflow-Model-to-Caffe/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://taosean.github.io">taosean's 学习之旅</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Caffe/">Caffe</a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/Porting/">Porting</a><a class="post-meta__tags" href="/tags/移植/">移植</a><a class="post-meta__tags" href="/tags/padding/">padding</a><a class="post-meta__tags" href="/tags/flatten/">flatten</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/10/29/Packaging-Python/"><i class="fa fa-chevron-left">  </i><span>打包 Python 工程</span></a></div><div class="next-post pull-right"><a href="/2019/07/04/Kalman-Filter/"><span>卡尔曼滤波</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By taosean</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>