<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="【转载】谈谈CNN中的位置和尺度问题"><meta name="keywords" content="平移不变性,平移等变性,translation invariance,translation equivalence"><meta name="author" content="taosean"><meta name="copyright" content="taosean"><title>【转载】谈谈CNN中的位置和尺度问题 | taosean's 学习之旅</title><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '3.7.1'
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-cnn是否存在平移和尺度的不变性和相等性"><span class="toc-number">1.</span> <span class="toc-text"> 1. CNN是否存在平移和尺度的不变性和相等性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-不变性和相等性的定义"><span class="toc-number">1.1.</span> <span class="toc-text"> 1.1 不变性和相等性的定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-cnn网络的执行过程"><span class="toc-number">1.2.</span> <span class="toc-text"> 1.2 CNN网络的执行过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-cnn网络潜在问题与改进"><span class="toc-number">1.3.</span> <span class="toc-text"> 1.3 CNN网络潜在问题与改进</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">taosean</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/taosean">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">40</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">122</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" href="https://github.com/xiaolai">李笑来</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top_img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">taosean's 学习之旅</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">【转载】谈谈CNN中的位置和尺度问题</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-03-18</time><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 8 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>在知乎上看到一篇文章写到关于 CNN 的平移不变性和平移等变性以及 CNN 对位置和深度信息预测的文章，觉得很有收获，因此转载在这里。里面也会穿插自己 的理解。</p>
<a id="more"></a>
<p>个人总结:</p>
<blockquote>
<p>xxx</p>
</blockquote>
<p>以下为正文：</p>
<h1 id="1-cnn是否存在平移和尺度的不变性和相等性"><a class="markdownIt-Anchor" href="#1-cnn是否存在平移和尺度的不变性和相等性"></a> 1. CNN是否存在平移和尺度的不变性和相等性</h1>
<h2 id="11-不变性和相等性的定义"><a class="markdownIt-Anchor" href="#11-不变性和相等性的定义"></a> 1.1 不变性和相等性的定义</h2>
<p>在介绍卷积神经网络（CNN）之前，我们对于不变性和相等性的理解可能来自于传统图像处理算法中的，平移、旋转、光照和尺度等不变性，比如HOG梯度方向直方图，由于cell的存在，其对于平移、旋转有一定的不变性，另外由于对图像局部对比度归一化的操作，使其对于光照也有着一定的不变性。又比如说SIFT特征提取，其对于以上四点都有着不变性，其中由于尺度金字塔，使得对尺度也有不变性。这里我们对于不变性的理解就是，同一对象发生平移、旋转、光照变化、尺度变换甚至形变等，其属性应该一致。下面我们给出具体的不变性和相等性的定义。</p>
<p>其中不变性（invariance）的定义正如上文所说，因此其形式为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><mo stretchy="false">[</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">F(x)=F[transform(x)]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">[</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>而对于相等性（equivalence），顾名思义，就是对输入进行变换之后，输出也发生相应的变换：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">[</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo><mi>F</mi><mo stretchy="false">[</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">transform[F(x)]=F[transform(x)]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">[</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>不过如果我们只考虑输出对于输入不变性和相等性的情况，则会难以理解，因为我们更多地是想象着特征层面的映射，比如：</p>
<p><img src="invariance-equivalence.jpg" alt="不变性和等变性"></p>
<p>那么特征层面对于输出的影响我们可能考虑得比较少，但是却实质存在，比如目标在图像中的平移和尺度等变换，在目标检测任务中，必须要使得网络具有相关的变换相等性，由此捕捉目标的位置和形状变化。而在图像分类、目标识别、行人重识别等任务中，又必须使得网络具有相关变换的不变性。这两点也是目标检测和行人检索领域中一个经典的矛盾问题，目前好像还没有特别好的解决，更多地是分阶段地执行不同的任务，防止特征共用。比如：经典的两阶段目标检测任务中，第一阶段是粗检测和前景背景分类，第二阶段是精修和具体类别分类，有一定的偏重。行人检索算法则大多是先检测后识别的策略。当然除了不变性和相等性的问题，还存在类内差异的问题，比如不同的人对于检测而言都是行人类别，对于识别而言则是不同的人，这对于特征提取也存在挑战。</p>
<h2 id="12-cnn网络的执行过程"><a class="markdownIt-Anchor" href="#12-cnn网络的执行过程"></a> 1.2 CNN网络的执行过程</h2>
<p>我记得我几年前第一次接触到深度学习的时候，对于全连接和CNN的局部连接形式，都有平移、尺度不变性的说法。对于全连接网络，由于下一层的每个节点都会与上一层进行连接：</p>
<p><img src="fc.jpg" alt="全连接网络"></p>
<p>因此无论输入发生了平移、尺度等什么变换，只要其属性没变，全连接网络更能捕捉其中的不变性。而对于卷积神经网络，我们都知道两个特点：局部连接和权值共享。</p>
<p><img src="cnn.jpg" alt="卷积神经网络和全连接网络对比"></p>
<p>对于局部连接，因为全连接参数太多，容易造成过拟合，并且图像领域更多地关注局部细节信息，所以局部连接方式有效。至于权值共享，也有减少参数的作用，很像图像处理中的滤波器。我们早期对于其不变性的理解更多是遵循一个宏观的感受，即由于卷积核的移位滤波，上一层的特征到下一层的特征相对位置<strong>宏观不变</strong>，直到最后输出，类似于全连接的效果，从而获得不变性。</p>
<h2 id="13-cnn网络潜在问题与改进"><a class="markdownIt-Anchor" href="#13-cnn网络潜在问题与改进"></a> 1.3 CNN网络潜在问题与改进</h2>
<p>正因为我刚说的宏观不变，使得输入在经过多次卷积、池化之后，微观/细节的变化累积放大，从而失去了这种不变性，接下来我会结合两篇论文进行介绍。</p>
<p>第一个是为了解决CNN平移不变性对抗性攻击的一篇ICML2019论文《<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1904.11486.pdf%3Ffbclid%3DIwAR1VRPIrctulC6EhTAhKFjIFrlx_JjKR09JnESzLELUnlTL40iOT5tYwotA" target="_blank" rel="noopener">Making Convolutional Networks Shift-Invariant Again</a>》。这篇文章主要讨论了CNN网络中的降采样对于平移不变性的影响：</p>
<p><img src="window1.jpg" alt=""></p>
<p>上图是对于一个窗户分别采用从0~7的平移量，其特征图与不平移的差异，可以明显看到，特征图出现了波动。相应地，上半部分是利用 pix2pix 生成的图像，我们可以看到随着平移量的增大，窗户中的竖直线从两根变成了一根。这一点就表明传统的 CNN 网络并不具有平移不变性。</p>
<p>首先，作者做了这样一个小实验，即采用 maxpooling 对一维向量[0011001100]进行池化，由此得到向量[01010]：</p>
<p><img src="experiment.jpg" alt="小实验"></p>
<p>接着，如果将输入向右平移一个单位，则得到向量[111111]：</p>
<p><img src="res.jpg" alt="小实验结果"></p>
<p>很明显，平移相等性和不变性都丢失了。接着作者做了进一步实验，利用余弦距离来刻画平移相等性，采用VGG网络对Cifar数据集进行试验：</p>
<p><img src="experiment2.jpg" alt=""></p>
<p>其中颜色越深说明差异越大，可以看到每次maxpooling都增加了特征的差异性，不过作者将max和pool操作分开了，为了区分取最大值和降采样的影响：<br>
<img src="maxpooling.jpg" alt=""></p>
<p>很明显，降采样对于平移相等性的影响更大，而CNN中涉及到降采样的操作有：池化（maxpooling和average pooling）和带步长的卷积（strided convolution）。对此作者提出了一种名为<strong>Anti_aliasing</strong>方法，中文叫做抗锯齿处理。传统信号处理领域中对于抗锯齿的技术，一般要么增大采样频率，但由于图像处理任务一般都需要降采样，这一点不适合。要么采用图像模糊（bluring）技术，<font color="orange"><em>根据 Nyquist 采样理论，是给定采样频率，通过降低原始信号的频率来使得信号能够被重构出来</em> </font>，如下图所示。对模糊化处理和未处理的原图像进行下采样，得到图中底部的两张图，模糊化处理的原图像下采样的图像还能看出一些轮廓，而未处理的原图像下采样的图像就显得更加混乱。</p>
<blockquote>
<p><font color="orange"><strong>对橙色部分的理解：</strong></font> 在信号处理中，根据 奈奎斯特 采样定理，如果采样频率大于信号最大频率的两倍，则原始信号能够被完全恢复出来。在这里，pooling 就是一种下采样的过程，当其参数固定后，相当于其采样频率也是固定的。在原图中，有很多边缘等细节，属于高频内容。因此，如果直接对其进行 pooling，也就是下采样，则采样频率无法达到原图最大频率的两倍，从而无法恢复出原图内容。这时，可以逆向思维，通过降低原图的频率，则相对地提高了 pooling 的采样频率。因此，先对原图进行一次低通滤波，也就是模糊处理，来降低它的频率，这样，经过 pooling 下采样后的图像就相对原图下采样的结果可理解了很多。<br>
关于图像滤波的内容，可以参考 <a href="//2021/03/18/Image-and-Filtering/" title="这篇文章">这篇文章</a></p>
</blockquote>
<p><img src="blur.jpg" alt="原图下采样和模糊后图像下采样对比"></p>
<p>作者就是采用了模糊的方式，提出了三种不同的blur kernel：</p>
<ul>
<li>Rectangle-2：[1, 1]，类似于均值池化和最近邻插值；</li>
<li>Triangle-2：[1, 2, 1]，类似于双线性插值；</li>
<li>Binomial-5：[1, 4, 6, 4, 1]，这个被用在拉普拉斯金字塔中。</li>
</ul>
<p>每个核都需要归一化，即除以核内所有元素之和，然后加入到降采样过程，即在降采样之前使用blur kernel进行卷积滤波：</p>
<p><img src="aa.jpg" alt=""></p>
<p>可以看到其效果很不错：</p>
<p><img src="res1.jpg" alt="实验效果"></p>
<p><img src="table1.jpg" alt="实验结果"></p>
<p>代码和模型见：<a href="https://richzhang.github.io/antialiased-cnns/" target="_blank" rel="noopener">https://richzhang.github.io/antialiased-cnns/</a>或者<a href="https://github.com/adobe/antialiased-cnns" target="_blank" rel="noopener">https://github.com/adobe/antialiased-cnns</a></p>
<br>
<p>第二篇是同年发表在JMLR的一篇论文《<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume20/19-519/19-519.pdf" target="_blank" rel="noopener">Why do deep convolutional networks generalize so poorly to small image transformations?</a>》。作者首先给出了几组示例，分别表示了平移、尺度以及轻微图像差异对网络预测分类置信度的影响：</p>
<p><img src="transformation.jpg" alt="平移、尺度和轻微图像差异对预测分类置信度的影响"></p>
<p>作者认为CNN忽视了采样定理，这一点之前Simoncelli等人已经在论文Shiftable multiscale transforms中验证了二次采样在平移不变性上的失败，他们在文中说：</p>
<blockquote>
<p>我们不能简单地把系统中的平移不变性寄希望于卷积和二次采样，输入信号的平移不意味着变换系数的简单平移，除非这个平移是每个二次采样因子的倍数。</p>
</blockquote>
<p>我们现有的网络框架中，越深的网络，降采样次数越多，因此出现的问题更多。紧接着，作者提出了几点论述：</p>
<ul>
<li>如果 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 是经过卷积操作且满足平移不变性的特征，那么全局池化操作 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_i r(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 也满足平移不变性；</li>
<li>对于特征提取器 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 和降采样因子 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span> ，如果输入的平移都可以在输出上线性插值反映出来：</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><munder><mo>∑</mo><mi>i</mi></munder><mi>B</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_{i}B(x-x_i)r(x_i)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.327674em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>由香农-奈奎斯特定理知， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 满足可移位性，要保证采样频率至少为最高信号频率的2倍。</p>
<p>接下来，作者对这些问题做了一些改进尝试：</p>
<ul>
<li><strong>抗锯齿</strong>，这个就是我们刚刚介绍的方法；</li>
<li><strong>数据增强</strong>，当前在很多图像任务中，我们基本都会采用随机裁剪、多尺度、颜色抖动等等数据增强手段，的确也让网络学习到了部分不变性；</li>
<li><strong>减少降采样</strong>，也就是说只依赖卷积对于输入尺度的减小来变化，这一点只对小图像适用，主要是因为计算代价太高。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">taosean</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://taosean.github.io/2021/03/18/Location-and-scale-in-CNN/">https://taosean.github.io/2021/03/18/Location-and-scale-in-CNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://taosean.github.io">taosean's 学习之旅</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/平移不变性/">平移不变性</a><a class="post-meta__tags" href="/tags/平移等变性/">平移等变性</a><a class="post-meta__tags" href="/tags/translation-invariance/">translation invariance</a><a class="post-meta__tags" href="/tags/translation-equivalence/">translation equivalence</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2021/03/18/Image-and-Filtering/"><span>【转载】图像与滤波</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '9738353720f54c04f11e',
  clientSecret: '1511bb92984c25bbde5236ba58e4673d4ff5f92e',
  repo: 'taosean.github.io',
  owner: 'taosean',
  admin: 'taosean',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top_img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By taosean</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>