<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Yolo v2 论文理解"><meta name="keywords" content="目标检测,回归,Yolo"><meta name="author" content="taosean"><meta name="copyright" content="taosean"><title>Yolo v2 论文理解 | taosean's 学习之旅</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.7.1'
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Better"><span class="toc-number">1.</span> <span class="toc-text">Better</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">1.1.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#High-Resolution-Classifier"><span class="toc-number">1.2.</span> <span class="toc-text">High Resolution Classifier</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional-With-Anchor-Boxes"><span class="toc-number">1.3.</span> <span class="toc-text">Convolutional With Anchor Boxes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dimension-Clusters"><span class="toc-number">1.4.</span> <span class="toc-text">Dimension Clusters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Direct-location-prediction"><span class="toc-number">1.5.</span> <span class="toc-text">Direct location prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fine-Grained-Features"><span class="toc-number">1.6.</span> <span class="toc-text">Fine-Grained Features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Scale-Training"><span class="toc-number">1.7.</span> <span class="toc-text">Multi-Scale Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Faster"><span class="toc-number">2.</span> <span class="toc-text">Faster</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Darknet-19"><span class="toc-number">2.1.</span> <span class="toc-text">Darknet-19</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文章"><span class="toc-number">3.</span> <span class="toc-text">参考文章</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">taosean</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">34</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">104</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">taosean's 学习之旅</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Yolo v2 论文理解</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-20</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文针对 Yolo v2 的一些处理方法和细节给出了自己的理解。不一定正确，如有错误请指正。<br><a id="more"></a><br>本文尝试了使用一系列的方法来提升检测效果。<br><br></p>
<h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul>
<li>在所有卷积层后加 Batch Normalization。</li>
<li>mAP 提高了2%。</li>
</ul>
<h3 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h3><ul>
<li>Yolo v1 使用 224×224 的图像预训练分类器，并用来对448的图像进行检测。这意味着网络的卷积层要在适应新的分辨率的同时还要学习检测。</li>
<li>Yolo v2 直接使用 448×448 的图像进行预训练。</li>
<li>mAP提高了4%。</li>
</ul>
<h3 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h3><ul>
<li>Yolo 通过在卷积层上加全连接层来直接预测bbox的坐标。而 Faster R-CNN 使用 RPN 网络来预测 bbox 相对于 anchor 的 offset 和 confidence。预测 offset 比直接预测坐标来得简单。</li>
<li>Yolo v2 去除了全连接层，并减少了一个 Pooling 层。这样最后的 feature map 尺寸是输入的1/32。在这里，网络使用 416 尺寸而不是 448，因为作者想要最后的 feature map 尺寸为奇数。$416\div32=13$。这是因为作者观察发现，很多物体都在图像的中间，因此检测这些图像中间的物体时，只需用最中心的那个位置而不是偶数情况下的中心4个位置。(个人理解为：在这种情况下，那些在图像中心位置的物体的中心点都会落在这 $13\times 13$ 栅格的中心格子中)</li>
<li>使用 anchor boxes 预测坐标的同时，Yolo v2 还对 conditional class probability 的预测机制和空间位置（栅格）做了解耦。<br>&emsp;&emsp;在Yolo v1 将输入图像划分为 $S×S$ 的栅格，每一个栅格预测 $B$ 个 bounding boxes，以及这些 bounding boxes 的 confidence scores。<br>&emsp;&emsp;每一个栅格还要预测 $C$ 个 conditional class probability（条件类别概率）：$Pr(Classi|Object)$。即在一个栅格包含一个 Object 的前提下，它属于某个类的概率。且每个栅格预测一组 ($C$个) 类概率，而不考虑框 $B$ 的数量。<br>&emsp;&emsp;Yolo v2 不再由栅格去预测条件类别概率，而由 Bounding boxes 去预测。在 Yolo v1 中每个栅格只有1组条件类别概率，而在 Yolo v2 中，因为每个栅格有 $B$ 个 bounding boxes，所以有 $B$ 组条件类别概率。<br>在 Yolo v1 中输出的维度为 $S\times S \times (B \times 5 + C )$，而 Yolo v2 为$S \times S \times (B \times (5 + C))$。如下图所示。</li>
<li>使用了Anchor box 后，mAP 从 69.5% 降到了 69.2%，但是 recall 从 81% 增加到了 88%。</li>
</ul>
<p><hr><br> <img src="pic1.png" alt="v1 和 v2 输出维度对比"></p>
<h3 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h3><p>&emsp;&emsp;采用 Anchor 机制后遇到两个遇到<font style="color:orange"> 两个问题 </font>，其中<font style="color:cyan"> 第一个 </font>为 anchor 尺寸的设置问题。Faster R-CNN 等网络的 anchor 的尺寸是人工选定的，虽然网络可以通过学习来调整预测框的尺寸，但是如果一开始就给一个较合适的 anchor 的话，网络学习起来会更加的容易。Yolo v2 通过 k-means 的方式来学习到 anchor 的尺寸分布情况。对训练集中的所有标定的框，即 GT box，根据他们的 (x,y,w,h) 的值进行 k-means 聚类。如果将用 (x,y,w,h) 来代表一个 GT box，并用4维向量的标准欧式距离来作为距离度量的话，大的框可能会比小的框产生更大的误差，比如 (1,1,2,2) 和 (2,2,4,4) 的欧式距离为$\sqrt{10}$，而 (2,2,4,4) 和 (4,4,8,8) 的欧式距离为 $2\sqrt{10}$，而如果采用IoU的度量方式，这两对框的距离相等都是3/4。而后一种情况所代表的两种框是前一种情况两种框尺寸的两倍。因此，可以看出，采用IoU的方式作为两个框之间的距离度量，可以避免框的尺寸带来的影响。<br>&emsp;&emsp;通过聚类，可以将所有的 (x,y,w,h) 聚为 k 类，最后得到 k 个类别中心，这 k 个类别中心就代表 k 个矩形框。论文中使用 Dimension Clusters 的结果如下图所示。<br><img src="pic2.jpg" alt="维度聚类-5类"></p>
<p>在网上看到另一种说法</p>
<blockquote>
<p>算法过程（k-means）是: 将每个 bbox 的宽和高相对整张图片的比例 $(w_r,h_r)$ 进行聚类,得到 k 个anchor box.<br>算法实现代码可以参考: <a href="https://github.com/PaulChongPeng/darknet/blob/master/tools/k_means_yolo.py" target="_blank" rel="noopener">k_means_yolo.py</a><br>其实根据距离函数就可以看出，k-means 一定是对 (x,y,w,h) 进行聚类的（计算 IoU 需要用到 (x,y) ）。只不过最后不关注聚类中心的 (x,y) ，只关注聚类中心的 (w,h)。上面代码中的操作也证实了这一点。</p>
</blockquote>
<p>&emsp;&emsp;与手工挑选的相比，使用 Dimension Clusters 的方法效果更好。对比效果如下图所示。<br><img src="pic3.jpg" alt="使用维度聚类效果对比"><br>其中 Cluster SSE 表示使用 sum squared error 作为度量进行聚类，Cluster IoU 表示使用 IoU 作为度量进行聚类。Anchor boxes 为采取类似 Faster R-CNN 中的方法。</p>
<h3 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h3><p>&emsp;&emsp;前面提到了两个问题，其中 <font color="cyan">第二个</font> 问题是：模型的不稳定性。不稳定性主要来源于预测框的(x,y)坐标。在 RPN 中，网络预测 $t_x$ 和 $t_y$ ，因此，框的中心(x,y)计算方式为:</p>
<script type="math/tex; mode=display">\begin{cases} x = (t_x * w_a) + x_a\\
    y = (t_y * h_a) + y_a\\
   \end{cases}</script><p>在原论文中，$x_a$ 和 $y_a$ 前使用的是减号，估计是作者的笔误。其中，$w_a$ 和 $h_a$ 为 anchor 的宽和长。如果 $t_x&gt;0$，bounding box 会向右移动 anchor 宽的 $t_x$ 倍，如果 $t_x&lt;0$，bounding box 会向左移动 anchor 宽的 $t_x$ 倍。由公式可以看出，由于 $t_x$ 和 $t_y$ 没有限制（取值范围没有限制），因此最后得到的 $x$ 和 $y$ 可以落在图像上的任意一个位置，因此一个 anchor 可能检测一个离自己很远的物体，尽管这个物体应该由离其自身近的 anchor 来检测。<br>&emsp;&emsp;Yolo v2 不采用预测 offset 的方法，而是延续 Yolo v1 预测 bbox 相对每个 grid cell 左上角的坐标，确保每个 bbox 的中心落在 grid cell 内。作者使用 logistic 函数来约束预测值。<br>&emsp;&emsp;对网络的输出 feature map (13×13)，Yolo v2 对每一个 grid cell 预测 5 个 bbox (对应 5 个 anchor)，每个 bbox 由 5 维向量 $( t_x,t_y,t_w,t_h,t_o)$ 表示。因此，对 feature map 上的每一个位置来说，都会预测出一个 5×5 的向量。(要牢记 feature map 上的每一个位置都对应图像上的一个 grid cell )。如果这个 grid cell 的左上角距离图像左上角偏移为 $(c_x,c_y)$，anchor box 的宽高为 $p_w, p_h$，那么这个预测的 bounding box 的中心点为 $(b_x, b_y)$，宽高为 $(b_w, b_h)$。计算方式如下图:</p>
<p><hr><br> <img src="pic4.png" alt="一个预测框的计算方式"><br> <hr><br> $\sigma(t_x)$ 是 bounding box 的中心相对栅格左上角的横坐标，$\sigma(t_y)$ 是纵坐标。$\sigma(t_o)$ 是 bounding box 的 confidence score。<br> 这样，就可以预测出 5 组 bbox.</p>
<p><font color="green">个人理解: </font>由于预测的值是 $( t_x,t_y,t_w,t_h,t_o)$ 这 5 个数，且 $(b_x,b_y,b_w,b_h,confidence)$ 可以由上面的预测值计算出来，因此不确定最后的 GT 向量是用哪一个来计算 loss。<del>个人猜测是 $(b_x,b_y,b_w,b_h,confidence)$。因为这样与 Yolo v1 中的 loss 一致。因此有可能 5 个 GT 都是一样的，就像 Yolo v1 中的一样。</del></p>
<p><center><font color="orange">---------------------------  损失计算理解更新  -----------------------------</font></center><br>通过阅读损失部分源码，感觉上面的猜测不正确。应该是使用 $(t_x,t_y,t_w,t_h,t_o)$ 这 5 个数进行损失计算的。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">delta_region_box</span><span class="params">(box truth, <span class="keyword">float</span> *x, <span class="keyword">float</span> *biases, <span class="keyword">int</span> n, <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> w, <span class="keyword">int</span> h, <span class="keyword">float</span> *delta, <span class="keyword">float</span> scale, <span class="keyword">int</span> stride)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    box pred = get_region_box(x, biases, n, index, i, j, w, h, stride);</span><br><span class="line">    <span class="keyword">float</span> iou = box_iou(pred, truth);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里的 i,j 是对 cell 的索引，因此 i,j 的范围都是 [0,12] 的整数. w,h 都是 13，delta 存储 loss， x 是预测结果，stride=169.</span></span><br><span class="line">    <span class="comment">// n 是 [0,5] 的整数，是一个确定 cell 内 bbox 的索引。</span></span><br><span class="line">    <span class="keyword">float</span> tx = (truth.x*w - i); <span class="comment">// 这里跟论文中有一点不一样，这里的 tx 其实相当于论文中的$\sigma&#123;t_x&#125;$，这里的 i 就相当于论文中的 cx。ty同理。</span></span><br><span class="line">    <span class="keyword">float</span> ty = (truth.y*h - j);</span><br><span class="line">    <span class="keyword">float</span> tw = <span class="built_in">log</span>(truth.w*w / biases[<span class="number">2</span>*n]); <span class="comment">// biases 数组中存储的是 5 个 anchor 的长宽。</span></span><br><span class="line">    <span class="keyword">float</span> th = <span class="built_in">log</span>(truth.h*h / biases[<span class="number">2</span>*n + <span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    delta[index + <span class="number">0</span>*stride] = scale * (tx - x[index + <span class="number">0</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">1</span>*stride] = scale * (ty - x[index + <span class="number">1</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">2</span>*stride] = scale * (tw - x[index + <span class="number">2</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">3</span>*stride] = scale * (th - x[index + <span class="number">3</span>*stride]);</span><br><span class="line">    <span class="keyword">return</span> iou;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从代码可以看出，计算每一个 bbox 的损失时，是将当前 bbox 所在 cell 对应的 GT box 的 $(x,y,w,h)$ 转换成 $(t_x,t_y,t_w,t_h)$ 后，再跟网络输出的预测值计算损失的。可以看出，针对同一 cell 内不同的 anchor 对应的预测框（一个 anchor 对应一个预测框），它们的 $(t_x,t_y)$ 真实值都是相等的，因为 $i,j$ 是 cell 的索引，所以对某一 cell 内的预测框来说，$i,j$ 都是相等的。但是 n 是 cell 内预测框，或者是 anchor 的索引，因此对每个预测框来说，他们的 $(t_w,t_h)$ 的真实值是不等的。（这里没有说一个 cell 是否只计算 responsable 的那个 bbox 的损失，需要查看别的地方的代码）</p>
<p>Yolo v2 论文没有给出损失函数。网上找到的一个损失函数如下，不知正确与否。<br><img src="pic6.png" alt="Yolo v2 损失函数"></p>
<h3 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h3><p>&emsp;&emsp;上述网络上的修改使 Yolo v2 最终在 13×13 的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但是用上细粒度特征的话，这可能对小尺度的物体检测有帮助。Faster R-CNN 和 SSD 都在不同层次的特征图上产生区域建议（SSD 直接就可看得出来这一点）（Faster R-CNN 有吗？），获得了多尺度的适应性。这里使用了一种不同的方法，简单添加了一个转移层 ( passthrough layer )，这一层要把浅层特征图 （分辨率为 26×26，是底层分辨率4倍）连接到深层特征图（concat）。</p>
<blockquote>
<p>补充：关于 passthrough layer，具体来说就是特征重排（不涉及到参数学习），前面 26×26×512 的特征图使用按行和按列隔行采样的方法，就可以得到4个新的特征图，维度都是 13×13×512，然后做 concat 操作，得到 13×13×2048 的特征图，将其拼接到后面的层，相当于做了一次特征融合，有利于检测小目标。</p>
</blockquote>
<h3 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h3><p>&emsp;&emsp;为了使网络具有较强的尺寸鲁棒性，即对不同尺寸的输入都有较好的检测效果。Yolo v2 每迭代几个 epoch 后就会随机选择输入图像的尺寸。由于网络下采样率为32，因此尺寸都是32的倍数，从 ${320,352,…,608}$ 中随机选择。这样，网络不得不学着对不同分辨率的图像都要检测得很好。<br><br></p>
<h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><p>为了速度，避免使用 VGG-16。提出了 Yolo v2 专用网络 Darknet-19。</p>
<h3 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h3><p>特点：</p>
<ol>
<li>主要使用 3×3 卷积核</li>
<li>每次 Pooling 后，通道数翻倍</li>
<li>在 3×3 卷积之间使用 1×1 进行特征压缩表示</li>
</ol>
<p>网络参数如下图所示:<br><img src="pic5.png" alt="网络参数"><br>&emsp;&emsp;在进行检测时，将Darknet-19的全连接去掉，换上3个 3×3 的卷积层，每一个卷积层通道均为 1024，每一个 3×3 卷积后面都要接上一个 1×1 的卷积。最后的输出尺寸为 $13×13×(5×(5+20))=13×13×125$。在最后一个 3×3×512 层和倒数第二个卷积层之间加上了 passthrough layer.</p>
<p><br></p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol>
<li><a href="https://blog.csdn.net/hrsstudy/article/details/70767950" target="_blank" rel="noopener">https://blog.csdn.net/hrsstudy/article/details/70767950</a></li>
<li><a href="https://blog.csdn.net/jesse_mx/article/details/53925356" target="_blank" rel="noopener">https://blog.csdn.net/jesse_mx/article/details/53925356</a></li>
<li><a href="https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md" target="_blank" rel="noopener">https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md</a></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">taosean</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://taosean.github.io/2018/04/20/Yolo-v2/">https://taosean.github.io/2018/04/20/Yolo-v2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/目标检测/">目标检测</a><a class="post-meta__tags" href="/tags/回归/">回归</a><a class="post-meta__tags" href="/tags/Yolo/">Yolo</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/04/25/Faster-R-CNN/"><i class="fa fa-chevron-left">  </i><span>RPN 中的 loss 理解</span></a></div><div class="next-post pull-right"><a href="/2018/04/19/Yolo/"><span>Yolo v1 论文理解</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By taosean</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>